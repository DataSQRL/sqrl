/Library/Java/JavaVirtualMachines/jdk-11.0.16.jdk/Contents/Home/bin/java -ea -Dsnapshots.update=false -Didea.test.cyclic.buffer.size=1048576 -javaagent:/Applications/IntelliJ IDEA CE.app/Contents/lib/idea_rt.jar=51717:/Applications/IntelliJ IDEA CE.app/Contents/bin -Dfile.encoding=UTF-8 -classpath /Users/henneberger/.m2/repository/org/junit/platform/junit-platform-launcher/1.8.2/junit-platform-launcher-1.8.2.jar:/Users/henneberger/.m2/repository/org/junit/vintage/junit-vintage-engine/5.8.2/junit-vintage-engine-5.8.2.jar:/Applications/IntelliJ IDEA CE.app/Contents/lib/idea_rt.jar:/Applications/IntelliJ IDEA CE.app/Contents/plugins/junit/lib/junit5-rt.jar:/Applications/IntelliJ IDEA CE.app/Contents/plugins/junit/lib/junit-rt.jar:/Users/henneberger/sqrl/sqrl-testing/sqrl-integration-tests/target/test-classes:/Users/henneberger/sqrl/sqrl-testing/sqrl-integration-tests/target/classes:/Users/henneberger/sqrl/sqrl-io/sqrl-io-format-json/target/classes:/Users/henneberger/sqrl/sqrl-io/sqrl-io-core/target/classes:/Users/henneberger/.m2/repository/org/apache/commons/commons-csv/1.9.0/commons-csv-1.9.0.jar:/Users/henneberger/.m2/repository/org/apache/commons/commons-lang3/3.12.0/commons-lang3-3.12.0.jar:/Users/henneberger/sqrl/sqrl-io/sqrl-io-format-csv/target/classes:/Users/henneberger/sqrl/sqrl-planner/sqrl-planner-schema-flexible/target/classes:/Users/henneberger/sqrl/sqrl-planner/sqrl-common/target/classes:/Users/henneberger/sqrl/sqrl-engines/sqrl-engine-core/target/classes:/Users/henneberger/sqrl/sqrl-execute/sqrl-execute-flink/sqrl-execute-flink-function/target/classes:/Users/henneberger/sqrl/sqrl-io/sqrl-io-schema-flexible/target/classes:/Users/henneberger/sqrl/sqrl-tools/sqrl-discovery/target/classes:/Users/henneberger/sqrl/sqrl-engines/sqrl-engine-flink/target/classes:/Users/henneberger/.m2/repository/org/apache/flink/flink-core/1.16.1/flink-core-1.16.1.jar:/Users/henneberger/.m2/repository/org/apache/flink/flink-annotations/1.16.1/flink-annotations-1.16.1.jar:/Users/henneberger/.m2/repository/org/apache/flink/flink-metrics-core/1.16.1/flink-metrics-core-1.16.1.jar:/Users/henneberger/.m2/repository/org/apache/flink/flink-shaded-asm-9/9.2-15.0/flink-shaded-asm-9-9.2-15.0.jar:/Users/henneberger/.m2/repository/org/apache/flink/flink-shaded-jackson/2.12.4-15.0/flink-shaded-jackson-2.12.4-15.0.jar:/Users/henneberger/.m2/repository/commons-collections/commons-collections/3.2.2/commons-collections-3.2.2.jar:/Users/henneberger/.m2/repository/org/apache/flink/flink-shaded-guava/30.1.1-jre-15.0/flink-shaded-guava-30.1.1-jre-15.0.jar:/Users/henneberger/.m2/repository/org/apache/flink/flink-streaming-java/1.16.1/flink-streaming-java-1.16.1.jar:/Users/henneberger/.m2/repository/org/apache/flink/flink-runtime/1.16.1/flink-runtime-1.16.1.jar:/Users/henneberger/.m2/repository/org/apache/flink/flink-rpc-core/1.16.1/flink-rpc-core-1.16.1.jar:/Users/henneberger/.m2/repository/org/apache/flink/flink-rpc-akka-loader/1.16.1/flink-rpc-akka-loader-1.16.1.jar:/Users/henneberger/.m2/repository/org/apache/flink/flink-queryable-state-client-java/1.16.1/flink-queryable-state-client-java-1.16.1.jar:/Users/henneberger/.m2/repository/org/apache/flink/flink-hadoop-fs/1.16.1/flink-hadoop-fs-1.16.1.jar:/Users/henneberger/.m2/repository/org/apache/flink/flink-shaded-netty/4.1.70.Final-15.0/flink-shaded-netty-4.1.70.Final-15.0.jar:/Users/henneberger/.m2/repository/org/apache/flink/flink-shaded-zookeeper-3/3.5.9-15.0/flink-shaded-zookeeper-3-3.5.9-15.0.jar:/Users/henneberger/.m2/repository/org/javassist/javassist/3.24.0-GA/javassist-3.24.0-GA.jar:/Users/henneberger/.m2/repository/org/apache/commons/commons-math3/3.6.1/commons-math3-3.6.1.jar:/Users/henneberger/.m2/repository/org/apache/flink/flink-clients/1.16.1/flink-clients-1.16.1.jar:/Users/henneberger/.m2/repository/org/apache/flink/flink-optimizer/1.16.1/flink-optimizer-1.16.1.jar:/Users/henneberger/sqrl/sqrl-tools/sqrl-cli/target/test-classes:/Users/henneberger/sqrl/sqrl-base/target/classes:/Users/henneberger/.m2/repository/com/fasterxml/jackson/core/jackson-core/2.14.0/jackson-core-2.14.0.jar:/Users/henneberger/.m2/repository/com/fasterxml/jackson/module/jackson-module-blackbird/2.14.0/jackson-module-blackbird-2.14.0.jar:/Users/henneberger/.m2/repository/com/fasterxml/jackson/dataformat/jackson-dataformat-yaml/2.14.0/jackson-dataformat-yaml-2.14.0.jar:/Users/henneberger/.m2/repository/com/fasterxml/jackson/datatype/jackson-datatype-jsr310/2.14.0/jackson-datatype-jsr310-2.14.0.jar:/Users/henneberger/.m2/repository/com/google/guava/guava/31.1-jre/guava-31.1-jre.jar:/Users/henneberger/.m2/repository/com/google/guava/failureaccess/1.0.1/failureaccess-1.0.1.jar:/Users/henneberger/.m2/repository/com/google/guava/listenablefuture/9999.0-empty-to-avoid-conflict-with-guava/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/Users/henneberger/.m2/repository/com/google/errorprone/error_prone_annotations/2.11.0/error_prone_annotations-2.11.0.jar:/Users/henneberger/.m2/repository/com/google/j2objc/j2objc-annotations/1.3/j2objc-annotations-1.3.jar:/Users/henneberger/.m2/repository/com/google/auto/service/auto-service/1.0.1/auto-service-1.0.1.jar:/Users/henneberger/.m2/repository/com/google/auto/service/auto-service-annotations/1.0.1/auto-service-annotations-1.0.1.jar:/Users/henneberger/.m2/repository/com/google/auto/auto-common/1.2/auto-common-1.2.jar:/Users/henneberger/.m2/repository/org/apache/commons/commons-configuration2/2.9.0/commons-configuration2-2.9.0.jar:/Users/henneberger/.m2/repository/commons-logging/commons-logging/1.2/commons-logging-1.2.jar:/Users/henneberger/.m2/repository/commons-beanutils/commons-beanutils/1.9.4/commons-beanutils-1.9.4.jar:/Users/henneberger/.m2/repository/org/apache/logging/log4j/log4j-api/2.19.0/log4j-api-2.19.0.jar:/Users/henneberger/.m2/repository/org/apache/logging/log4j/log4j-core/2.19.0/log4j-core-2.19.0.jar:/Users/henneberger/.m2/repository/org/apache/logging/log4j/log4j-slf4j-impl/2.19.0/log4j-slf4j-impl-2.19.0.jar:/Users/henneberger/sqrl/sqrl-execute/sqrl-execute-http/sqrl-execute-http-aws-lambda/target/classes:/Users/henneberger/.m2/repository/com/amazonaws/aws-lambda-java-core/1.2.2/aws-lambda-java-core-1.2.2.jar:/Users/henneberger/.m2/repository/com/amazonaws/aws-lambda-java-events/3.11.0/aws-lambda-java-events-3.11.0.jar:/Users/henneberger/.m2/repository/joda-time/joda-time/2.6/joda-time-2.6.jar:/Users/henneberger/.m2/repository/ch/qos/logback/logback-classic/1.4.6/logback-classic-1.4.6.jar:/Users/henneberger/.m2/repository/ch/qos/logback/logback-core/1.4.6/logback-core-1.4.6.jar:/Users/henneberger/.m2/repository/org/jooq/jooq/3.16.12/jooq-3.16.12.jar:/Users/henneberger/.m2/repository/io/r2dbc/r2dbc-spi/0.9.0.RELEASE/r2dbc-spi-0.9.0.RELEASE.jar:/Users/henneberger/.m2/repository/jakarta/xml/bind/jakarta.xml.bind-api/3.0.0/jakarta.xml.bind-api-3.0.0.jar:/Users/henneberger/.m2/repository/com/sun/activation/jakarta.activation/2.0.0/jakarta.activation-2.0.0.jar:/Users/henneberger/sqrl/sqrl-io/sqrl-io-jdbc/target/classes:/Users/henneberger/.m2/repository/com/esotericsoftware/kryo/kryo/2.24.0/kryo-2.24.0.jar:/Users/henneberger/.m2/repository/com/esotericsoftware/minlog/minlog/1.2/minlog-1.2.jar:/Users/henneberger/.m2/repository/org/objenesis/objenesis/2.1/objenesis-2.1.jar:/Users/henneberger/sqrl/sqrl-engines/sqrl-engine-kafka/target/classes:/Users/henneberger/sqrl/sqrl-io/sqrl-io-file/target/classes:/Users/henneberger/.m2/repository/org/apache/flink/flink-connector-files/1.16.1/flink-connector-files-1.16.1.jar:/Users/henneberger/sqrl/sqrl-io/sqrl-io-kafka/target/classes:/Users/henneberger/sqrl/sqrl-io/sqrl-io-print/target/classes:/Users/henneberger/sqrl/sqrl-execute/sqrl-execute-flink/sqrl-execute-flink-connectors/sqrl-execute-flink-postgres/target/classes:/Users/henneberger/.m2/repository/info/picocli/picocli/4.7.4/picocli-4.7.4.jar:/Users/henneberger/sqrl/sqrl-execute/sqrl-execute-flink/sqrl-execute-flink-connectors/sqrl-execute-flink-inmem/target/classes:/Users/henneberger/sqrl/sqrl-io/sqrl-io-mem/target/classes:/Users/henneberger/.m2/repository/org/apache/flink/flink-file-sink-common/1.16.1/flink-file-sink-common-1.16.1.jar:/Users/henneberger/.m2/repository/org/apache/flink/flink-shaded-force-shading/15.0/flink-shaded-force-shading-15.0.jar:/Users/henneberger/sqrl/sqrl-execute/sqrl-execute-flink/sqrl-execute-flink-core/target/classes:/Users/henneberger/.m2/repository/org/apache/flink/flink-json/1.16.1/flink-json-1.16.1.jar:/Users/henneberger/.m2/repository/com/google/code/findbugs/jsr305/1.3.9/jsr305-1.3.9.jar:/Users/henneberger/.m2/repository/com/opencsv/opencsv/5.7.1/opencsv-5.7.1.jar:/Users/henneberger/.m2/repository/org/apache/commons/commons-text/1.10.0/commons-text-1.10.0.jar:/Users/henneberger/.m2/repository/org/apache/flink/flink-table-api-java-bridge/1.16.1/flink-table-api-java-bridge-1.16.1.jar:/Users/henneberger/.m2/repository/org/apache/flink/flink-table-api-java/1.16.1/flink-table-api-java-1.16.1.jar:/Users/henneberger/.m2/repository/org/apache/flink/flink-table-common/1.16.1/flink-table-common-1.16.1.jar:/Users/henneberger/.m2/repository/com/ibm/icu/icu4j/67.1/icu4j-67.1.jar:/Users/henneberger/.m2/repository/org/apache/flink/flink-table-api-bridge-base/1.16.1/flink-table-api-bridge-base-1.16.1.jar:/Users/henneberger/.m2/repository/org/apache/flink/flink-java/1.16.1/flink-java-1.16.1.jar:/Users/henneberger/.m2/repository/com/twitter/chill-java/0.7.6/chill-java-0.7.6.jar:/Users/henneberger/sqrl/sqrl-execute/sqrl-execute-flink/sqrl-execute-flink-connectors/sqrl-execute-flink-jdbc/target/classes:/Users/henneberger/.m2/repository/org/apache/flink/flink-connector-jdbc/1.16.1/flink-connector-jdbc-1.16.1.jar:/Users/henneberger/sqrl/sqrl-execute/sqrl-execute-flink/sqrl-execute-flink-connectors/sqrl-execute-flink-h2/target/classes:/Users/henneberger/.m2/repository/com/h2database/h2/2.1.214/h2-2.1.214.jar:/Users/henneberger/sqrl/sqrl-execute/sqrl-execute-flink/sqrl-execute-flink-connectors/sqrl-execute-flink-file/target/classes:/Users/henneberger/.m2/repository/org/apache/flink/flink-s3-fs-hadoop/1.16.1/flink-s3-fs-hadoop-1.16.1.jar:/Users/henneberger/sqrl/sqrl-execute/sqrl-execute-flink/sqrl-execute-flink-connectors/sqrl-execute-flink-kafka/target/classes:/Users/henneberger/.m2/repository/org/apache/flink/flink-connector-kafka/1.16.1/flink-connector-kafka-1.16.1.jar:/Users/henneberger/.m2/repository/org/apache/flink/flink-connector-base/1.16.1/flink-connector-base-1.16.1.jar:/Users/henneberger/sqrl/sqrl-execute/sqrl-execute-flink/sqrl-execute-flink-connectors/sqrl-execute-flink-print/target/classes:/Users/henneberger/sqrl/sqrl-execute/sqrl-execute-flink/sqrl-execute-flink-connectors/sqrl-execute-flink-sqlite/target/classes:/Users/henneberger/.m2/repository/org/xerial/sqlite-jdbc/3.40.0.0/sqlite-jdbc-3.40.0.0.jar:/Users/henneberger/sqrl/sqrl-tools/sqrl-packager/target/classes:/Users/henneberger/sqrl/sqrl-planner/sqrl-planner-local/target/classes:/Users/henneberger/.m2/repository/commons-codec/commons-codec/1.15/commons-codec-1.15.jar:/Users/henneberger/.m2/repository/org/apache/commons/commons-collections4/4.4/commons-collections4-4.4.jar:/Users/henneberger/.m2/repository/net/lingala/zip4j/zip4j/2.11.2/zip4j-2.11.2.jar:/Users/henneberger/sqrl/sqrl-execute/sqrl-execute-http/sqrl-execute-http-vertx/target/classes:/Users/henneberger/sqrl/sqrl-execute/sqrl-execute-http/sqrl-execute-http-core/target/classes:/Users/henneberger/.m2/repository/com/graphql-java/graphql-java/19.2/graphql-java-19.2.jar:/Users/henneberger/.m2/repository/com/graphql-java/java-dataloader/3.2.0/java-dataloader-3.2.0.jar:/Users/henneberger/.m2/repository/io/vertx/vertx-config/4.3.5/vertx-config-4.3.5.jar:/Users/henneberger/.m2/repository/io/vertx/vertx-web-openapi/4.3.5/vertx-web-openapi-4.3.5.jar:/Users/henneberger/.m2/repository/io/vertx/vertx-web-validation/4.3.5/vertx-web-validation-4.3.5.jar:/Users/henneberger/.m2/repository/io/vertx/vertx-json-schema/4.3.5/vertx-json-schema-4.3.5.jar:/Users/henneberger/.m2/repository/org/yaml/snakeyaml/1.32/snakeyaml-1.32.jar:/Users/henneberger/.m2/repository/io/vertx/vertx-jdbc-client/4.3.5/vertx-jdbc-client-4.3.5.jar:/Users/henneberger/.m2/repository/io/vertx/vertx-sql-client/4.3.5/vertx-sql-client-4.3.5.jar:/Users/henneberger/.m2/repository/com/mchange/c3p0/0.9.5.5/c3p0-0.9.5.5.jar:/Users/henneberger/.m2/repository/com/mchange/mchange-commons-java/0.2.19/mchange-commons-java-0.2.19.jar:/Users/henneberger/.m2/repository/io/vertx/vertx-web/4.3.5/vertx-web-4.3.5.jar:/Users/henneberger/.m2/repository/io/vertx/vertx-bridge-common/4.3.5/vertx-bridge-common-4.3.5.jar:/Users/henneberger/.m2/repository/io/vertx/vertx-web-graphql/4.3.5/vertx-web-graphql-4.3.5.jar:/Users/henneberger/.m2/repository/io/vertx/vertx-pg-client/4.3.5/vertx-pg-client-4.3.5.jar:/Users/henneberger/.m2/repository/io/agroal/agroal-pool/1.11/agroal-pool-1.11.jar:/Users/henneberger/.m2/repository/io/agroal/agroal-api/1.11/agroal-api-1.11.jar:/Users/henneberger/.m2/repository/org/reactivestreams/reactive-streams/1.0.4/reactive-streams-1.0.4.jar:/Users/henneberger/.m2/repository/io/vertx/vertx-kafka-client/4.3.5/vertx-kafka-client-4.3.5.jar:/Users/henneberger/.m2/repository/io/projectreactor/reactor-core/3.5.6/reactor-core-3.5.6.jar:/Users/henneberger/.m2/repository/io/netty/netty-codec/4.1.92.Final/netty-codec-4.1.92.Final.jar:/Users/henneberger/.m2/repository/io/netty/netty-common/4.1.92.Final/netty-common-4.1.92.Final.jar:/Users/henneberger/.m2/repository/io/netty/netty-buffer/4.1.92.Final/netty-buffer-4.1.92.Final.jar:/Users/henneberger/.m2/repository/io/netty/netty-transport/4.1.92.Final/netty-transport-4.1.92.Final.jar:/Users/henneberger/.m2/repository/org/postgresql/postgresql/42.5.0/postgresql-42.5.0.jar:/Users/henneberger/.m2/repository/org/checkerframework/checker-qual/3.5.0/checker-qual-3.5.0.jar:/Users/henneberger/.m2/repository/com/ongres/scram/common/2.1/common-2.1.jar:/Users/henneberger/.m2/repository/com/ongres/stringprep/saslprep/1.1/saslprep-1.1.jar:/Users/henneberger/.m2/repository/com/ongres/stringprep/stringprep/1.1/stringprep-1.1.jar:/Users/henneberger/.m2/repository/com/ongres/scram/client/2.1/client-2.1.jar:/Users/henneberger/sqrl/sqrl-tools/sqrl-cli/target/classes:/Users/henneberger/.m2/repository/io/vertx/vertx-junit5/4.3.5/vertx-junit5-4.3.5.jar:/Users/henneberger/.m2/repository/io/vertx/vertx-core/4.3.5/vertx-core-4.3.5.jar:/Users/henneberger/.m2/repository/io/netty/netty-handler/4.1.85.Final/netty-handler-4.1.85.Final.jar:/Users/henneberger/.m2/repository/io/netty/netty-transport-native-unix-common/4.1.85.Final/netty-transport-native-unix-common-4.1.85.Final.jar:/Users/henneberger/.m2/repository/io/netty/netty-handler-proxy/4.1.85.Final/netty-handler-proxy-4.1.85.Final.jar:/Users/henneberger/.m2/repository/io/netty/netty-codec-socks/4.1.85.Final/netty-codec-socks-4.1.85.Final.jar:/Users/henneberger/.m2/repository/io/netty/netty-codec-http/4.1.85.Final/netty-codec-http-4.1.85.Final.jar:/Users/henneberger/.m2/repository/io/netty/netty-codec-http2/4.1.85.Final/netty-codec-http2-4.1.85.Final.jar:/Users/henneberger/.m2/repository/io/netty/netty-resolver/4.1.85.Final/netty-resolver-4.1.85.Final.jar:/Users/henneberger/.m2/repository/io/netty/netty-resolver-dns/4.1.85.Final/netty-resolver-dns-4.1.85.Final.jar:/Users/henneberger/.m2/repository/io/netty/netty-codec-dns/4.1.85.Final/netty-codec-dns-4.1.85.Final.jar:/Users/henneberger/.m2/repository/org/junit/jupiter/junit-jupiter-api/5.8.2/junit-jupiter-api-5.8.2.jar:/Users/henneberger/.m2/repository/org/opentest4j/opentest4j/1.2.0/opentest4j-1.2.0.jar:/Users/henneberger/.m2/repository/org/junit/platform/junit-platform-commons/1.8.2/junit-platform-commons-1.8.2.jar:/Users/henneberger/.m2/repository/io/vertx/vertx-web-client/4.3.5/vertx-web-client-4.3.5.jar:/Users/henneberger/.m2/repository/io/vertx/vertx-uri-template/4.3.5/vertx-uri-template-4.3.5.jar:/Users/henneberger/.m2/repository/io/vertx/vertx-web-common/4.3.5/vertx-web-common-4.3.5.jar:/Users/henneberger/.m2/repository/io/vertx/vertx-auth-common/4.3.5/vertx-auth-common-4.3.5.jar:/Users/henneberger/.m2/repository/org/testcontainers/testcontainers/1.18.0/testcontainers-1.18.0.jar:/Users/henneberger/.m2/repository/junit/junit/4.13.2/junit-4.13.2.jar:/Users/henneberger/.m2/repository/org/hamcrest/hamcrest-core/1.3/hamcrest-core-1.3.jar:/Users/henneberger/.m2/repository/org/slf4j/slf4j-api/1.7.36/slf4j-api-1.7.36.jar:/Users/henneberger/.m2/repository/org/apache/commons/commons-compress/1.22/commons-compress-1.22.jar:/Users/henneberger/.m2/repository/org/rnorth/duct-tape/duct-tape/1.0.8/duct-tape-1.0.8.jar:/Users/henneberger/.m2/repository/org/jetbrains/annotations/17.0.0/annotations-17.0.0.jar:/Users/henneberger/.m2/repository/com/github/docker-java/docker-java-api/3.3.0/docker-java-api-3.3.0.jar:/Users/henneberger/.m2/repository/com/github/docker-java/docker-java-transport-zerodep/3.3.0/docker-java-transport-zerodep-3.3.0.jar:/Users/henneberger/.m2/repository/com/github/docker-java/docker-java-transport/3.3.0/docker-java-transport-3.3.0.jar:/Users/henneberger/.m2/repository/net/java/dev/jna/jna/5.12.1/jna-5.12.1.jar:/Users/henneberger/.m2/repository/org/testcontainers/junit-jupiter/1.18.0/junit-jupiter-1.18.0.jar:/Users/henneberger/.m2/repository/org/testcontainers/postgresql/1.18.0/postgresql-1.18.0.jar:/Users/henneberger/.m2/repository/org/testcontainers/jdbc/1.18.0/jdbc-1.18.0.jar:/Users/henneberger/.m2/repository/org/testcontainers/database-commons/1.18.0/database-commons-1.18.0.jar:/Users/henneberger/.m2/repository/org/apache/kafka/kafka-clients/3.4.0/kafka-clients-3.4.0.jar:/Users/henneberger/.m2/repository/com/github/luben/zstd-jni/1.5.2-1/zstd-jni-1.5.2-1.jar:/Users/henneberger/.m2/repository/org/lz4/lz4-java/1.8.0/lz4-java-1.8.0.jar:/Users/henneberger/.m2/repository/org/xerial/snappy/snappy-java/1.1.8.4/snappy-java-1.1.8.4.jar:/Users/henneberger/.m2/repository/org/apache/kafka/kafka-clients/3.4.0/kafka-clients-3.4.0-test.jar:/Users/henneberger/.m2/repository/org/apache/kafka/kafka_2.12/3.4.0/kafka_2.12-3.4.0.jar:/Users/henneberger/.m2/repository/org/scala-lang/scala-library/2.12.15/scala-library-2.12.15.jar:/Users/henneberger/.m2/repository/org/apache/kafka/kafka-server-common/3.4.0/kafka-server-common-3.4.0.jar:/Users/henneberger/.m2/repository/org/apache/kafka/kafka-group-coordinator/3.4.0/kafka-group-coordinator-3.4.0.jar:/Users/henneberger/.m2/repository/org/apache/kafka/kafka-metadata/3.4.0/kafka-metadata-3.4.0.jar:/Users/henneberger/.m2/repository/org/apache/kafka/kafka-raft/3.4.0/kafka-raft-3.4.0.jar:/Users/henneberger/.m2/repository/org/apache/kafka/kafka-storage/3.4.0/kafka-storage-3.4.0.jar:/Users/henneberger/.m2/repository/org/apache/kafka/kafka-storage-api/3.4.0/kafka-storage-api-3.4.0.jar:/Users/henneberger/.m2/repository/net/sourceforge/argparse4j/argparse4j/0.7.0/argparse4j-0.7.0.jar:/Users/henneberger/.m2/repository/com/fasterxml/jackson/core/jackson-databind/2.14.0/jackson-databind-2.14.0.jar:/Users/henneberger/.m2/repository/com/fasterxml/jackson/module/jackson-module-scala_2.12/2.13.4/jackson-module-scala_2.12-2.13.4.jar:/Users/henneberger/.m2/repository/com/thoughtworks/paranamer/paranamer/2.8/paranamer-2.8.jar:/Users/henneberger/.m2/repository/com/fasterxml/jackson/dataformat/jackson-dataformat-csv/2.13.4/jackson-dataformat-csv-2.13.4.jar:/Users/henneberger/.m2/repository/com/fasterxml/jackson/datatype/jackson-datatype-jdk8/2.14.0/jackson-datatype-jdk8-2.14.0.jar:/Users/henneberger/.m2/repository/net/sf/jopt-simple/jopt-simple/5.0.4/jopt-simple-5.0.4.jar:/Users/henneberger/.m2/repository/org/bitbucket/b_c/jose4j/0.7.9/jose4j-0.7.9.jar:/Users/henneberger/.m2/repository/com/yammer/metrics/metrics-core/2.2.0/metrics-core-2.2.0.jar:/Users/henneberger/.m2/repository/org/scala-lang/modules/scala-collection-compat_2.12/2.6.0/scala-collection-compat_2.12-2.6.0.jar:/Users/henneberger/.m2/repository/org/scala-lang/modules/scala-java8-compat_2.12/1.0.2/scala-java8-compat_2.12-1.0.2.jar:/Users/henneberger/.m2/repository/org/scala-lang/scala-reflect/2.12.15/scala-reflect-2.12.15.jar:/Users/henneberger/.m2/repository/com/typesafe/scala-logging/scala-logging_2.12/3.9.4/scala-logging_2.12-3.9.4.jar:/Users/henneberger/.m2/repository/io/dropwizard/metrics/metrics-core/4.1.12.1/metrics-core-4.1.12.1.jar:/Users/henneberger/.m2/repository/org/apache/zookeeper/zookeeper/3.6.3/zookeeper-3.6.3.jar:/Users/henneberger/.m2/repository/org/apache/zookeeper/zookeeper-jute/3.6.3/zookeeper-jute-3.6.3.jar:/Users/henneberger/.m2/repository/org/apache/yetus/audience-annotations/0.5.0/audience-annotations-0.5.0.jar:/Users/henneberger/.m2/repository/io/netty/netty-transport-native-epoll/4.1.63.Final/netty-transport-native-epoll-4.1.63.Final.jar:/Users/henneberger/.m2/repository/commons-cli/commons-cli/1.4/commons-cli-1.4.jar:/Users/henneberger/.m2/repository/org/apache/kafka/kafka_2.12/3.4.0/kafka_2.12-3.4.0-test.jar:/Users/henneberger/.m2/repository/org/apache/kafka/kafka-streams/3.4.0/kafka-streams-3.4.0.jar:/Users/henneberger/.m2/repository/org/rocksdb/rocksdbjni/7.1.2/rocksdbjni-7.1.2.jar:/Users/henneberger/.m2/repository/com/fasterxml/jackson/core/jackson-annotations/2.14.0/jackson-annotations-2.14.0.jar:/Users/henneberger/.m2/repository/org/apache/kafka/kafka-streams/3.4.0/kafka-streams-3.4.0-test.jar:/Users/henneberger/.m2/repository/org/apache/kafka/kafka-streams-test-utils/3.4.0/kafka-streams-test-utils-3.4.0.jar:/Users/henneberger/sqrl/sqrl-planner/sqrl-common/target/test-classes:/Users/henneberger/.m2/repository/org/apache/flink/flink-table-planner_2.12/1.16.1/flink-table-planner_2.12-1.16.1.jar:/Users/henneberger/.m2/repository/org/codehaus/janino/commons-compiler/3.0.11/commons-compiler-3.0.11.jar:/Users/henneberger/.m2/repository/org/codehaus/janino/janino/3.0.11/janino-3.0.11.jar:/Users/henneberger/.m2/repository/org/apache/flink/flink-scala_2.12/1.16.1/flink-scala_2.12-1.16.1.jar:/Users/henneberger/.m2/repository/org/scala-lang/scala-compiler/2.12.7/scala-compiler-2.12.7.jar:/Users/henneberger/.m2/repository/org/scala-lang/modules/scala-xml_2.12/1.0.6/scala-xml_2.12-1.0.6.jar:/Users/henneberger/.m2/repository/com/twitter/chill_2.12/0.7.6/chill_2.12-0.7.6.jar:/Users/henneberger/.m2/repository/org/apache/flink/flink-table-runtime/1.16.1/flink-table-runtime-1.16.1.jar:/Users/henneberger/.m2/repository/org/apache/flink/flink-cep/1.16.1/flink-cep-1.16.1.jar:/Users/henneberger/.m2/repository/org/antlr/antlr4-runtime/4.9.2/antlr4-runtime-4.9.2.jar:/Users/henneberger/.m2/repository/commons-io/commons-io/2.11.0/commons-io-2.11.0.jar:/Users/henneberger/sqrl/sqrl-planner/sqrl-planner-local/target/test-classes:/Users/henneberger/sqrl/sqrl-engines/sqrl-engine-jdbc/target/classes:/Users/henneberger/sqrl/sqrl-engines/sqrl-engine-server/target/classes:/Users/henneberger/.m2/repository/com/google/inject/guice/5.1.0/guice-5.1.0.jar:/Users/henneberger/.m2/repository/javax/inject/javax.inject/1/javax.inject-1.jar:/Users/henneberger/.m2/repository/aopalliance/aopalliance/1.0/aopalliance-1.0.jar:/Users/henneberger/.m2/repository/org/glassfish/javax.el/3.0.0/javax.el-3.0.0.jar:/Users/henneberger/.m2/repository/org/projectlombok/lombok/1.18.24/lombok-1.18.24.jar:/Users/henneberger/.m2/repository/org/junit/jupiter/junit-jupiter-engine/5.8.2/junit-jupiter-engine-5.8.2.jar:/Users/henneberger/.m2/repository/org/junit/platform/junit-platform-engine/1.8.2/junit-platform-engine-1.8.2.jar:/Users/henneberger/.m2/repository/org/apiguardian/apiguardian-api/1.1.2/apiguardian-api-1.1.2.jar:/Users/henneberger/.m2/repository/org/junit/jupiter/junit-jupiter-params/5.8.2/junit-jupiter-params-5.8.2.jar com.intellij.rt.junit.JUnitStarter -ideVersion5 -junit5 com.datasqrl.flink.MutationSubscriptionTest
11:28:21.930 [main] INFO  org.testcontainers.utility.ImageNameSubstitutor - Image name substitution will be performed by: DefaultImageNameSubstitutor (composite of 'ConfigurationFileImageNameSubstitutor' and 'PrefixingImageNameSubstitutor')
11:28:22.254 [main] INFO  org.testcontainers.dockerclient.DockerClientProviderStrategy - Loaded org.testcontainers.dockerclient.UnixSocketClientProviderStrategy from ~/.testcontainers.properties, will try it first
11:28:22.789 [main] INFO  org.testcontainers.dockerclient.DockerClientProviderStrategy - Found Docker environment with local Unix socket (unix:///var/run/docker.sock)
11:28:22.790 [main] INFO  org.testcontainers.DockerClientFactory - Docker host IP address is localhost
11:28:22.812 [main] INFO  org.testcontainers.DockerClientFactory - Connected to docker:
  Server Version: 20.10.23
  API Version: 1.41
  Operating System: Docker Desktop
  Total Memory: 1989 MB
11:28:22.844 [main] INFO  tc.testcontainers/ryuk:0.4.0 - Creating container for image: testcontainers/ryuk:0.4.0
11:28:23.220 [main] INFO  tc.testcontainers/ryuk:0.4.0 - Container testcontainers/ryuk:0.4.0 is starting: a0ae193687e79977594b025a2ead21fb6063fe8056239fcddcabbb330dbf33b8
11:28:23.808 [main] INFO  tc.testcontainers/ryuk:0.4.0 - Container testcontainers/ryuk:0.4.0 started in PT0.987868S
11:28:23.829 [main] INFO  org.testcontainers.utility.RyukResourceReaper - Ryuk started - will monitor and terminate Testcontainers containers on JVM exit
11:28:23.830 [main] INFO  org.testcontainers.DockerClientFactory - Checking the system...
11:28:23.831 [main] INFO  org.testcontainers.DockerClientFactory - ✔︎ Docker server version should be at least 1.6.0
11:28:23.832 [main] INFO  tc.postgres:14.2 - Creating container for image: postgres:14.2
11:28:23.877 [main] INFO  tc.postgres:14.2 - Container postgres:14.2 is starting: cce2717b1fb04cb0641ab71f8fc8d2312e36693b2de317f4f6259ded346c43d8
11:28:25.241 [main] INFO  tc.postgres:14.2 - Container postgres:14.2 started in PT1.409666S
11:28:25.242 [main] INFO  tc.postgres:14.2 - Container is started (JDBC URL: jdbc:postgresql://localhost:51721/datasqrl?loggerLevel=OFF)
11:28:25.411 [main] WARN  io.netty.resolver.dns.DnsServerAddressStreamProviders - Can not find io.netty.resolver.dns.macos.MacOSDnsServerAddressStreamProvider in the classpath, fallback to system defaults. This may result in incorrect DNS resolutions on MacOS. Check whether you have a dependency on 'io.netty:netty-resolver-dns-native-macos'
11:28:25.442 [main] INFO  kafka.utils.Log4jControllerRegistration$ - Registered kafka:type=kafka.Log4jController MBean
11:28:25.469 [main] INFO  org.apache.zookeeper.server.ZooKeeperServer -
11:28:25.469 [main] INFO  org.apache.zookeeper.server.ZooKeeperServer -   ______                  _
11:28:25.469 [main] INFO  org.apache.zookeeper.server.ZooKeeperServer -  |___  /                 | |
11:28:25.469 [main] INFO  org.apache.zookeeper.server.ZooKeeperServer -     / /    ___     ___   | | __   ___    ___   _ __     ___   _ __
11:28:25.469 [main] INFO  org.apache.zookeeper.server.ZooKeeperServer -    / /    / _ \   / _ \  | |/ /  / _ \  / _ \ | '_ \   / _ \ | '__|
11:28:25.469 [main] INFO  org.apache.zookeeper.server.ZooKeeperServer -   / /__  | (_) | | (_) | |   <  |  __/ |  __/ | |_) | |  __/ | |
11:28:25.469 [main] INFO  org.apache.zookeeper.server.ZooKeeperServer -  /_____|  \___/   \___/  |_|\_\  \___|  \___| | .__/   \___| |_|
11:28:25.469 [main] INFO  org.apache.zookeeper.server.ZooKeeperServer -                                               | |
11:28:25.469 [main] INFO  org.apache.zookeeper.server.ZooKeeperServer -                                               |_|
11:28:25.469 [main] INFO  org.apache.zookeeper.server.ZooKeeperServer -
11:28:25.472 [main] INFO  org.apache.zookeeper.server.ZooKeeperServer - Server environment:zookeeper.version=3.6.3--6401e4ad2087061bc6b9f80dec2d69f2e3c8660a, built on 04/08/2021 16:35 GMT
11:28:25.472 [main] INFO  org.apache.zookeeper.server.ZooKeeperServer - Server environment:host.name=localhost
11:28:25.472 [main] INFO  org.apache.zookeeper.server.ZooKeeperServer - Server environment:java.version=11.0.16
11:28:25.472 [main] INFO  org.apache.zookeeper.server.ZooKeeperServer - Server environment:java.vendor=Oracle Corporation
11:28:25.472 [main] INFO  org.apache.zookeeper.server.ZooKeeperServer - Server environment:java.home=/Library/Java/JavaVirtualMachines/jdk-11.0.16.jdk/Contents/Home
11:28:25.472 [main] INFO  org.apache.zookeeper.server.ZooKeeperServer - Server environment:java.class.path=/Users/henneberger/.m2/repository/org/junit/platform/junit-platform-launcher/1.8.2/junit-platform-launcher-1.8.2.jar:/Users/henneberger/.m2/repository/org/junit/vintage/junit-vintage-engine/5.8.2/junit-vintage-engine-5.8.2.jar:/Applications/IntelliJ IDEA CE.app/Contents/lib/idea_rt.jar:/Applications/IntelliJ IDEA CE.app/Contents/plugins/junit/lib/junit5-rt.jar:/Applications/IntelliJ IDEA CE.app/Contents/plugins/junit/lib/junit-rt.jar:/Users/henneberger/sqrl/sqrl-testing/sqrl-integration-tests/target/test-classes:/Users/henneberger/sqrl/sqrl-testing/sqrl-integration-tests/target/classes:/Users/henneberger/sqrl/sqrl-io/sqrl-io-format-json/target/classes:/Users/henneberger/sqrl/sqrl-io/sqrl-io-core/target/classes:/Users/henneberger/.m2/repository/org/apache/commons/commons-csv/1.9.0/commons-csv-1.9.0.jar:/Users/henneberger/.m2/repository/org/apache/commons/commons-lang3/3.12.0/commons-lang3-3.12.0.jar:/Users/henneberger/sqrl/sqrl-io/sqrl-io-format-csv/target/classes:/Users/henneberger/sqrl/sqrl-planner/sqrl-planner-schema-flexible/target/classes:/Users/henneberger/sqrl/sqrl-planner/sqrl-common/target/classes:/Users/henneberger/sqrl/sqrl-engines/sqrl-engine-core/target/classes:/Users/henneberger/sqrl/sqrl-execute/sqrl-execute-flink/sqrl-execute-flink-function/target/classes:/Users/henneberger/sqrl/sqrl-io/sqrl-io-schema-flexible/target/classes:/Users/henneberger/sqrl/sqrl-tools/sqrl-discovery/target/classes:/Users/henneberger/sqrl/sqrl-engines/sqrl-engine-flink/target/classes:/Users/henneberger/.m2/repository/org/apache/flink/flink-core/1.16.1/flink-core-1.16.1.jar:/Users/henneberger/.m2/repository/org/apache/flink/flink-annotations/1.16.1/flink-annotations-1.16.1.jar:/Users/henneberger/.m2/repository/org/apache/flink/flink-metrics-core/1.16.1/flink-metrics-core-1.16.1.jar:/Users/henneberger/.m2/repository/org/apache/flink/flink-shaded-asm-9/9.2-15.0/flink-shaded-asm-9-9.2-15.0.jar:/Users/henneberger/.m2/repository/org/apache/flink/flink-shaded-jackson/2.12.4-15.0/flink-shaded-jackson-2.12.4-15.0.jar:/Users/henneberger/.m2/repository/commons-collections/commons-collections/3.2.2/commons-collections-3.2.2.jar:/Users/henneberger/.m2/repository/org/apache/flink/flink-shaded-guava/30.1.1-jre-15.0/flink-shaded-guava-30.1.1-jre-15.0.jar:/Users/henneberger/.m2/repository/org/apache/flink/flink-streaming-java/1.16.1/flink-streaming-java-1.16.1.jar:/Users/henneberger/.m2/repository/org/apache/flink/flink-runtime/1.16.1/flink-runtime-1.16.1.jar:/Users/henneberger/.m2/repository/org/apache/flink/flink-rpc-core/1.16.1/flink-rpc-core-1.16.1.jar:/Users/henneberger/.m2/repository/org/apache/flink/flink-rpc-akka-loader/1.16.1/flink-rpc-akka-loader-1.16.1.jar:/Users/henneberger/.m2/repository/org/apache/flink/flink-queryable-state-client-java/1.16.1/flink-queryable-state-client-java-1.16.1.jar:/Users/henneberger/.m2/repository/org/apache/flink/flink-hadoop-fs/1.16.1/flink-hadoop-fs-1.16.1.jar:/Users/henneberger/.m2/repository/org/apache/flink/flink-shaded-netty/4.1.70.Final-15.0/flink-shaded-netty-4.1.70.Final-15.0.jar:/Users/henneberger/.m2/repository/org/apache/flink/flink-shaded-zookeeper-3/3.5.9-15.0/flink-shaded-zookeeper-3-3.5.9-15.0.jar:/Users/henneberger/.m2/repository/org/javassist/javassist/3.24.0-GA/javassist-3.24.0-GA.jar:/Users/henneberger/.m2/repository/org/apache/commons/commons-math3/3.6.1/commons-math3-3.6.1.jar:/Users/henneberger/.m2/repository/org/apache/flink/flink-clients/1.16.1/flink-clients-1.16.1.jar:/Users/henneberger/.m2/repository/org/apache/flink/flink-optimizer/1.16.1/flink-optimizer-1.16.1.jar:/Users/henneberger/sqrl/sqrl-tools/sqrl-cli/target/test-classes:/Users/henneberger/sqrl/sqrl-base/target/classes:/Users/henneberger/.m2/repository/com/fasterxml/jackson/core/jackson-core/2.14.0/jackson-core-2.14.0.jar:/Users/henneberger/.m2/repository/com/fasterxml/jackson/module/jackson-module-blackbird/2.14.0/jackson-module-blackbird-2.14.0.jar:/Users/henneberger/.m2/repository/com/fasterxml/jackson/dataformat/jackson-dataformat-yaml/2.14.0/jackson-dataformat-yaml-2.14.0.jar:/Users/henneberger/.m2/repository/com/fasterxml/jackson/datatype/jackson-datatype-jsr310/2.14.0/jackson-datatype-jsr310-2.14.0.jar:/Users/henneberger/.m2/repository/com/google/guava/guava/31.1-jre/guava-31.1-jre.jar:/Users/henneberger/.m2/repository/com/google/guava/failureaccess/1.0.1/failureaccess-1.0.1.jar:/Users/henneberger/.m2/repository/com/google/guava/listenablefuture/9999.0-empty-to-avoid-conflict-with-guava/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/Users/henneberger/.m2/repository/com/google/errorprone/error_prone_annotations/2.11.0/error_prone_annotations-2.11.0.jar:/Users/henneberger/.m2/repository/com/google/j2objc/j2objc-annotations/1.3/j2objc-annotations-1.3.jar:/Users/henneberger/.m2/repository/com/google/auto/service/auto-service/1.0.1/auto-service-1.0.1.jar:/Users/henneberger/.m2/repository/com/google/auto/service/auto-service-annotations/1.0.1/auto-service-annotations-1.0.1.jar:/Users/henneberger/.m2/repository/com/google/auto/auto-common/1.2/auto-common-1.2.jar:/Users/henneberger/.m2/repository/org/apache/commons/commons-configuration2/2.9.0/commons-configuration2-2.9.0.jar:/Users/henneberger/.m2/repository/commons-logging/commons-logging/1.2/commons-logging-1.2.jar:/Users/henneberger/.m2/repository/commons-beanutils/commons-beanutils/1.9.4/commons-beanutils-1.9.4.jar:/Users/henneberger/.m2/repository/org/apache/logging/log4j/log4j-api/2.19.0/log4j-api-2.19.0.jar:/Users/henneberger/.m2/repository/org/apache/logging/log4j/log4j-core/2.19.0/log4j-core-2.19.0.jar:/Users/henneberger/.m2/repository/org/apache/logging/log4j/log4j-slf4j-impl/2.19.0/log4j-slf4j-impl-2.19.0.jar:/Users/henneberger/sqrl/sqrl-execute/sqrl-execute-http/sqrl-execute-http-aws-lambda/target/classes:/Users/henneberger/.m2/repository/com/amazonaws/aws-lambda-java-core/1.2.2/aws-lambda-java-core-1.2.2.jar:/Users/henneberger/.m2/repository/com/amazonaws/aws-lambda-java-events/3.11.0/aws-lambda-java-events-3.11.0.jar:/Users/henneberger/.m2/repository/joda-time/joda-time/2.6/joda-time-2.6.jar:/Users/henneberger/.m2/repository/ch/qos/logback/logback-classic/1.4.6/logback-classic-1.4.6.jar:/Users/henneberger/.m2/repository/ch/qos/logback/logback-core/1.4.6/logback-core-1.4.6.jar:/Users/henneberger/.m2/repository/org/jooq/jooq/3.16.12/jooq-3.16.12.jar:/Users/henneberger/.m2/repository/io/r2dbc/r2dbc-spi/0.9.0.RELEASE/r2dbc-spi-0.9.0.RELEASE.jar:/Users/henneberger/.m2/repository/jakarta/xml/bind/jakarta.xml.bind-api/3.0.0/jakarta.xml.bind-api-3.0.0.jar:/Users/henneberger/.m2/repository/com/sun/activation/jakarta.activation/2.0.0/jakarta.activation-2.0.0.jar:/Users/henneberger/sqrl/sqrl-io/sqrl-io-jdbc/target/classes:/Users/henneberger/.m2/repository/com/esotericsoftware/kryo/kryo/2.24.0/kryo-2.24.0.jar:/Users/henneberger/.m2/repository/com/esotericsoftware/minlog/minlog/1.2/minlog-1.2.jar:/Users/henneberger/.m2/repository/org/objenesis/objenesis/2.1/objenesis-2.1.jar:/Users/henneberger/sqrl/sqrl-engines/sqrl-engine-kafka/target/classes:/Users/henneberger/sqrl/sqrl-io/sqrl-io-file/target/classes:/Users/henneberger/.m2/repository/org/apache/flink/flink-connector-files/1.16.1/flink-connector-files-1.16.1.jar:/Users/henneberger/sqrl/sqrl-io/sqrl-io-kafka/target/classes:/Users/henneberger/sqrl/sqrl-io/sqrl-io-print/target/classes:/Users/henneberger/sqrl/sqrl-execute/sqrl-execute-flink/sqrl-execute-flink-connectors/sqrl-execute-flink-postgres/target/classes:/Users/henneberger/.m2/repository/info/picocli/picocli/4.7.4/picocli-4.7.4.jar:/Users/henneberger/sqrl/sqrl-execute/sqrl-execute-flink/sqrl-execute-flink-connectors/sqrl-execute-flink-inmem/target/classes:/Users/henneberger/sqrl/sqrl-io/sqrl-io-mem/target/classes:/Users/henneberger/.m2/repository/org/apache/flink/flink-file-sink-common/1.16.1/flink-file-sink-common-1.16.1.jar:/Users/henneberger/.m2/repository/org/apache/flink/flink-shaded-force-shading/15.0/flink-shaded-force-shading-15.0.jar:/Users/henneberger/sqrl/sqrl-execute/sqrl-execute-flink/sqrl-execute-flink-core/target/classes:/Users/henneberger/.m2/repository/org/apache/flink/flink-json/1.16.1/flink-json-1.16.1.jar:/Users/henneberger/.m2/repository/com/google/code/findbugs/jsr305/1.3.9/jsr305-1.3.9.jar:/Users/henneberger/.m2/repository/com/opencsv/opencsv/5.7.1/opencsv-5.7.1.jar:/Users/henneberger/.m2/repository/org/apache/commons/commons-text/1.10.0/commons-text-1.10.0.jar:/Users/henneberger/.m2/repository/org/apache/flink/flink-table-api-java-bridge/1.16.1/flink-table-api-java-bridge-1.16.1.jar:/Users/henneberger/.m2/repository/org/apache/flink/flink-table-api-java/1.16.1/flink-table-api-java-1.16.1.jar:/Users/henneberger/.m2/repository/org/apache/flink/flink-table-common/1.16.1/flink-table-common-1.16.1.jar:/Users/henneberger/.m2/repository/com/ibm/icu/icu4j/67.1/icu4j-67.1.jar:/Users/henneberger/.m2/repository/org/apache/flink/flink-table-api-bridge-base/1.16.1/flink-table-api-bridge-base-1.16.1.jar:/Users/henneberger/.m2/repository/org/apache/flink/flink-java/1.16.1/flink-java-1.16.1.jar:/Users/henneberger/.m2/repository/com/twitter/chill-java/0.7.6/chill-java-0.7.6.jar:/Users/henneberger/sqrl/sqrl-execute/sqrl-execute-flink/sqrl-execute-flink-connectors/sqrl-execute-flink-jdbc/target/classes:/Users/henneberger/.m2/repository/org/apache/flink/flink-connector-jdbc/1.16.1/flink-connector-jdbc-1.16.1.jar:/Users/henneberger/sqrl/sqrl-execute/sqrl-execute-flink/sqrl-execute-flink-connectors/sqrl-execute-flink-h2/target/classes:/Users/henneberger/.m2/repository/com/h2database/h2/2.1.214/h2-2.1.214.jar:/Users/henneberger/sqrl/sqrl-execute/sqrl-execute-flink/sqrl-execute-flink-connectors/sqrl-execute-flink-file/target/classes:/Users/henneberger/.m2/repository/org/apache/flink/flink-s3-fs-hadoop/1.16.1/flink-s3-fs-hadoop-1.16.1.jar:/Users/henneberger/sqrl/sqrl-execute/sqrl-execute-flink/sqrl-execute-flink-connectors/sqrl-execute-flink-kafka/target/classes:/Users/henneberger/.m2/repository/org/apache/flink/flink-connector-kafka/1.16.1/flink-connector-kafka-1.16.1.jar:/Users/henneberger/.m2/repository/org/apache/flink/flink-connector-base/1.16.1/flink-connector-base-1.16.1.jar:/Users/henneberger/sqrl/sqrl-execute/sqrl-execute-flink/sqrl-execute-flink-connectors/sqrl-execute-flink-print/target/classes:/Users/henneberger/sqrl/sqrl-execute/sqrl-execute-flink/sqrl-execute-flink-connectors/sqrl-execute-flink-sqlite/target/classes:/Users/henneberger/.m2/repository/org/xerial/sqlite-jdbc/3.40.0.0/sqlite-jdbc-3.40.0.0.jar:/Users/henneberger/sqrl/sqrl-tools/sqrl-packager/target/classes:/Users/henneberger/sqrl/sqrl-planner/sqrl-planner-local/target/classes:/Users/henneberger/.m2/repository/commons-codec/commons-codec/1.15/commons-codec-1.15.jar:/Users/henneberger/.m2/repository/org/apache/commons/commons-collections4/4.4/commons-collections4-4.4.jar:/Users/henneberger/.m2/repository/net/lingala/zip4j/zip4j/2.11.2/zip4j-2.11.2.jar:/Users/henneberger/sqrl/sqrl-execute/sqrl-execute-http/sqrl-execute-http-vertx/target/classes:/Users/henneberger/sqrl/sqrl-execute/sqrl-execute-http/sqrl-execute-http-core/target/classes:/Users/henneberger/.m2/repository/com/graphql-java/graphql-java/19.2/graphql-java-19.2.jar:/Users/henneberger/.m2/repository/com/graphql-java/java-dataloader/3.2.0/java-dataloader-3.2.0.jar:/Users/henneberger/.m2/repository/io/vertx/vertx-config/4.3.5/vertx-config-4.3.5.jar:/Users/henneberger/.m2/repository/io/vertx/vertx-web-openapi/4.3.5/vertx-web-openapi-4.3.5.jar:/Users/henneberger/.m2/repository/io/vertx/vertx-web-validation/4.3.5/vertx-web-validation-4.3.5.jar:/Users/henneberger/.m2/repository/io/vertx/vertx-json-schema/4.3.5/vertx-json-schema-4.3.5.jar:/Users/henneberger/.m2/repository/org/yaml/snakeyaml/1.32/snakeyaml-1.32.jar:/Users/henneberger/.m2/repository/io/vertx/vertx-jdbc-client/4.3.5/vertx-jdbc-client-4.3.5.jar:/Users/henneberger/.m2/repository/io/vertx/vertx-sql-client/4.3.5/vertx-sql-client-4.3.5.jar:/Users/henneberger/.m2/repository/com/mchange/c3p0/0.9.5.5/c3p0-0.9.5.5.jar:/Users/henneberger/.m2/repository/com/mchange/mchange-commons-java/0.2.19/mchange-commons-java-0.2.19.jar:/Users/henneberger/.m2/repository/io/vertx/vertx-web/4.3.5/vertx-web-4.3.5.jar:/Users/henneberger/.m2/repository/io/vertx/vertx-bridge-common/4.3.5/vertx-bridge-common-4.3.5.jar:/Users/henneberger/.m2/repository/io/vertx/vertx-web-graphql/4.3.5/vertx-web-graphql-4.3.5.jar:/Users/henneberger/.m2/repository/io/vertx/vertx-pg-client/4.3.5/vertx-pg-client-4.3.5.jar:/Users/henneberger/.m2/repository/io/agroal/agroal-pool/1.11/agroal-pool-1.11.jar:/Users/henneberger/.m2/repository/io/agroal/agroal-api/1.11/agroal-api-1.11.jar:/Users/henneberger/.m2/repository/org/reactivestreams/reactive-streams/1.0.4/reactive-streams-1.0.4.jar:/Users/henneberger/.m2/repository/io/vertx/vertx-kafka-client/4.3.5/vertx-kafka-client-4.3.5.jar:/Users/henneberger/.m2/repository/io/projectreactor/reactor-core/3.5.6/reactor-core-3.5.6.jar:/Users/henneberger/.m2/repository/io/netty/netty-codec/4.1.92.Final/netty-codec-4.1.92.Final.jar:/Users/henneberger/.m2/repository/io/netty/netty-common/4.1.92.Final/netty-common-4.1.92.Final.jar:/Users/henneberger/.m2/repository/io/netty/netty-buffer/4.1.92.Final/netty-buffer-4.1.92.Final.jar:/Users/henneberger/.m2/repository/io/netty/netty-transport/4.1.92.Final/netty-transport-4.1.92.Final.jar:/Users/henneberger/.m2/repository/org/postgresql/postgresql/42.5.0/postgresql-42.5.0.jar:/Users/henneberger/.m2/repository/org/checkerframework/checker-qual/3.5.0/checker-qual-3.5.0.jar:/Users/henneberger/.m2/repository/com/ongres/scram/common/2.1/common-2.1.jar:/Users/henneberger/.m2/repository/com/ongres/stringprep/saslprep/1.1/saslprep-1.1.jar:/Users/henneberger/.m2/repository/com/ongres/stringprep/stringprep/1.1/stringprep-1.1.jar:/Users/henneberger/.m2/repository/com/ongres/scram/client/2.1/client-2.1.jar:/Users/henneberger/sqrl/sqrl-tools/sqrl-cli/target/classes:/Users/henneberger/.m2/repository/io/vertx/vertx-junit5/4.3.5/vertx-junit5-4.3.5.jar:/Users/henneberger/.m2/repository/io/vertx/vertx-core/4.3.5/vertx-core-4.3.5.jar:/Users/henneberger/.m2/repository/io/netty/netty-handler/4.1.85.Final/netty-handler-4.1.85.Final.jar:/Users/henneberger/.m2/repository/io/netty/netty-transport-native-unix-common/4.1.85.Final/netty-transport-native-unix-common-4.1.85.Final.jar:/Users/henneberger/.m2/repository/io/netty/netty-handler-proxy/4.1.85.Final/netty-handler-proxy-4.1.85.Final.jar:/Users/henneberger/.m2/repository/io/netty/netty-codec-socks/4.1.85.Final/netty-codec-socks-4.1.85.Final.jar:/Users/henneberger/.m2/repository/io/netty/netty-codec-http/4.1.85.Final/netty-codec-http-4.1.85.Final.jar:/Users/henneberger/.m2/repository/io/netty/netty-codec-http2/4.1.85.Final/netty-codec-http2-4.1.85.Final.jar:/Users/henneberger/.m2/repository/io/netty/netty-resolver/4.1.85.Final/netty-resolver-4.1.85.Final.jar:/Users/henneberger/.m2/repository/io/netty/netty-resolver-dns/4.1.85.Final/netty-resolver-dns-4.1.85.Final.jar:/Users/henneberger/.m2/repository/io/netty/netty-codec-dns/4.1.85.Final/netty-codec-dns-4.1.85.Final.jar:/Users/henneberger/.m2/repository/org/junit/jupiter/junit-jupiter-api/5.8.2/junit-jupiter-api-5.8.2.jar:/Users/henneberger/.m2/repository/org/opentest4j/opentest4j/1.2.0/opentest4j-1.2.0.jar:/Users/henneberger/.m2/repository/org/junit/platform/junit-platform-commons/1.8.2/junit-platform-commons-1.8.2.jar:/Users/henneberger/.m2/repository/io/vertx/vertx-web-client/4.3.5/vertx-web-client-4.3.5.jar:/Users/henneberger/.m2/repository/io/vertx/vertx-uri-template/4.3.5/vertx-uri-template-4.3.5.jar:/Users/henneberger/.m2/repository/io/vertx/vertx-web-common/4.3.5/vertx-web-common-4.3.5.jar:/Users/henneberger/.m2/repository/io/vertx/vertx-auth-common/4.3.5/vertx-auth-common-4.3.5.jar:/Users/henneberger/.m2/repository/org/testcontainers/testcontainers/1.18.0/testcontainers-1.18.0.jar:/Users/henneberger/.m2/repository/junit/junit/4.13.2/junit-4.13.2.jar:/Users/henneberger/.m2/repository/org/hamcrest/hamcrest-core/1.3/hamcrest-core-1.3.jar:/Users/henneberger/.m2/repository/org/slf4j/slf4j-api/1.7.36/slf4j-api-1.7.36.jar:/Users/henneberger/.m2/repository/org/apache/commons/commons-compress/1.22/commons-compress-1.22.jar:/Users/henneberger/.m2/repository/org/rnorth/duct-tape/duct-tape/1.0.8/duct-tape-1.0.8.jar:/Users/henneberger/.m2/repository/org/jetbrains/annotations/17.0.0/annotations-17.0.0.jar:/Users/henneberger/.m2/repository/com/github/docker-java/docker-java-api/3.3.0/docker-java-api-3.3.0.jar:/Users/henneberger/.m2/repository/com/github/docker-java/docker-java-transport-zerodep/3.3.0/docker-java-transport-zerodep-3.3.0.jar:/Users/henneberger/.m2/repository/com/github/docker-java/docker-java-transport/3.3.0/docker-java-transport-3.3.0.jar:/Users/henneberger/.m2/repository/net/java/dev/jna/jna/5.12.1/jna-5.12.1.jar:/Users/henneberger/.m2/repository/org/testcontainers/junit-jupiter/1.18.0/junit-jupiter-1.18.0.jar:/Users/henneberger/.m2/repository/org/testcontainers/postgresql/1.18.0/postgresql-1.18.0.jar:/Users/henneberger/.m2/repository/org/testcontainers/jdbc/1.18.0/jdbc-1.18.0.jar:/Users/henneberger/.m2/repository/org/testcontainers/database-commons/1.18.0/database-commons-1.18.0.jar:/Users/henneberger/.m2/repository/org/apache/kafka/kafka-clients/3.4.0/kafka-clients-3.4.0.jar:/Users/henneberger/.m2/repository/com/github/luben/zstd-jni/1.5.2-1/zstd-jni-1.5.2-1.jar:/Users/henneberger/.m2/repository/org/lz4/lz4-java/1.8.0/lz4-java-1.8.0.jar:/Users/henneberger/.m2/repository/org/xerial/snappy/snappy-java/1.1.8.4/snappy-java-1.1.8.4.jar:/Users/henneberger/.m2/repository/org/apache/kafka/kafka-clients/3.4.0/kafka-clients-3.4.0-test.jar:/Users/henneberger/.m2/repository/org/apache/kafka/kafka_2.12/3.4.0/kafka_2.12-3.4.0.jar:/Users/henneberger/.m2/repository/org/scala-lang/scala-library/2.12.15/scala-library-2.12.15.jar:/Users/henneberger/.m2/repository/org/apache/kafka/kafka-server-common/3.4.0/kafka-server-common-3.4.0.jar:/Users/henneberger/.m2/repository/org/apache/kafka/kafka-group-coordinator/3.4.0/kafka-group-coordinator-3.4.0.jar:/Users/henneberger/.m2/repository/org/apache/kafka/kafka-metadata/3.4.0/kafka-metadata-3.4.0.jar:/Users/henneberger/.m2/repository/org/apache/kafka/kafka-raft/3.4.0/kafka-raft-3.4.0.jar:/Users/henneberger/.m2/repository/org/apache/kafka/kafka-storage/3.4.0/kafka-storage-3.4.0.jar:/Users/henneberger/.m2/repository/org/apache/kafka/kafka-storage-api/3.4.0/kafka-storage-api-3.4.0.jar:/Users/henneberger/.m2/repository/net/sourceforge/argparse4j/argparse4j/0.7.0/argparse4j-0.7.0.jar:/Users/henneberger/.m2/repository/com/fasterxml/jackson/core/jackson-databind/2.14.0/jackson-databind-2.14.0.jar:/Users/henneberger/.m2/repository/com/fasterxml/jackson/module/jackson-module-scala_2.12/2.13.4/jackson-module-scala_2.12-2.13.4.jar:/Users/henneberger/.m2/repository/com/thoughtworks/paranamer/paranamer/2.8/paranamer-2.8.jar:/Users/henneberger/.m2/repository/com/fasterxml/jackson/dataformat/jackson-dataformat-csv/2.13.4/jackson-dataformat-csv-2.13.4.jar:/Users/henneberger/.m2/repository/com/fasterxml/jackson/datatype/jackson-datatype-jdk8/2.14.0/jackson-datatype-jdk8-2.14.0.jar:/Users/henneberger/.m2/repository/net/sf/jopt-simple/jopt-simple/5.0.4/jopt-simple-5.0.4.jar:/Users/henneberger/.m2/repository/org/bitbucket/b_c/jose4j/0.7.9/jose4j-0.7.9.jar:/Users/henneberger/.m2/repository/com/yammer/metrics/metrics-core/2.2.0/metrics-core-2.2.0.jar:/Users/henneberger/.m2/repository/org/scala-lang/modules/scala-collection-compat_2.12/2.6.0/scala-collection-compat_2.12-2.6.0.jar:/Users/henneberger/.m2/repository/org/scala-lang/modules/scala-java8-compat_2.12/1.0.2/scala-java8-compat_2.12-1.0.2.jar:/Users/henneberger/.m2/repository/org/scala-lang/scala-reflect/2.12.15/scala-reflect-2.12.15.jar:/Users/henneberger/.m2/repository/com/typesafe/scala-logging/scala-logging_2.12/3.9.4/scala-logging_2.12-3.9.4.jar:/Users/henneberger/.m2/repository/io/dropwizard/metrics/metrics-core/4.1.12.1/metrics-core-4.1.12.1.jar:/Users/henneberger/.m2/repository/org/apache/zookeeper/zookeeper/3.6.3/zookeeper-3.6.3.jar:/Users/henneberger/.m2/repository/org/apache/zookeeper/zookeeper-jute/3.6.3/zookeeper-jute-3.6.3.jar:/Users/henneberger/.m2/repository/org/apache/yetus/audience-annotations/0.5.0/audience-annotations-0.5.0.jar:/Users/henneberger/.m2/repository/io/netty/netty-transport-native-epoll/4.1.63.Final/netty-transport-native-epoll-4.1.63.Final.jar:/Users/henneberger/.m2/repository/commons-cli/commons-cli/1.4/commons-cli-1.4.jar:/Users/henneberger/.m2/repository/org/apache/kafka/kafka_2.12/3.4.0/kafka_2.12-3.4.0-test.jar:/Users/henneberger/.m2/repository/org/apache/kafka/kafka-streams/3.4.0/kafka-streams-3.4.0.jar:/Users/henneberger/.m2/repository/org/rocksdb/rocksdbjni/7.1.2/rocksdbjni-7.1.2.jar:/Users/henneberger/.m2/repository/com/fasterxml/jackson/core/jackson-annotations/2.14.0/jackson-annotations-2.14.0.jar:/Users/henneberger/.m2/repository/org/apache/kafka/kafka-streams/3.4.0/kafka-streams-3.4.0-test.jar:/Users/henneberger/.m2/repository/org/apache/kafka/kafka-streams-test-utils/3.4.0/kafka-streams-test-utils-3.4.0.jar:/Users/henneberger/sqrl/sqrl-planner/sqrl-common/target/test-classes:/Users/henneberger/.m2/repository/org/apache/flink/flink-table-planner_2.12/1.16.1/flink-table-planner_2.12-1.16.1.jar:/Users/henneberger/.m2/repository/org/codehaus/janino/commons-compiler/3.0.11/commons-compiler-3.0.11.jar:/Users/henneberger/.m2/repository/org/codehaus/janino/janino/3.0.11/janino-3.0.11.jar:/Users/henneberger/.m2/repository/org/apache/flink/flink-scala_2.12/1.16.1/flink-scala_2.12-1.16.1.jar:/Users/henneberger/.m2/repository/org/scala-lang/scala-compiler/2.12.7/scala-compiler-2.12.7.jar:/Users/henneberger/.m2/repository/org/scala-lang/modules/scala-xml_2.12/1.0.6/scala-xml_2.12-1.0.6.jar:/Users/henneberger/.m2/repository/com/twitter/chill_2.12/0.7.6/chill_2.12-0.7.6.jar:/Users/henneberger/.m2/repository/org/apache/flink/flink-table-runtime/1.16.1/flink-table-runtime-1.16.1.jar:/Users/henneberger/.m2/repository/org/apache/flink/flink-cep/1.16.1/flink-cep-1.16.1.jar:/Users/henneberger/.m2/repository/org/antlr/antlr4-runtime/4.9.2/antlr4-runtime-4.9.2.jar:/Users/henneberger/.m2/repository/commons-io/commons-io/2.11.0/commons-io-2.11.0.jar:/Users/henneberger/sqrl/sqrl-planner/sqrl-planner-local/target/test-classes:/Users/henneberger/sqrl/sqrl-engines/sqrl-engine-jdbc/target/classes:/Users/henneberger/sqrl/sqrl-engines/sqrl-engine-server/target/classes:/Users/henneberger/.m2/repository/com/google/inject/guice/5.1.0/guice-5.1.0.jar:/Users/henneberger/.m2/repository/javax/inject/javax.inject/1/javax.inject-1.jar:/Users/henneberger/.m2/repository/aopalliance/aopalliance/1.0/aopalliance-1.0.jar:/Users/henneberger/.m2/repository/org/glassfish/javax.el/3.0.0/javax.el-3.0.0.jar:/Users/henneberger/.m2/repository/org/projectlombok/lombok/1.18.24/lombok-1.18.24.jar:/Users/henneberger/.m2/repository/org/junit/jupiter/junit-jupiter-engine/5.8.2/junit-jupiter-engine-5.8.2.jar:/Users/henneberger/.m2/repository/org/junit/platform/junit-platform-engine/1.8.2/junit-platform-engine-1.8.2.jar:/Users/henneberger/.m2/repository/org/apiguardian/apiguardian-api/1.1.2/apiguardian-api-1.1.2.jar:/Users/henneberger/.m2/repository/org/junit/jupiter/junit-jupiter-params/5.8.2/junit-jupiter-params-5.8.2.jar
11:28:25.473 [main] INFO  org.apache.zookeeper.server.ZooKeeperServer - Server environment:java.library.path=/Users/henneberger/Library/Java/Extensions:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java:.
11:28:25.473 [main] INFO  org.apache.zookeeper.server.ZooKeeperServer - Server environment:java.io.tmpdir=/var/folders/w3/dxm786y50_b8p5tj7xktkngh0000gn/T/
11:28:25.473 [main] INFO  org.apache.zookeeper.server.ZooKeeperServer - Server environment:java.compiler=<NA>
11:28:25.473 [main] INFO  org.apache.zookeeper.server.ZooKeeperServer - Server environment:os.name=Mac OS X
11:28:25.473 [main] INFO  org.apache.zookeeper.server.ZooKeeperServer - Server environment:os.arch=aarch64
11:28:25.473 [main] INFO  org.apache.zookeeper.server.ZooKeeperServer - Server environment:os.version=13.4
11:28:25.473 [main] INFO  org.apache.zookeeper.server.ZooKeeperServer - Server environment:user.name=henneberger
11:28:25.473 [main] INFO  org.apache.zookeeper.server.ZooKeeperServer - Server environment:user.home=/Users/henneberger
11:28:25.473 [main] INFO  org.apache.zookeeper.server.ZooKeeperServer - Server environment:user.dir=/Users/henneberger/sqrl/sqrl-testing/sqrl-integration-tests
11:28:25.473 [main] INFO  org.apache.zookeeper.server.ZooKeeperServer - Server environment:os.memory.free=114MB
11:28:25.473 [main] INFO  org.apache.zookeeper.server.ZooKeeperServer - Server environment:os.memory.max=2048MB
11:28:25.473 [main] INFO  org.apache.zookeeper.server.ZooKeeperServer - Server environment:os.memory.total=154MB
11:28:25.473 [main] INFO  org.apache.zookeeper.server.ZooKeeperServer - zookeeper.enableEagerACLCheck = false
11:28:25.473 [main] INFO  org.apache.zookeeper.server.ZooKeeperServer - zookeeper.digest.enabled = true
11:28:25.473 [main] INFO  org.apache.zookeeper.server.ZooKeeperServer - zookeeper.closeSessionTxn.enabled = true
11:28:25.473 [main] INFO  org.apache.zookeeper.server.ZooKeeperServer - zookeeper.flushDelay=0
11:28:25.473 [main] INFO  org.apache.zookeeper.server.ZooKeeperServer - zookeeper.maxWriteQueuePollTime=0
11:28:25.473 [main] INFO  org.apache.zookeeper.server.ZooKeeperServer - zookeeper.maxBatchSize=1000
11:28:25.473 [main] INFO  org.apache.zookeeper.server.ZooKeeperServer - zookeeper.intBufferStartingSizeBytes = 1024
11:28:25.476 [main] INFO  org.apache.zookeeper.server.persistence.FileTxnSnapLog - zookeeper.snapshot.trust.empty : false
11:28:25.488 [main] INFO  org.apache.zookeeper.server.watch.WatchManagerFactory - Using org.apache.zookeeper.server.watch.WatchManager as watch manager
11:28:25.488 [main] INFO  org.apache.zookeeper.server.watch.WatchManagerFactory - Using org.apache.zookeeper.server.watch.WatchManager as watch manager
11:28:25.488 [main] INFO  org.apache.zookeeper.server.ZKDatabase - zookeeper.snapshotSizeFactor = 0.33
11:28:25.488 [main] INFO  org.apache.zookeeper.server.ZKDatabase - zookeeper.commitLogCount=500
11:28:25.491 [main] INFO  org.apache.zookeeper.server.BlueThrottle - Weighed connection throttling is disabled
11:28:25.492 [main] INFO  org.apache.zookeeper.server.ZooKeeperServer - minSessionTimeout set to 1600
11:28:25.492 [main] INFO  org.apache.zookeeper.server.ZooKeeperServer - maxSessionTimeout set to 16000
11:28:25.493 [main] INFO  org.apache.zookeeper.server.ResponseCache - Response cache size is initialized with value 400.
11:28:25.493 [main] INFO  org.apache.zookeeper.server.ResponseCache - Response cache size is initialized with value 400.
11:28:25.494 [main] INFO  org.apache.zookeeper.server.util.RequestPathMetricsCollector - zookeeper.pathStats.slotCapacity = 60
11:28:25.494 [main] INFO  org.apache.zookeeper.server.util.RequestPathMetricsCollector - zookeeper.pathStats.slotDuration = 15
11:28:25.494 [main] INFO  org.apache.zookeeper.server.util.RequestPathMetricsCollector - zookeeper.pathStats.maxDepth = 6
11:28:25.494 [main] INFO  org.apache.zookeeper.server.util.RequestPathMetricsCollector - zookeeper.pathStats.initialDelay = 5
11:28:25.494 [main] INFO  org.apache.zookeeper.server.util.RequestPathMetricsCollector - zookeeper.pathStats.delay = 5
11:28:25.494 [main] INFO  org.apache.zookeeper.server.util.RequestPathMetricsCollector - zookeeper.pathStats.enabled = false
11:28:25.495 [main] INFO  org.apache.zookeeper.server.ZooKeeperServer - The max bytes for all large requests are set to 104857600
11:28:25.495 [main] INFO  org.apache.zookeeper.server.ZooKeeperServer - The large request threshold is set to -1
11:28:25.495 [main] INFO  org.apache.zookeeper.server.ZooKeeperServer - Created server with tickTime 800 minSessionTimeout 1600 maxSessionTimeout 16000 clientPortListenBacklog -1 datadir /var/folders/w3/dxm786y50_b8p5tj7xktkngh0000gn/T/kafka-17437573507719402825/version-2 snapdir /var/folders/w3/dxm786y50_b8p5tj7xktkngh0000gn/T/kafka-14698998962791312343/version-2
11:28:25.502 [main] WARN  org.apache.zookeeper.server.ServerCnxnFactory - maxCnxns is not configured, using default value 0.
11:28:25.503 [main] INFO  org.apache.zookeeper.server.NIOServerCnxnFactory - Configuring NIO connection handler with 10s sessionless connection timeout, 2 selector thread(s), 16 worker threads, and 64 kB direct buffers.
11:28:25.511 [main] INFO  org.apache.zookeeper.server.NIOServerCnxnFactory - binding to port /127.0.0.1:0
11:28:25.528 [main] INFO  org.apache.zookeeper.server.persistence.SnapStream - zookeeper.snapshot.compression.method = CHECKED
11:28:25.528 [main] INFO  org.apache.zookeeper.server.persistence.FileTxnSnapLog - Snapshotting: 0x0 to /var/folders/w3/dxm786y50_b8p5tj7xktkngh0000gn/T/kafka-14698998962791312343/version-2/snapshot.0
11:28:25.531 [main] INFO  org.apache.zookeeper.server.ZKDatabase - Snapshot loaded in 16 ms, highest zxid is 0x0, digest is 1371985504
11:28:25.531 [main] INFO  org.apache.zookeeper.server.persistence.FileTxnSnapLog - Snapshotting: 0x0 to /var/folders/w3/dxm786y50_b8p5tj7xktkngh0000gn/T/kafka-14698998962791312343/version-2/snapshot.0
11:28:25.531 [main] INFO  org.apache.zookeeper.server.ZooKeeperServer - Snapshot taken in 0 ms
11:28:25.540 [ProcessThread(sid:0 cport:51722):] INFO  org.apache.zookeeper.server.PrepRequestProcessor - PrepRequestProcessor (sid:0) started, reconfigEnabled=false
11:28:25.541 [main] INFO  org.apache.zookeeper.server.RequestThrottler - zookeeper.request_throttler.shutdownTimeout = 10000
11:28:25.727 [main] INFO  kafka.server.KafkaConfig - KafkaConfig values:
	advertised.listeners = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name =
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = null
	controller.quorum.append.linger.ms = 25
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = []
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 0
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = null
	inter.broker.protocol.version = 3.4-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = PLAINTEXT://localhost:0
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 2097152
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /var/folders/w3/dxm786y50_b8p5tj7xktkngh0000gn/T/kafka-5258867200498972615/log7884909346953798293
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides =
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000000
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 0
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 5
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = []
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.manager.class.name = null
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = null
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = null
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 5
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 127.0.0.1:51722
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.session.timeout.ms = 10000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null

11:28:25.746 [main] INFO  org.apache.zookeeper.common.X509Util - Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation
11:28:25.803 [main] INFO  kafka.server.KafkaServer - starting
11:28:25.803 [main] INFO  kafka.server.KafkaServer - Connecting to zookeeper on 127.0.0.1:51722
11:28:25.814 [main] INFO  kafka.zookeeper.ZooKeeperClient - [ZooKeeperClient Kafka server] Initializing a new session to 127.0.0.1:51722.
11:28:25.817 [main] INFO  org.apache.zookeeper.ZooKeeper - Client environment:zookeeper.version=3.6.3--6401e4ad2087061bc6b9f80dec2d69f2e3c8660a, built on 04/08/2021 16:35 GMT
11:28:25.817 [main] INFO  org.apache.zookeeper.ZooKeeper - Client environment:host.name=localhost
11:28:25.817 [main] INFO  org.apache.zookeeper.ZooKeeper - Client environment:java.version=11.0.16
11:28:25.817 [main] INFO  org.apache.zookeeper.ZooKeeper - Client environment:java.vendor=Oracle Corporation
11:28:25.817 [main] INFO  org.apache.zookeeper.ZooKeeper - Client environment:java.home=/Library/Java/JavaVirtualMachines/jdk-11.0.16.jdk/Contents/Home
11:28:25.817 [main] INFO  org.apache.zookeeper.ZooKeeper - Client environment:java.class.path=/Users/henneberger/.m2/repository/org/junit/platform/junit-platform-launcher/1.8.2/junit-platform-launcher-1.8.2.jar:/Users/henneberger/.m2/repository/org/junit/vintage/junit-vintage-engine/5.8.2/junit-vintage-engine-5.8.2.jar:/Applications/IntelliJ IDEA CE.app/Contents/lib/idea_rt.jar:/Applications/IntelliJ IDEA CE.app/Contents/plugins/junit/lib/junit5-rt.jar:/Applications/IntelliJ IDEA CE.app/Contents/plugins/junit/lib/junit-rt.jar:/Users/henneberger/sqrl/sqrl-testing/sqrl-integration-tests/target/test-classes:/Users/henneberger/sqrl/sqrl-testing/sqrl-integration-tests/target/classes:/Users/henneberger/sqrl/sqrl-io/sqrl-io-format-json/target/classes:/Users/henneberger/sqrl/sqrl-io/sqrl-io-core/target/classes:/Users/henneberger/.m2/repository/org/apache/commons/commons-csv/1.9.0/commons-csv-1.9.0.jar:/Users/henneberger/.m2/repository/org/apache/commons/commons-lang3/3.12.0/commons-lang3-3.12.0.jar:/Users/henneberger/sqrl/sqrl-io/sqrl-io-format-csv/target/classes:/Users/henneberger/sqrl/sqrl-planner/sqrl-planner-schema-flexible/target/classes:/Users/henneberger/sqrl/sqrl-planner/sqrl-common/target/classes:/Users/henneberger/sqrl/sqrl-engines/sqrl-engine-core/target/classes:/Users/henneberger/sqrl/sqrl-execute/sqrl-execute-flink/sqrl-execute-flink-function/target/classes:/Users/henneberger/sqrl/sqrl-io/sqrl-io-schema-flexible/target/classes:/Users/henneberger/sqrl/sqrl-tools/sqrl-discovery/target/classes:/Users/henneberger/sqrl/sqrl-engines/sqrl-engine-flink/target/classes:/Users/henneberger/.m2/repository/org/apache/flink/flink-core/1.16.1/flink-core-1.16.1.jar:/Users/henneberger/.m2/repository/org/apache/flink/flink-annotations/1.16.1/flink-annotations-1.16.1.jar:/Users/henneberger/.m2/repository/org/apache/flink/flink-metrics-core/1.16.1/flink-metrics-core-1.16.1.jar:/Users/henneberger/.m2/repository/org/apache/flink/flink-shaded-asm-9/9.2-15.0/flink-shaded-asm-9-9.2-15.0.jar:/Users/henneberger/.m2/repository/org/apache/flink/flink-shaded-jackson/2.12.4-15.0/flink-shaded-jackson-2.12.4-15.0.jar:/Users/henneberger/.m2/repository/commons-collections/commons-collections/3.2.2/commons-collections-3.2.2.jar:/Users/henneberger/.m2/repository/org/apache/flink/flink-shaded-guava/30.1.1-jre-15.0/flink-shaded-guava-30.1.1-jre-15.0.jar:/Users/henneberger/.m2/repository/org/apache/flink/flink-streaming-java/1.16.1/flink-streaming-java-1.16.1.jar:/Users/henneberger/.m2/repository/org/apache/flink/flink-runtime/1.16.1/flink-runtime-1.16.1.jar:/Users/henneberger/.m2/repository/org/apache/flink/flink-rpc-core/1.16.1/flink-rpc-core-1.16.1.jar:/Users/henneberger/.m2/repository/org/apache/flink/flink-rpc-akka-loader/1.16.1/flink-rpc-akka-loader-1.16.1.jar:/Users/henneberger/.m2/repository/org/apache/flink/flink-queryable-state-client-java/1.16.1/flink-queryable-state-client-java-1.16.1.jar:/Users/henneberger/.m2/repository/org/apache/flink/flink-hadoop-fs/1.16.1/flink-hadoop-fs-1.16.1.jar:/Users/henneberger/.m2/repository/org/apache/flink/flink-shaded-netty/4.1.70.Final-15.0/flink-shaded-netty-4.1.70.Final-15.0.jar:/Users/henneberger/.m2/repository/org/apache/flink/flink-shaded-zookeeper-3/3.5.9-15.0/flink-shaded-zookeeper-3-3.5.9-15.0.jar:/Users/henneberger/.m2/repository/org/javassist/javassist/3.24.0-GA/javassist-3.24.0-GA.jar:/Users/henneberger/.m2/repository/org/apache/commons/commons-math3/3.6.1/commons-math3-3.6.1.jar:/Users/henneberger/.m2/repository/org/apache/flink/flink-clients/1.16.1/flink-clients-1.16.1.jar:/Users/henneberger/.m2/repository/org/apache/flink/flink-optimizer/1.16.1/flink-optimizer-1.16.1.jar:/Users/henneberger/sqrl/sqrl-tools/sqrl-cli/target/test-classes:/Users/henneberger/sqrl/sqrl-base/target/classes:/Users/henneberger/.m2/repository/com/fasterxml/jackson/core/jackson-core/2.14.0/jackson-core-2.14.0.jar:/Users/henneberger/.m2/repository/com/fasterxml/jackson/module/jackson-module-blackbird/2.14.0/jackson-module-blackbird-2.14.0.jar:/Users/henneberger/.m2/repository/com/fasterxml/jackson/dataformat/jackson-dataformat-yaml/2.14.0/jackson-dataformat-yaml-2.14.0.jar:/Users/henneberger/.m2/repository/com/fasterxml/jackson/datatype/jackson-datatype-jsr310/2.14.0/jackson-datatype-jsr310-2.14.0.jar:/Users/henneberger/.m2/repository/com/google/guava/guava/31.1-jre/guava-31.1-jre.jar:/Users/henneberger/.m2/repository/com/google/guava/failureaccess/1.0.1/failureaccess-1.0.1.jar:/Users/henneberger/.m2/repository/com/google/guava/listenablefuture/9999.0-empty-to-avoid-conflict-with-guava/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/Users/henneberger/.m2/repository/com/google/errorprone/error_prone_annotations/2.11.0/error_prone_annotations-2.11.0.jar:/Users/henneberger/.m2/repository/com/google/j2objc/j2objc-annotations/1.3/j2objc-annotations-1.3.jar:/Users/henneberger/.m2/repository/com/google/auto/service/auto-service/1.0.1/auto-service-1.0.1.jar:/Users/henneberger/.m2/repository/com/google/auto/service/auto-service-annotations/1.0.1/auto-service-annotations-1.0.1.jar:/Users/henneberger/.m2/repository/com/google/auto/auto-common/1.2/auto-common-1.2.jar:/Users/henneberger/.m2/repository/org/apache/commons/commons-configuration2/2.9.0/commons-configuration2-2.9.0.jar:/Users/henneberger/.m2/repository/commons-logging/commons-logging/1.2/commons-logging-1.2.jar:/Users/henneberger/.m2/repository/commons-beanutils/commons-beanutils/1.9.4/commons-beanutils-1.9.4.jar:/Users/henneberger/.m2/repository/org/apache/logging/log4j/log4j-api/2.19.0/log4j-api-2.19.0.jar:/Users/henneberger/.m2/repository/org/apache/logging/log4j/log4j-core/2.19.0/log4j-core-2.19.0.jar:/Users/henneberger/.m2/repository/org/apache/logging/log4j/log4j-slf4j-impl/2.19.0/log4j-slf4j-impl-2.19.0.jar:/Users/henneberger/sqrl/sqrl-execute/sqrl-execute-http/sqrl-execute-http-aws-lambda/target/classes:/Users/henneberger/.m2/repository/com/amazonaws/aws-lambda-java-core/1.2.2/aws-lambda-java-core-1.2.2.jar:/Users/henneberger/.m2/repository/com/amazonaws/aws-lambda-java-events/3.11.0/aws-lambda-java-events-3.11.0.jar:/Users/henneberger/.m2/repository/joda-time/joda-time/2.6/joda-time-2.6.jar:/Users/henneberger/.m2/repository/ch/qos/logback/logback-classic/1.4.6/logback-classic-1.4.6.jar:/Users/henneberger/.m2/repository/ch/qos/logback/logback-core/1.4.6/logback-core-1.4.6.jar:/Users/henneberger/.m2/repository/org/jooq/jooq/3.16.12/jooq-3.16.12.jar:/Users/henneberger/.m2/repository/io/r2dbc/r2dbc-spi/0.9.0.RELEASE/r2dbc-spi-0.9.0.RELEASE.jar:/Users/henneberger/.m2/repository/jakarta/xml/bind/jakarta.xml.bind-api/3.0.0/jakarta.xml.bind-api-3.0.0.jar:/Users/henneberger/.m2/repository/com/sun/activation/jakarta.activation/2.0.0/jakarta.activation-2.0.0.jar:/Users/henneberger/sqrl/sqrl-io/sqrl-io-jdbc/target/classes:/Users/henneberger/.m2/repository/com/esotericsoftware/kryo/kryo/2.24.0/kryo-2.24.0.jar:/Users/henneberger/.m2/repository/com/esotericsoftware/minlog/minlog/1.2/minlog-1.2.jar:/Users/henneberger/.m2/repository/org/objenesis/objenesis/2.1/objenesis-2.1.jar:/Users/henneberger/sqrl/sqrl-engines/sqrl-engine-kafka/target/classes:/Users/henneberger/sqrl/sqrl-io/sqrl-io-file/target/classes:/Users/henneberger/.m2/repository/org/apache/flink/flink-connector-files/1.16.1/flink-connector-files-1.16.1.jar:/Users/henneberger/sqrl/sqrl-io/sqrl-io-kafka/target/classes:/Users/henneberger/sqrl/sqrl-io/sqrl-io-print/target/classes:/Users/henneberger/sqrl/sqrl-execute/sqrl-execute-flink/sqrl-execute-flink-connectors/sqrl-execute-flink-postgres/target/classes:/Users/henneberger/.m2/repository/info/picocli/picocli/4.7.4/picocli-4.7.4.jar:/Users/henneberger/sqrl/sqrl-execute/sqrl-execute-flink/sqrl-execute-flink-connectors/sqrl-execute-flink-inmem/target/classes:/Users/henneberger/sqrl/sqrl-io/sqrl-io-mem/target/classes:/Users/henneberger/.m2/repository/org/apache/flink/flink-file-sink-common/1.16.1/flink-file-sink-common-1.16.1.jar:/Users/henneberger/.m2/repository/org/apache/flink/flink-shaded-force-shading/15.0/flink-shaded-force-shading-15.0.jar:/Users/henneberger/sqrl/sqrl-execute/sqrl-execute-flink/sqrl-execute-flink-core/target/classes:/Users/henneberger/.m2/repository/org/apache/flink/flink-json/1.16.1/flink-json-1.16.1.jar:/Users/henneberger/.m2/repository/com/google/code/findbugs/jsr305/1.3.9/jsr305-1.3.9.jar:/Users/henneberger/.m2/repository/com/opencsv/opencsv/5.7.1/opencsv-5.7.1.jar:/Users/henneberger/.m2/repository/org/apache/commons/commons-text/1.10.0/commons-text-1.10.0.jar:/Users/henneberger/.m2/repository/org/apache/flink/flink-table-api-java-bridge/1.16.1/flink-table-api-java-bridge-1.16.1.jar:/Users/henneberger/.m2/repository/org/apache/flink/flink-table-api-java/1.16.1/flink-table-api-java-1.16.1.jar:/Users/henneberger/.m2/repository/org/apache/flink/flink-table-common/1.16.1/flink-table-common-1.16.1.jar:/Users/henneberger/.m2/repository/com/ibm/icu/icu4j/67.1/icu4j-67.1.jar:/Users/henneberger/.m2/repository/org/apache/flink/flink-table-api-bridge-base/1.16.1/flink-table-api-bridge-base-1.16.1.jar:/Users/henneberger/.m2/repository/org/apache/flink/flink-java/1.16.1/flink-java-1.16.1.jar:/Users/henneberger/.m2/repository/com/twitter/chill-java/0.7.6/chill-java-0.7.6.jar:/Users/henneberger/sqrl/sqrl-execute/sqrl-execute-flink/sqrl-execute-flink-connectors/sqrl-execute-flink-jdbc/target/classes:/Users/henneberger/.m2/repository/org/apache/flink/flink-connector-jdbc/1.16.1/flink-connector-jdbc-1.16.1.jar:/Users/henneberger/sqrl/sqrl-execute/sqrl-execute-flink/sqrl-execute-flink-connectors/sqrl-execute-flink-h2/target/classes:/Users/henneberger/.m2/repository/com/h2database/h2/2.1.214/h2-2.1.214.jar:/Users/henneberger/sqrl/sqrl-execute/sqrl-execute-flink/sqrl-execute-flink-connectors/sqrl-execute-flink-file/target/classes:/Users/henneberger/.m2/repository/org/apache/flink/flink-s3-fs-hadoop/1.16.1/flink-s3-fs-hadoop-1.16.1.jar:/Users/henneberger/sqrl/sqrl-execute/sqrl-execute-flink/sqrl-execute-flink-connectors/sqrl-execute-flink-kafka/target/classes:/Users/henneberger/.m2/repository/org/apache/flink/flink-connector-kafka/1.16.1/flink-connector-kafka-1.16.1.jar:/Users/henneberger/.m2/repository/org/apache/flink/flink-connector-base/1.16.1/flink-connector-base-1.16.1.jar:/Users/henneberger/sqrl/sqrl-execute/sqrl-execute-flink/sqrl-execute-flink-connectors/sqrl-execute-flink-print/target/classes:/Users/henneberger/sqrl/sqrl-execute/sqrl-execute-flink/sqrl-execute-flink-connectors/sqrl-execute-flink-sqlite/target/classes:/Users/henneberger/.m2/repository/org/xerial/sqlite-jdbc/3.40.0.0/sqlite-jdbc-3.40.0.0.jar:/Users/henneberger/sqrl/sqrl-tools/sqrl-packager/target/classes:/Users/henneberger/sqrl/sqrl-planner/sqrl-planner-local/target/classes:/Users/henneberger/.m2/repository/commons-codec/commons-codec/1.15/commons-codec-1.15.jar:/Users/henneberger/.m2/repository/org/apache/commons/commons-collections4/4.4/commons-collections4-4.4.jar:/Users/henneberger/.m2/repository/net/lingala/zip4j/zip4j/2.11.2/zip4j-2.11.2.jar:/Users/henneberger/sqrl/sqrl-execute/sqrl-execute-http/sqrl-execute-http-vertx/target/classes:/Users/henneberger/sqrl/sqrl-execute/sqrl-execute-http/sqrl-execute-http-core/target/classes:/Users/henneberger/.m2/repository/com/graphql-java/graphql-java/19.2/graphql-java-19.2.jar:/Users/henneberger/.m2/repository/com/graphql-java/java-dataloader/3.2.0/java-dataloader-3.2.0.jar:/Users/henneberger/.m2/repository/io/vertx/vertx-config/4.3.5/vertx-config-4.3.5.jar:/Users/henneberger/.m2/repository/io/vertx/vertx-web-openapi/4.3.5/vertx-web-openapi-4.3.5.jar:/Users/henneberger/.m2/repository/io/vertx/vertx-web-validation/4.3.5/vertx-web-validation-4.3.5.jar:/Users/henneberger/.m2/repository/io/vertx/vertx-json-schema/4.3.5/vertx-json-schema-4.3.5.jar:/Users/henneberger/.m2/repository/org/yaml/snakeyaml/1.32/snakeyaml-1.32.jar:/Users/henneberger/.m2/repository/io/vertx/vertx-jdbc-client/4.3.5/vertx-jdbc-client-4.3.5.jar:/Users/henneberger/.m2/repository/io/vertx/vertx-sql-client/4.3.5/vertx-sql-client-4.3.5.jar:/Users/henneberger/.m2/repository/com/mchange/c3p0/0.9.5.5/c3p0-0.9.5.5.jar:/Users/henneberger/.m2/repository/com/mchange/mchange-commons-java/0.2.19/mchange-commons-java-0.2.19.jar:/Users/henneberger/.m2/repository/io/vertx/vertx-web/4.3.5/vertx-web-4.3.5.jar:/Users/henneberger/.m2/repository/io/vertx/vertx-bridge-common/4.3.5/vertx-bridge-common-4.3.5.jar:/Users/henneberger/.m2/repository/io/vertx/vertx-web-graphql/4.3.5/vertx-web-graphql-4.3.5.jar:/Users/henneberger/.m2/repository/io/vertx/vertx-pg-client/4.3.5/vertx-pg-client-4.3.5.jar:/Users/henneberger/.m2/repository/io/agroal/agroal-pool/1.11/agroal-pool-1.11.jar:/Users/henneberger/.m2/repository/io/agroal/agroal-api/1.11/agroal-api-1.11.jar:/Users/henneberger/.m2/repository/org/reactivestreams/reactive-streams/1.0.4/reactive-streams-1.0.4.jar:/Users/henneberger/.m2/repository/io/vertx/vertx-kafka-client/4.3.5/vertx-kafka-client-4.3.5.jar:/Users/henneberger/.m2/repository/io/projectreactor/reactor-core/3.5.6/reactor-core-3.5.6.jar:/Users/henneberger/.m2/repository/io/netty/netty-codec/4.1.92.Final/netty-codec-4.1.92.Final.jar:/Users/henneberger/.m2/repository/io/netty/netty-common/4.1.92.Final/netty-common-4.1.92.Final.jar:/Users/henneberger/.m2/repository/io/netty/netty-buffer/4.1.92.Final/netty-buffer-4.1.92.Final.jar:/Users/henneberger/.m2/repository/io/netty/netty-transport/4.1.92.Final/netty-transport-4.1.92.Final.jar:/Users/henneberger/.m2/repository/org/postgresql/postgresql/42.5.0/postgresql-42.5.0.jar:/Users/henneberger/.m2/repository/org/checkerframework/checker-qual/3.5.0/checker-qual-3.5.0.jar:/Users/henneberger/.m2/repository/com/ongres/scram/common/2.1/common-2.1.jar:/Users/henneberger/.m2/repository/com/ongres/stringprep/saslprep/1.1/saslprep-1.1.jar:/Users/henneberger/.m2/repository/com/ongres/stringprep/stringprep/1.1/stringprep-1.1.jar:/Users/henneberger/.m2/repository/com/ongres/scram/client/2.1/client-2.1.jar:/Users/henneberger/sqrl/sqrl-tools/sqrl-cli/target/classes:/Users/henneberger/.m2/repository/io/vertx/vertx-junit5/4.3.5/vertx-junit5-4.3.5.jar:/Users/henneberger/.m2/repository/io/vertx/vertx-core/4.3.5/vertx-core-4.3.5.jar:/Users/henneberger/.m2/repository/io/netty/netty-handler/4.1.85.Final/netty-handler-4.1.85.Final.jar:/Users/henneberger/.m2/repository/io/netty/netty-transport-native-unix-common/4.1.85.Final/netty-transport-native-unix-common-4.1.85.Final.jar:/Users/henneberger/.m2/repository/io/netty/netty-handler-proxy/4.1.85.Final/netty-handler-proxy-4.1.85.Final.jar:/Users/henneberger/.m2/repository/io/netty/netty-codec-socks/4.1.85.Final/netty-codec-socks-4.1.85.Final.jar:/Users/henneberger/.m2/repository/io/netty/netty-codec-http/4.1.85.Final/netty-codec-http-4.1.85.Final.jar:/Users/henneberger/.m2/repository/io/netty/netty-codec-http2/4.1.85.Final/netty-codec-http2-4.1.85.Final.jar:/Users/henneberger/.m2/repository/io/netty/netty-resolver/4.1.85.Final/netty-resolver-4.1.85.Final.jar:/Users/henneberger/.m2/repository/io/netty/netty-resolver-dns/4.1.85.Final/netty-resolver-dns-4.1.85.Final.jar:/Users/henneberger/.m2/repository/io/netty/netty-codec-dns/4.1.85.Final/netty-codec-dns-4.1.85.Final.jar:/Users/henneberger/.m2/repository/org/junit/jupiter/junit-jupiter-api/5.8.2/junit-jupiter-api-5.8.2.jar:/Users/henneberger/.m2/repository/org/opentest4j/opentest4j/1.2.0/opentest4j-1.2.0.jar:/Users/henneberger/.m2/repository/org/junit/platform/junit-platform-commons/1.8.2/junit-platform-commons-1.8.2.jar:/Users/henneberger/.m2/repository/io/vertx/vertx-web-client/4.3.5/vertx-web-client-4.3.5.jar:/Users/henneberger/.m2/repository/io/vertx/vertx-uri-template/4.3.5/vertx-uri-template-4.3.5.jar:/Users/henneberger/.m2/repository/io/vertx/vertx-web-common/4.3.5/vertx-web-common-4.3.5.jar:/Users/henneberger/.m2/repository/io/vertx/vertx-auth-common/4.3.5/vertx-auth-common-4.3.5.jar:/Users/henneberger/.m2/repository/org/testcontainers/testcontainers/1.18.0/testcontainers-1.18.0.jar:/Users/henneberger/.m2/repository/junit/junit/4.13.2/junit-4.13.2.jar:/Users/henneberger/.m2/repository/org/hamcrest/hamcrest-core/1.3/hamcrest-core-1.3.jar:/Users/henneberger/.m2/repository/org/slf4j/slf4j-api/1.7.36/slf4j-api-1.7.36.jar:/Users/henneberger/.m2/repository/org/apache/commons/commons-compress/1.22/commons-compress-1.22.jar:/Users/henneberger/.m2/repository/org/rnorth/duct-tape/duct-tape/1.0.8/duct-tape-1.0.8.jar:/Users/henneberger/.m2/repository/org/jetbrains/annotations/17.0.0/annotations-17.0.0.jar:/Users/henneberger/.m2/repository/com/github/docker-java/docker-java-api/3.3.0/docker-java-api-3.3.0.jar:/Users/henneberger/.m2/repository/com/github/docker-java/docker-java-transport-zerodep/3.3.0/docker-java-transport-zerodep-3.3.0.jar:/Users/henneberger/.m2/repository/com/github/docker-java/docker-java-transport/3.3.0/docker-java-transport-3.3.0.jar:/Users/henneberger/.m2/repository/net/java/dev/jna/jna/5.12.1/jna-5.12.1.jar:/Users/henneberger/.m2/repository/org/testcontainers/junit-jupiter/1.18.0/junit-jupiter-1.18.0.jar:/Users/henneberger/.m2/repository/org/testcontainers/postgresql/1.18.0/postgresql-1.18.0.jar:/Users/henneberger/.m2/repository/org/testcontainers/jdbc/1.18.0/jdbc-1.18.0.jar:/Users/henneberger/.m2/repository/org/testcontainers/database-commons/1.18.0/database-commons-1.18.0.jar:/Users/henneberger/.m2/repository/org/apache/kafka/kafka-clients/3.4.0/kafka-clients-3.4.0.jar:/Users/henneberger/.m2/repository/com/github/luben/zstd-jni/1.5.2-1/zstd-jni-1.5.2-1.jar:/Users/henneberger/.m2/repository/org/lz4/lz4-java/1.8.0/lz4-java-1.8.0.jar:/Users/henneberger/.m2/repository/org/xerial/snappy/snappy-java/1.1.8.4/snappy-java-1.1.8.4.jar:/Users/henneberger/.m2/repository/org/apache/kafka/kafka-clients/3.4.0/kafka-clients-3.4.0-test.jar:/Users/henneberger/.m2/repository/org/apache/kafka/kafka_2.12/3.4.0/kafka_2.12-3.4.0.jar:/Users/henneberger/.m2/repository/org/scala-lang/scala-library/2.12.15/scala-library-2.12.15.jar:/Users/henneberger/.m2/repository/org/apache/kafka/kafka-server-common/3.4.0/kafka-server-common-3.4.0.jar:/Users/henneberger/.m2/repository/org/apache/kafka/kafka-group-coordinator/3.4.0/kafka-group-coordinator-3.4.0.jar:/Users/henneberger/.m2/repository/org/apache/kafka/kafka-metadata/3.4.0/kafka-metadata-3.4.0.jar:/Users/henneberger/.m2/repository/org/apache/kafka/kafka-raft/3.4.0/kafka-raft-3.4.0.jar:/Users/henneberger/.m2/repository/org/apache/kafka/kafka-storage/3.4.0/kafka-storage-3.4.0.jar:/Users/henneberger/.m2/repository/org/apache/kafka/kafka-storage-api/3.4.0/kafka-storage-api-3.4.0.jar:/Users/henneberger/.m2/repository/net/sourceforge/argparse4j/argparse4j/0.7.0/argparse4j-0.7.0.jar:/Users/henneberger/.m2/repository/com/fasterxml/jackson/core/jackson-databind/2.14.0/jackson-databind-2.14.0.jar:/Users/henneberger/.m2/repository/com/fasterxml/jackson/module/jackson-module-scala_2.12/2.13.4/jackson-module-scala_2.12-2.13.4.jar:/Users/henneberger/.m2/repository/com/thoughtworks/paranamer/paranamer/2.8/paranamer-2.8.jar:/Users/henneberger/.m2/repository/com/fasterxml/jackson/dataformat/jackson-dataformat-csv/2.13.4/jackson-dataformat-csv-2.13.4.jar:/Users/henneberger/.m2/repository/com/fasterxml/jackson/datatype/jackson-datatype-jdk8/2.14.0/jackson-datatype-jdk8-2.14.0.jar:/Users/henneberger/.m2/repository/net/sf/jopt-simple/jopt-simple/5.0.4/jopt-simple-5.0.4.jar:/Users/henneberger/.m2/repository/org/bitbucket/b_c/jose4j/0.7.9/jose4j-0.7.9.jar:/Users/henneberger/.m2/repository/com/yammer/metrics/metrics-core/2.2.0/metrics-core-2.2.0.jar:/Users/henneberger/.m2/repository/org/scala-lang/modules/scala-collection-compat_2.12/2.6.0/scala-collection-compat_2.12-2.6.0.jar:/Users/henneberger/.m2/repository/org/scala-lang/modules/scala-java8-compat_2.12/1.0.2/scala-java8-compat_2.12-1.0.2.jar:/Users/henneberger/.m2/repository/org/scala-lang/scala-reflect/2.12.15/scala-reflect-2.12.15.jar:/Users/henneberger/.m2/repository/com/typesafe/scala-logging/scala-logging_2.12/3.9.4/scala-logging_2.12-3.9.4.jar:/Users/henneberger/.m2/repository/io/dropwizard/metrics/metrics-core/4.1.12.1/metrics-core-4.1.12.1.jar:/Users/henneberger/.m2/repository/org/apache/zookeeper/zookeeper/3.6.3/zookeeper-3.6.3.jar:/Users/henneberger/.m2/repository/org/apache/zookeeper/zookeeper-jute/3.6.3/zookeeper-jute-3.6.3.jar:/Users/henneberger/.m2/repository/org/apache/yetus/audience-annotations/0.5.0/audience-annotations-0.5.0.jar:/Users/henneberger/.m2/repository/io/netty/netty-transport-native-epoll/4.1.63.Final/netty-transport-native-epoll-4.1.63.Final.jar:/Users/henneberger/.m2/repository/commons-cli/commons-cli/1.4/commons-cli-1.4.jar:/Users/henneberger/.m2/repository/org/apache/kafka/kafka_2.12/3.4.0/kafka_2.12-3.4.0-test.jar:/Users/henneberger/.m2/repository/org/apache/kafka/kafka-streams/3.4.0/kafka-streams-3.4.0.jar:/Users/henneberger/.m2/repository/org/rocksdb/rocksdbjni/7.1.2/rocksdbjni-7.1.2.jar:/Users/henneberger/.m2/repository/com/fasterxml/jackson/core/jackson-annotations/2.14.0/jackson-annotations-2.14.0.jar:/Users/henneberger/.m2/repository/org/apache/kafka/kafka-streams/3.4.0/kafka-streams-3.4.0-test.jar:/Users/henneberger/.m2/repository/org/apache/kafka/kafka-streams-test-utils/3.4.0/kafka-streams-test-utils-3.4.0.jar:/Users/henneberger/sqrl/sqrl-planner/sqrl-common/target/test-classes:/Users/henneberger/.m2/repository/org/apache/flink/flink-table-planner_2.12/1.16.1/flink-table-planner_2.12-1.16.1.jar:/Users/henneberger/.m2/repository/org/codehaus/janino/commons-compiler/3.0.11/commons-compiler-3.0.11.jar:/Users/henneberger/.m2/repository/org/codehaus/janino/janino/3.0.11/janino-3.0.11.jar:/Users/henneberger/.m2/repository/org/apache/flink/flink-scala_2.12/1.16.1/flink-scala_2.12-1.16.1.jar:/Users/henneberger/.m2/repository/org/scala-lang/scala-compiler/2.12.7/scala-compiler-2.12.7.jar:/Users/henneberger/.m2/repository/org/scala-lang/modules/scala-xml_2.12/1.0.6/scala-xml_2.12-1.0.6.jar:/Users/henneberger/.m2/repository/com/twitter/chill_2.12/0.7.6/chill_2.12-0.7.6.jar:/Users/henneberger/.m2/repository/org/apache/flink/flink-table-runtime/1.16.1/flink-table-runtime-1.16.1.jar:/Users/henneberger/.m2/repository/org/apache/flink/flink-cep/1.16.1/flink-cep-1.16.1.jar:/Users/henneberger/.m2/repository/org/antlr/antlr4-runtime/4.9.2/antlr4-runtime-4.9.2.jar:/Users/henneberger/.m2/repository/commons-io/commons-io/2.11.0/commons-io-2.11.0.jar:/Users/henneberger/sqrl/sqrl-planner/sqrl-planner-local/target/test-classes:/Users/henneberger/sqrl/sqrl-engines/sqrl-engine-jdbc/target/classes:/Users/henneberger/sqrl/sqrl-engines/sqrl-engine-server/target/classes:/Users/henneberger/.m2/repository/com/google/inject/guice/5.1.0/guice-5.1.0.jar:/Users/henneberger/.m2/repository/javax/inject/javax.inject/1/javax.inject-1.jar:/Users/henneberger/.m2/repository/aopalliance/aopalliance/1.0/aopalliance-1.0.jar:/Users/henneberger/.m2/repository/org/glassfish/javax.el/3.0.0/javax.el-3.0.0.jar:/Users/henneberger/.m2/repository/org/projectlombok/lombok/1.18.24/lombok-1.18.24.jar:/Users/henneberger/.m2/repository/org/junit/jupiter/junit-jupiter-engine/5.8.2/junit-jupiter-engine-5.8.2.jar:/Users/henneberger/.m2/repository/org/junit/platform/junit-platform-engine/1.8.2/junit-platform-engine-1.8.2.jar:/Users/henneberger/.m2/repository/org/apiguardian/apiguardian-api/1.1.2/apiguardian-api-1.1.2.jar:/Users/henneberger/.m2/repository/org/junit/jupiter/junit-jupiter-params/5.8.2/junit-jupiter-params-5.8.2.jar
11:28:25.817 [main] INFO  org.apache.zookeeper.ZooKeeper - Client environment:java.library.path=/Users/henneberger/Library/Java/Extensions:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java:.
11:28:25.817 [main] INFO  org.apache.zookeeper.ZooKeeper - Client environment:java.io.tmpdir=/var/folders/w3/dxm786y50_b8p5tj7xktkngh0000gn/T/
11:28:25.817 [main] INFO  org.apache.zookeeper.ZooKeeper - Client environment:java.compiler=<NA>
11:28:25.818 [main] INFO  org.apache.zookeeper.ZooKeeper - Client environment:os.name=Mac OS X
11:28:25.818 [main] INFO  org.apache.zookeeper.ZooKeeper - Client environment:os.arch=aarch64
11:28:25.818 [main] INFO  org.apache.zookeeper.ZooKeeper - Client environment:os.version=13.4
11:28:25.818 [main] INFO  org.apache.zookeeper.ZooKeeper - Client environment:user.name=henneberger
11:28:25.818 [main] INFO  org.apache.zookeeper.ZooKeeper - Client environment:user.home=/Users/henneberger
11:28:25.818 [main] INFO  org.apache.zookeeper.ZooKeeper - Client environment:user.dir=/Users/henneberger/sqrl/sqrl-testing/sqrl-integration-tests
11:28:25.818 [main] INFO  org.apache.zookeeper.ZooKeeper - Client environment:os.memory.free=117MB
11:28:25.818 [main] INFO  org.apache.zookeeper.ZooKeeper - Client environment:os.memory.max=2048MB
11:28:25.818 [main] INFO  org.apache.zookeeper.ZooKeeper - Client environment:os.memory.total=154MB
11:28:25.820 [main] INFO  org.apache.zookeeper.ZooKeeper - Initiating client connection, connectString=127.0.0.1:51722 sessionTimeout=10000 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@33324c05
11:28:25.821 [main] INFO  org.apache.zookeeper.ClientCnxnSocket - jute.maxbuffer value is 4194304 Bytes
11:28:25.825 [main] INFO  org.apache.zookeeper.ClientCnxn - zookeeper.request.timeout value is 0. feature enabled=false
11:28:25.825 [main-SendThread(127.0.0.1:51722)] INFO  org.apache.zookeeper.ClientCnxn - Opening socket connection to server /127.0.0.1:51722.
11:28:25.826 [main] INFO  kafka.zookeeper.ZooKeeperClient - [ZooKeeperClient Kafka server] Waiting until connected.
11:28:25.828 [main-SendThread(127.0.0.1:51722)] INFO  org.apache.zookeeper.ClientCnxn - Socket connection established, initiating session, client: /127.0.0.1:51723, server: /127.0.0.1:51722
11:28:25.836 [SyncThread:0] INFO  org.apache.zookeeper.server.persistence.FileTxnLog - Creating new log file: log.1
11:28:25.840 [SyncThread:0] INFO  org.apache.zookeeper.audit.ZKAuditProvider - ZooKeeper audit is disabled.
11:28:25.843 [main-SendThread(127.0.0.1:51722)] INFO  org.apache.zookeeper.ClientCnxn - Session establishment complete on server /127.0.0.1:51722, session id = 0x1000730070e0000, negotiated timeout = 10000
11:28:25.845 [main] INFO  kafka.zookeeper.ZooKeeperClient - [ZooKeeperClient Kafka server] Connected.
11:28:25.915 [main] INFO  kafka.server.KafkaServer - Cluster ID = D0jK8KEKRqCvHg2y7XT-hQ
11:28:25.917 [main] WARN  kafka.server.BrokerMetadataCheckpoint - No meta.properties file under dir /var/folders/w3/dxm786y50_b8p5tj7xktkngh0000gn/T/kafka-5258867200498972615/log7884909346953798293/meta.properties
11:28:25.970 [main] INFO  kafka.server.KafkaConfig - KafkaConfig values:
	advertised.listeners = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name =
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = null
	controller.quorum.append.linger.ms = 25
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = []
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 0
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = null
	inter.broker.protocol.version = 3.4-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = PLAINTEXT://localhost:0
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 2097152
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /var/folders/w3/dxm786y50_b8p5tj7xktkngh0000gn/T/kafka-5258867200498972615/log7884909346953798293
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides =
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000000
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 0
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 5
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = []
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.manager.class.name = null
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = null
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = null
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 5
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 127.0.0.1:51722
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.session.timeout.ms = 10000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null

11:28:25.992 [ThrottledChannelReaper-Fetch] INFO  kafka.server.ClientQuotaManager$ThrottledChannelReaper - [ThrottledChannelReaper-Fetch]: Starting
11:28:25.992 [ThrottledChannelReaper-Produce] INFO  kafka.server.ClientQuotaManager$ThrottledChannelReaper - [ThrottledChannelReaper-Produce]: Starting
11:28:25.993 [ThrottledChannelReaper-Request] INFO  kafka.server.ClientQuotaManager$ThrottledChannelReaper - [ThrottledChannelReaper-Request]: Starting
11:28:25.995 [ThrottledChannelReaper-ControllerMutation] INFO  kafka.server.ClientQuotaManager$ThrottledChannelReaper - [ThrottledChannelReaper-ControllerMutation]: Starting
11:28:26.019 [main] INFO  kafka.log.LogManager - Loading logs from log dirs ArrayBuffer(/var/folders/w3/dxm786y50_b8p5tj7xktkngh0000gn/T/kafka-5258867200498972615/log7884909346953798293)
11:28:26.021 [main] INFO  kafka.log.LogManager - Attempting recovery for all logs in /var/folders/w3/dxm786y50_b8p5tj7xktkngh0000gn/T/kafka-5258867200498972615/log7884909346953798293 since no clean shutdown file was found
11:28:26.030 [main] INFO  kafka.log.LogManager - Loaded 0 logs in 0ms.
11:28:26.031 [main] INFO  kafka.log.LogManager - Starting log cleanup with a period of 300000 ms.
11:28:26.032 [main] INFO  kafka.log.LogManager - Starting log flusher with a default period of 9223372036854775807 ms.
11:28:26.040 [main] INFO  kafka.log.LogCleaner - Starting the log cleaner
11:28:26.048 [kafka-log-cleaner-thread-0] INFO  kafka.log.LogCleaner - [kafka-log-cleaner-thread-0]: Starting
11:28:26.059 [feature-zk-node-event-process-thread] INFO  kafka.server.FinalizedFeatureChangeListener$ChangeNotificationProcessorThread - [feature-zk-node-event-process-thread]: Starting
11:28:26.065 [feature-zk-node-event-process-thread] INFO  kafka.server.FinalizedFeatureChangeListener - Feature ZK node at path: /feature does not exist
11:28:26.086 [BrokerToControllerChannelManager broker=0 name=forwarding] INFO  kafka.server.BrokerToControllerRequestThread - [BrokerToControllerChannelManager broker=0 name=forwarding]: Starting
11:28:26.366 [main] INFO  kafka.network.ConnectionQuotas - Updated connection-accept-rate max connection creation rate to 2147483647
11:28:26.370 [main] INFO  kafka.network.DataPlaneAcceptor - Awaiting socket connections on localhost:51724.
11:28:26.383 [main] INFO  kafka.network.SocketServer - [SocketServer listenerType=ZK_BROKER, nodeId=0] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT)
11:28:26.388 [BrokerToControllerChannelManager broker=0 name=alterPartition] INFO  kafka.server.BrokerToControllerRequestThread - [BrokerToControllerChannelManager broker=0 name=alterPartition]: Starting
11:28:26.410 [ExpirationReaper-0-Produce] INFO  kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper - [ExpirationReaper-0-Produce]: Starting
11:28:26.410 [ExpirationReaper-0-Fetch] INFO  kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper - [ExpirationReaper-0-Fetch]: Starting
11:28:26.411 [ExpirationReaper-0-DeleteRecords] INFO  kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper - [ExpirationReaper-0-DeleteRecords]: Starting
11:28:26.411 [ExpirationReaper-0-ElectLeader] INFO  kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper - [ExpirationReaper-0-ElectLeader]: Starting
11:28:26.420 [LogDirFailureHandler] INFO  kafka.server.ReplicaManager$LogDirFailureHandler - [LogDirFailureHandler]: Starting
11:28:26.430 [main] INFO  kafka.zk.KafkaZkClient - Creating /brokers/ids/0 (is it secure? false)
11:28:26.439 [main] INFO  kafka.zk.KafkaZkClient - Stat of the created znode at /brokers/ids/0 is: 25,25,1687544906436,1687544906436,1,0,0,72065496896110592,204,0,25

11:28:26.440 [main] INFO  kafka.zk.KafkaZkClient - Registered broker 0 at path /brokers/ids/0 with addresses: PLAINTEXT://localhost:51724, czxid (broker epoch): 25
11:28:26.485 [controller-event-thread] INFO  kafka.controller.ControllerEventManager$ControllerEventThread - [ControllerEventThread controllerId=0] Starting
11:28:26.490 [ExpirationReaper-0-topic] INFO  kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper - [ExpirationReaper-0-topic]: Starting
11:28:26.494 [controller-event-thread] INFO  kafka.zk.KafkaZkClient - Successfully created /controller_epoch with initial epoch 0
11:28:26.495 [ExpirationReaper-0-Heartbeat] INFO  kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper - [ExpirationReaper-0-Heartbeat]: Starting
11:28:26.496 [ExpirationReaper-0-Rebalance] INFO  kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper - [ExpirationReaper-0-Rebalance]: Starting
11:28:26.498 [controller-event-thread] INFO  kafka.controller.KafkaController - [Controller id=0] 0 successfully elected as the controller. Epoch incremented to 1 and epoch zk version is now 1
11:28:26.500 [controller-event-thread] INFO  kafka.controller.KafkaController - [Controller id=0] Creating FeatureZNode at path: /feature with contents: FeatureZNode(2,Enabled,Map())
11:28:26.501 [main-EventThread] INFO  kafka.server.FinalizedFeatureChangeListener - Feature ZK node created at path: /feature
11:28:26.510 [main] INFO  kafka.coordinator.group.GroupCoordinator - [GroupCoordinator 0]: Starting up.
11:28:26.513 [main] INFO  kafka.coordinator.group.GroupCoordinator - [GroupCoordinator 0]: Startup complete.
11:28:26.523 [controller-event-thread] INFO  kafka.controller.KafkaController - [Controller id=0] Registering handlers
11:28:26.523 [feature-zk-node-event-process-thread] INFO  kafka.server.metadata.ZkMetadataCache - [MetadataCache brokerId=0] Updated cache from existing <empty> to latest FinalizedFeaturesAndEpoch(features=Map(), epoch=0).
11:28:26.526 [controller-event-thread] INFO  kafka.controller.KafkaController - [Controller id=0] Deleting log dir event notifications
11:28:26.527 [controller-event-thread] INFO  kafka.controller.KafkaController - [Controller id=0] Deleting isr change notifications
11:28:26.529 [controller-event-thread] INFO  kafka.controller.KafkaController - [Controller id=0] Initializing controller context
11:28:26.530 [main] INFO  kafka.coordinator.transaction.TransactionCoordinator - [TransactionCoordinator id=0] Starting up.
11:28:26.532 [main] INFO  kafka.coordinator.transaction.TransactionCoordinator - [TransactionCoordinator id=0] Startup complete.
11:28:26.532 [TxnMarkerSenderThread-0] INFO  kafka.coordinator.transaction.TransactionMarkerChannelManager - [Transaction Marker Channel Manager 0]: Starting
11:28:26.544 [controller-event-thread] INFO  kafka.controller.KafkaController - [Controller id=0] Initialized broker epochs cache: Map(0 -> 25)
11:28:26.553 [controller-event-thread] INFO  kafka.controller.KafkaController - [Controller id=0] Currently active brokers in the cluster: Set(0)
11:28:26.553 [Controller-0-to-broker-0-send-thread] INFO  kafka.controller.RequestSendThread - [RequestSendThread controllerId=0] Starting
11:28:26.554 [controller-event-thread] INFO  kafka.controller.KafkaController - [Controller id=0] Currently shutting brokers in the cluster: Set()
11:28:26.554 [controller-event-thread] INFO  kafka.controller.KafkaController - [Controller id=0] Current list of topics in the cluster: Set()
11:28:26.554 [controller-event-thread] INFO  kafka.controller.KafkaController - [Controller id=0] Fetching topic deletions in progress
11:28:26.555 [controller-event-thread] INFO  kafka.controller.KafkaController - [Controller id=0] List of topics to be deleted:
11:28:26.555 [controller-event-thread] INFO  kafka.controller.KafkaController - [Controller id=0] List of topics ineligible for deletion:
11:28:26.555 [controller-event-thread] INFO  kafka.controller.KafkaController - [Controller id=0] Initializing topic deletion manager
11:28:26.555 [controller-event-thread] INFO  kafka.controller.TopicDeletionManager - [Topic Deletion Manager 0] Initializing manager with initial deletions: Set(), initial ineligible deletions: Set()
11:28:26.556 [controller-event-thread] INFO  kafka.controller.KafkaController - [Controller id=0] Sending update metadata request
11:28:26.558 [controller-event-thread] INFO  state.change.logger - [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers Set(0) for 0 partitions
11:28:26.561 [controller-event-thread] INFO  kafka.controller.ZkReplicaStateMachine - [ReplicaStateMachine controllerId=0] Initializing replica state
11:28:26.562 [controller-event-thread] INFO  kafka.controller.ZkReplicaStateMachine - [ReplicaStateMachine controllerId=0] Triggering online replica state changes
11:28:26.564 [controller-event-thread] INFO  kafka.controller.ZkReplicaStateMachine - [ReplicaStateMachine controllerId=0] Triggering offline replica state changes
11:28:26.564 [controller-event-thread] INFO  kafka.controller.ZkPartitionStateMachine - [PartitionStateMachine controllerId=0] Initializing partition state
11:28:26.564 [controller-event-thread] INFO  kafka.controller.ZkPartitionStateMachine - [PartitionStateMachine controllerId=0] Triggering online partition state changes
11:28:26.565 [ExpirationReaper-0-AlterAcls] INFO  kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper - [ExpirationReaper-0-AlterAcls]: Starting
11:28:26.566 [controller-event-thread] INFO  kafka.controller.KafkaController - [Controller id=0] Ready to serve as the new controller with epoch 1
11:28:26.567 [Controller-0-to-broker-0-send-thread] INFO  kafka.controller.RequestSendThread - [RequestSendThread controllerId=0] Controller 0 connected to localhost:51724 (id: 0 rack: null) for sending state change requests
11:28:26.571 [controller-event-thread] INFO  kafka.controller.KafkaController - [Controller id=0] Partitions undergoing preferred replica election:
11:28:26.571 [controller-event-thread] INFO  kafka.controller.KafkaController - [Controller id=0] Partitions that completed preferred replica election:
11:28:26.571 [controller-event-thread] INFO  kafka.controller.KafkaController - [Controller id=0] Skipping preferred replica election for partitions due to topic deletion:
11:28:26.572 [controller-event-thread] INFO  kafka.controller.KafkaController - [Controller id=0] Resuming preferred replica election for partitions:
11:28:26.572 [controller-event-thread] INFO  kafka.controller.KafkaController - [Controller id=0] Starting replica leader election (PREFERRED) for partitions  triggered by ZkTriggered
11:28:26.582 [controller-event-thread] INFO  kafka.controller.KafkaController - [Controller id=0] Starting the controller scheduler
11:28:26.584 [/config/changes-event-process-thread] INFO  kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread - [/config/changes-event-process-thread]: Starting
11:28:26.597 [main] INFO  kafka.network.SocketServer - [SocketServer listenerType=ZK_BROKER, nodeId=0] Enabling request processing.
11:28:26.601 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 3.4.0
11:28:26.601 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 2e1947d240607d53
11:28:26.601 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1687544901743
11:28:26.603 [main] INFO  kafka.server.KafkaServer - [KafkaServer id=0] started
11:28:26.618 [main] INFO  com.datasqrl.flink.AbstractSubscriptionTest - Kafka started: []
11:28:26.696 [BrokerToControllerChannelManager broker=0 name=alterPartition] INFO  kafka.server.BrokerToControllerRequestThread - [BrokerToControllerChannelManager broker=0 name=alterPartition]: Recorded new controller, from now on will use node localhost:51724 (id: 0 rack: null)
11:28:26.698 [BrokerToControllerChannelManager broker=0 name=forwarding] INFO  kafka.server.BrokerToControllerRequestThread - [BrokerToControllerChannelManager broker=0 name=forwarding]: Recorded new controller, from now on will use node localhost:51724 (id: 0 rack: null)
11:28:30.520 [main] INFO  org.apache.kafka.clients.admin.AdminClientConfig - AdminClientConfig values:
	auto.include.jmx.reporter = true
	bootstrap.servers = [localhost:51724]
	client.dns.lookup = use_all_dns_ips
	client.id =
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

11:28:30.542 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 3.4.0
11:28:30.542 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 2e1947d240607d53
11:28:30.542 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1687544910542
11:28:30.575 [data-plane-kafka-request-handler-2] INFO  kafka.zk.AdminZkClient - Creating topic schema-createevent with configuration {} and initial partition assignment Map(0 -> ArrayBuffer(0))
11:28:30.587 [controller-event-thread] INFO  kafka.controller.KafkaController - [Controller id=0] New topics: [Set(schema-createevent)], deleted topics: [Set()], new partition replica assignment [Set(TopicIdReplicaAssignment(schema-createevent,Some(p1MvE0ssR9mnYo7ybouNhQ),Map(schema-createevent-0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=))))]
11:28:30.588 [controller-event-thread] INFO  kafka.controller.KafkaController - [Controller id=0] New partition creation callback for schema-createevent-0
11:28:30.589 [controller-event-thread] INFO  state.change.logger - [Controller id=0 epoch=1] Changed partition schema-createevent-0 state from NonExistentPartition to NewPartition with assigned replicas 0
11:28:30.589 [controller-event-thread] INFO  state.change.logger - [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers Set() for 0 partitions
11:28:30.589 [data-plane-kafka-request-handler-2] INFO  kafka.zk.AdminZkClient - Creating topic event-3 with configuration {} and initial partition assignment Map(0 -> ArrayBuffer(0))
11:28:30.593 [controller-event-thread] INFO  state.change.logger - [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers Set() for 0 partitions
11:28:30.609 [controller-event-thread] INFO  state.change.logger - [Controller id=0 epoch=1] Changed partition schema-createevent-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), leaderRecoveryState=RECOVERED, partitionEpoch=0)
11:28:30.611 [controller-event-thread] INFO  state.change.logger - [Controller id=0 epoch=1] Sending LeaderAndIsr request to broker 0 with 1 become-leader and 0 become-follower partitions
11:28:30.612 [controller-event-thread] INFO  state.change.logger - [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers Set(0) for 1 partitions
11:28:30.612 [controller-event-thread] INFO  state.change.logger - [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers Set() for 0 partitions
11:28:30.614 [controller-event-thread] INFO  kafka.controller.KafkaController - [Controller id=0] New topics: [Set(event-3)], deleted topics: [Set()], new partition replica assignment [Set(TopicIdReplicaAssignment(event-3,Some(3m77TBFRQdqdP8Xjdo77rg),Map(event-3-0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=))))]
11:28:30.615 [controller-event-thread] INFO  kafka.controller.KafkaController - [Controller id=0] New partition creation callback for event-3-0
11:28:30.615 [controller-event-thread] INFO  state.change.logger - [Controller id=0 epoch=1] Changed partition event-3-0 state from NonExistentPartition to NewPartition with assigned replicas 0
11:28:30.615 [controller-event-thread] INFO  state.change.logger - [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers Set() for 0 partitions
11:28:30.615 [controller-event-thread] INFO  state.change.logger - [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers Set() for 0 partitions
11:28:30.615 [data-plane-kafka-request-handler-3] INFO  state.change.logger - [Broker id=0] Handling LeaderAndIsr request correlationId 1 from controller 0 for 1 partitions
11:28:30.618 [controller-event-thread] INFO  state.change.logger - [Controller id=0 epoch=1] Changed partition event-3-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), leaderRecoveryState=RECOVERED, partitionEpoch=0)
11:28:30.618 [controller-event-thread] INFO  state.change.logger - [Controller id=0 epoch=1] Sending LeaderAndIsr request to broker 0 with 1 become-leader and 0 become-follower partitions
11:28:30.618 [controller-event-thread] INFO  state.change.logger - [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers Set(0) for 1 partitions
11:28:30.619 [controller-event-thread] INFO  state.change.logger - [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers Set() for 0 partitions
11:28:30.633 [data-plane-kafka-request-handler-3] INFO  kafka.server.ReplicaFetcherManager - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(schema-createevent-0)
11:28:30.634 [data-plane-kafka-request-handler-3] INFO  state.change.logger - [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 1 from controller 0 epoch 1 as part of the become-leader transition for 1 partitions
11:28:30.674 [data-plane-kafka-request-handler-3] INFO  kafka.log.UnifiedLog$ - [LogLoader partition=schema-createevent-0, dir=/var/folders/w3/dxm786y50_b8p5tj7xktkngh0000gn/T/kafka-5258867200498972615/log7884909346953798293] Loading producer state till offset 0 with message format version 2
11:28:30.686 [data-plane-kafka-request-handler-3] INFO  kafka.log.LogManager - Created log for partition schema-createevent-0 in /var/folders/w3/dxm786y50_b8p5tj7xktkngh0000gn/T/kafka-5258867200498972615/log7884909346953798293/schema-createevent-0 with properties {}
11:28:30.687 [data-plane-kafka-request-handler-3] INFO  kafka.cluster.Partition - [Partition schema-createevent-0 broker=0] No checkpointed highwatermark is found for partition schema-createevent-0
11:28:30.688 [data-plane-kafka-request-handler-3] INFO  kafka.cluster.Partition - [Partition schema-createevent-0 broker=0] Log loaded for partition schema-createevent-0 with initial high watermark 0
11:28:30.688 [data-plane-kafka-request-handler-3] INFO  state.change.logger - [Broker id=0] Leader schema-createevent-0 with topic id Some(p1MvE0ssR9mnYo7ybouNhQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [0], adding replicas [] and removing replicas []. Previous leader epoch was -1.
11:28:30.709 [data-plane-kafka-request-handler-3] INFO  state.change.logger - [Broker id=0] Finished LeaderAndIsr request in 0ms correlationId 1 from controller 0 for 1 partitions
11:28:30.713 [data-plane-kafka-request-handler-7] INFO  state.change.logger - [Broker id=0] Add 1 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 2
11:28:30.714 [data-plane-kafka-request-handler-6] INFO  state.change.logger - [Broker id=0] Handling LeaderAndIsr request correlationId 3 from controller 0 for 1 partitions
11:28:30.715 [data-plane-kafka-request-handler-6] INFO  kafka.server.ReplicaFetcherManager - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(event-3-0)
11:28:30.715 [data-plane-kafka-request-handler-6] INFO  state.change.logger - [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 3 from controller 0 epoch 1 as part of the become-leader transition for 1 partitions
11:28:30.717 [data-plane-kafka-request-handler-6] INFO  kafka.log.UnifiedLog$ - [LogLoader partition=event-3-0, dir=/var/folders/w3/dxm786y50_b8p5tj7xktkngh0000gn/T/kafka-5258867200498972615/log7884909346953798293] Loading producer state till offset 0 with message format version 2
11:28:30.718 [data-plane-kafka-request-handler-6] INFO  kafka.log.LogManager - Created log for partition event-3-0 in /var/folders/w3/dxm786y50_b8p5tj7xktkngh0000gn/T/kafka-5258867200498972615/log7884909346953798293/event-3-0 with properties {}
11:28:30.718 [data-plane-kafka-request-handler-6] INFO  kafka.cluster.Partition - [Partition event-3-0 broker=0] No checkpointed highwatermark is found for partition event-3-0
11:28:30.718 [data-plane-kafka-request-handler-6] INFO  kafka.cluster.Partition - [Partition event-3-0 broker=0] Log loaded for partition event-3-0 with initial high watermark 0
11:28:30.718 [data-plane-kafka-request-handler-6] INFO  state.change.logger - [Broker id=0] Leader event-3-0 with topic id Some(3m77TBFRQdqdP8Xjdo77rg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [0], adding replicas [] and removing replicas []. Previous leader epoch was -1.
11:28:30.726 [data-plane-kafka-request-handler-6] INFO  state.change.logger - [Broker id=0] Finished LeaderAndIsr request in 0ms correlationId 3 from controller 0 for 1 partitions
11:28:30.727 [data-plane-kafka-request-handler-5] INFO  state.change.logger - [Broker id=0] Add 1 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 4
11:28:30.732 [kafka-admin-client-thread | adminclient-1] INFO  org.apache.kafka.common.utils.AppInfoParser - App info kafka.admin.client for adminclient-1 unregistered
11:28:30.734 [kafka-admin-client-thread | adminclient-1] INFO  org.apache.kafka.common.metrics.Metrics - Metrics scheduler closed
11:28:30.734 [kafka-admin-client-thread | adminclient-1] INFO  org.apache.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
11:28:30.734 [kafka-admin-client-thread | adminclient-1] INFO  org.apache.kafka.common.metrics.Metrics - Metrics reporters closed
11:28:30.776 [main] INFO  com.datasqrl.engine.server.GenericJavaServerEngine - Server started at: http://localhost:8888/graphiql/
11:28:30.849 [vert.x-eventloop-thread-0] INFO  org.apache.kafka.clients.producer.ProducerConfig - ProducerConfig values:
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:51724]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

11:28:30.856 [vert.x-eventloop-thread-0] INFO  org.apache.kafka.clients.producer.KafkaProducer - [Producer clientId=producer-1] Instantiated an idempotent producer.
11:28:30.869 [vert.x-eventloop-thread-0] WARN  org.apache.kafka.clients.producer.ProducerConfig - These configurations '[name]' were supplied but are not used yet.
11:28:30.869 [vert.x-eventloop-thread-0] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 3.4.0
11:28:30.869 [vert.x-eventloop-thread-0] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 2e1947d240607d53
11:28:30.869 [vert.x-eventloop-thread-0] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1687544910869
11:28:30.877 [kafka-producer-network-thread | producer-1] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-1] Cluster ID: D0jK8KEKRqCvHg2y7XT-hQ
11:28:30.883 [controller-event-thread] INFO  kafka.controller.KafkaController - [Controller id=0] Acquired new producerId block ProducerIdsBlock(assignedBrokerId=0, firstProducerId=0, size=1000) by writing to Zk with path version 1
11:28:30.886 [kafka-producer-network-thread | producer-1] INFO  org.apache.kafka.clients.producer.internals.TransactionManager - [Producer clientId=producer-1] ProducerId set to 0 with epoch 0
11:28:30.889 [vert.x-eventloop-thread-0] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values:
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [localhost:51724]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-bcb673c0-fcf5-4124-98a8-1773e55d92e4-1
	client.rack =
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = bcb673c0-fcf5-4124-98a8-1773e55d92e4
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

11:28:30.920 [vert.x-eventloop-thread-0] WARN  org.apache.kafka.clients.consumer.ConsumerConfig - These configurations '[name]' were supplied but are not used yet.
11:28:30.920 [vert.x-eventloop-thread-0] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 3.4.0
11:28:30.920 [vert.x-eventloop-thread-0] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 2e1947d240607d53
11:28:30.920 [vert.x-eventloop-thread-0] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1687544910920
11:28:30.923 [vert.x-kafka-consumer-thread-0] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-bcb673c0-fcf5-4124-98a8-1773e55d92e4-1, groupId=bcb673c0-fcf5-4124-98a8-1773e55d92e4] Subscribed to topic(s): event-3
11:28:31.100 [vert.x-eventloop-thread-0] INFO  com.datasqrl.graphql.kafka.KafkaSinkConsumer - Subscribed to topic: event-3
11:28:31.100 [vert.x-eventloop-thread-0] INFO  com.datasqrl.graphql.GraphQLServer - HTTP server started on port 8888
11:28:31.157 [ForkJoinPool.commonPool-worker-3] INFO  org.apache.flink.api.java.typeutils.TypeExtractor - class com.datasqrl.error.ErrorCollection does not contain a getter for field errors
11:28:31.157 [ForkJoinPool.commonPool-worker-3] INFO  org.apache.flink.api.java.typeutils.TypeExtractor - class com.datasqrl.error.ErrorCollection does not contain a setter for field errors
11:28:31.157 [ForkJoinPool.commonPool-worker-3] INFO  org.apache.flink.api.java.typeutils.TypeExtractor - Class class com.datasqrl.error.ErrorCollection cannot be used as a POJO type because not all fields are valid POJO fields, and must be processed as GenericType. Please read the Flink documentation on "Data Types & Serialization" for details of the effect on performance and schema evolution.
11:28:31.157 [ForkJoinPool.commonPool-worker-3] INFO  org.apache.flink.api.java.typeutils.TypeExtractor - Field InputError#errors will be processed as GenericType. Please read the Flink documentation on "Data Types & Serialization" for details of the effect on performance and schema evolution.
11:28:31.160 [ForkJoinPool.commonPool-worker-3] INFO  org.apache.flink.api.java.typeutils.TypeExtractor - Field Raw#data will be processed as GenericType. Please read the Flink documentation on "Data Types & Serialization" for details of the effect on performance and schema evolution.
11:28:31.160 [ForkJoinPool.commonPool-worker-3] INFO  org.apache.flink.api.java.typeutils.TypeExtractor - class java.util.UUID does not contain a getter for field mostSigBits
11:28:31.160 [ForkJoinPool.commonPool-worker-3] INFO  org.apache.flink.api.java.typeutils.TypeExtractor - class java.util.UUID does not contain a setter for field mostSigBits
11:28:31.160 [ForkJoinPool.commonPool-worker-3] INFO  org.apache.flink.api.java.typeutils.TypeExtractor - Class class java.util.UUID cannot be used as a POJO type because not all fields are valid POJO fields, and must be processed as GenericType. Please read the Flink documentation on "Data Types & Serialization" for details of the effect on performance and schema evolution.
11:28:31.160 [ForkJoinPool.commonPool-worker-3] INFO  org.apache.flink.api.java.typeutils.TypeExtractor - Field Raw#uuid will be processed as GenericType. Please read the Flink documentation on "Data Types & Serialization" for details of the effect on performance and schema evolution.
WARNING: An illegal reflective access operation has occurred
WARNING: Illegal reflective access by org.apache.flink.api.java.ClosureCleaner (file:/Users/henneberger/.m2/repository/org/apache/flink/flink-core/1.16.1/flink-core-1.16.1.jar) to field java.lang.String.value
WARNING: Please consider reporting this to the maintainers of org.apache.flink.api.java.ClosureCleaner
WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
WARNING: All illegal access operations will be denied in a future release
11:28:31.215 [ForkJoinPool.commonPool-worker-3] INFO  org.apache.flink.api.java.typeutils.TypeExtractor - class com.datasqrl.error.ErrorCollection does not contain a getter for field errors
11:28:31.215 [ForkJoinPool.commonPool-worker-3] INFO  org.apache.flink.api.java.typeutils.TypeExtractor - class com.datasqrl.error.ErrorCollection does not contain a setter for field errors
11:28:31.215 [ForkJoinPool.commonPool-worker-3] INFO  org.apache.flink.api.java.typeutils.TypeExtractor - Class class com.datasqrl.error.ErrorCollection cannot be used as a POJO type because not all fields are valid POJO fields, and must be processed as GenericType. Please read the Flink documentation on "Data Types & Serialization" for details of the effect on performance and schema evolution.
11:28:31.215 [ForkJoinPool.commonPool-worker-3] INFO  org.apache.flink.api.java.typeutils.TypeExtractor - Field InputError#errors will be processed as GenericType. Please read the Flink documentation on "Data Types & Serialization" for details of the effect on performance and schema evolution.
11:28:31.215 [ForkJoinPool.commonPool-worker-3] INFO  org.apache.flink.api.java.typeutils.TypeExtractor - Field Named#data will be processed as GenericType. Please read the Flink documentation on "Data Types & Serialization" for details of the effect on performance and schema evolution.
11:28:31.215 [ForkJoinPool.commonPool-worker-3] INFO  org.apache.flink.api.java.typeutils.TypeExtractor - class java.util.UUID does not contain a getter for field mostSigBits
11:28:31.215 [ForkJoinPool.commonPool-worker-3] INFO  org.apache.flink.api.java.typeutils.TypeExtractor - class java.util.UUID does not contain a setter for field mostSigBits
11:28:31.216 [ForkJoinPool.commonPool-worker-3] INFO  org.apache.flink.api.java.typeutils.TypeExtractor - Class class java.util.UUID cannot be used as a POJO type because not all fields are valid POJO fields, and must be processed as GenericType. Please read the Flink documentation on "Data Types & Serialization" for details of the effect on performance and schema evolution.
11:28:31.216 [ForkJoinPool.commonPool-worker-3] INFO  org.apache.flink.api.java.typeutils.TypeExtractor - Field Named#uuid will be processed as GenericType. Please read the Flink documentation on "Data Types & Serialization" for details of the effect on performance and schema evolution.
11:28:31.583 [controller-event-thread] INFO  kafka.controller.KafkaController - [Controller id=0] Processing automatic preferred replica leader election
11:28:32.694 [ForkJoinPool.commonPool-worker-3] INFO  org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils - The configuration option taskmanager.cpu.cores required for local execution is not set, setting it to the maximal possible value.
11:28:32.695 [ForkJoinPool.commonPool-worker-3] INFO  org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils - The configuration option taskmanager.memory.task.heap.size required for local execution is not set, setting it to the maximal possible value.
11:28:32.695 [ForkJoinPool.commonPool-worker-3] INFO  org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils - The configuration option taskmanager.memory.task.off-heap.size required for local execution is not set, setting it to the maximal possible value.
11:28:32.695 [ForkJoinPool.commonPool-worker-3] INFO  org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils - The configuration option taskmanager.memory.network.min required for local execution is not set, setting it to its default value 64 mb.
11:28:32.695 [ForkJoinPool.commonPool-worker-3] INFO  org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils - The configuration option taskmanager.memory.network.max required for local execution is not set, setting it to its default value 64 mb.
11:28:32.695 [ForkJoinPool.commonPool-worker-3] INFO  org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils - The configuration option taskmanager.memory.managed.size required for local execution is not set, setting it to its default value 128 mb.
11:28:32.696 [ForkJoinPool.commonPool-worker-3] INFO  org.apache.flink.runtime.minicluster.MiniCluster - Starting Flink Mini Cluster
11:28:32.962 [ForkJoinPool.commonPool-worker-3] INFO  org.apache.flink.runtime.minicluster.MiniCluster - Starting Metrics Registry
11:28:33.009 [ForkJoinPool.commonPool-worker-3] INFO  org.apache.flink.runtime.metrics.MetricRegistryImpl - No metrics reporter configured, no metrics will be exposed/reported.
11:28:33.009 [ForkJoinPool.commonPool-worker-3] INFO  org.apache.flink.runtime.minicluster.MiniCluster - Starting RPC Service(s)
11:28:33.018 [ForkJoinPool.commonPool-worker-3] INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils - Trying to start local actor system
11:28:33.407 [flink-akka.actor.default-dispatcher-5] INFO  akka.event.slf4j.Slf4jLogger - Slf4jLogger started
11:28:33.474 [ForkJoinPool.commonPool-worker-3] INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils - Actor system started at akka://flink
11:28:33.485 [ForkJoinPool.commonPool-worker-3] INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils - Trying to start local actor system
11:28:33.492 [flink-metrics-6] INFO  akka.event.slf4j.Slf4jLogger - Slf4jLogger started
11:28:33.496 [ForkJoinPool.commonPool-worker-3] INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils - Actor system started at akka://flink-metrics
11:28:33.506 [ForkJoinPool.commonPool-worker-3] INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService - Starting RPC endpoint for org.apache.flink.runtime.metrics.dump.MetricQueryService at akka://flink-metrics/user/rpc/MetricQueryService .
11:28:33.525 [ForkJoinPool.commonPool-worker-3] INFO  org.apache.flink.runtime.blob.BlobServer - Created BLOB server storage directory /var/folders/w3/dxm786y50_b8p5tj7xktkngh0000gn/T/minicluster_989f21e6f40680e2d267ea4ab487e61c/blobStorage
11:28:33.528 [ForkJoinPool.commonPool-worker-3] INFO  org.apache.flink.runtime.blob.BlobServer - Started BLOB server at 0.0.0.0:51732 - max concurrent requests: 50 - max backlog: 1000
11:28:33.665 [ForkJoinPool.commonPool-worker-3] WARN  org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
11:28:33.700 [ForkJoinPool.commonPool-worker-3] INFO  org.apache.flink.runtime.security.token.KerberosDelegationTokenManagerFactory - Cannot use kerberos delegation token manager no valid kerberos credentials provided.
11:28:33.702 [ForkJoinPool.commonPool-worker-3] INFO  org.apache.flink.runtime.blob.PermanentBlobCache - Created BLOB cache storage directory /var/folders/w3/dxm786y50_b8p5tj7xktkngh0000gn/T/minicluster_989f21e6f40680e2d267ea4ab487e61c/blobStorage
11:28:33.703 [ForkJoinPool.commonPool-worker-3] INFO  org.apache.flink.runtime.blob.TransientBlobCache - Created BLOB cache storage directory /var/folders/w3/dxm786y50_b8p5tj7xktkngh0000gn/T/minicluster_989f21e6f40680e2d267ea4ab487e61c/blobStorage
11:28:33.704 [ForkJoinPool.commonPool-worker-3] INFO  org.apache.flink.runtime.minicluster.MiniCluster - Starting 1 TaskManager(s)
11:28:33.708 [ForkJoinPool.commonPool-worker-3] INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner - Starting TaskManager with ResourceID: d07bd3b6-9e42-41a8-9a36-13b767338620
11:28:33.715 [ForkJoinPool.commonPool-worker-3] INFO  org.apache.flink.runtime.taskexecutor.TaskManagerServices - Temporary file directory '/var/folders/w3/dxm786y50_b8p5tj7xktkngh0000gn/T': total 228 GB, usable 23 GB (10.09% usable)
11:28:33.718 [ForkJoinPool.commonPool-worker-3] INFO  org.apache.flink.runtime.io.disk.iomanager.IOManager - Created a new FileChannelManager for spilling of task related data to disk (joins, sorting, ...). Used directories:
	/var/folders/w3/dxm786y50_b8p5tj7xktkngh0000gn/T/flink-io-b23f7d97-3eae-47ce-80cd-a38fa1615b7b
11:28:33.725 [ForkJoinPool.commonPool-worker-3] INFO  org.apache.flink.runtime.io.network.NettyShuffleServiceFactory - Created a new FileChannelManager for storing result partitions of BLOCKING shuffles. Used directories:
	/var/folders/w3/dxm786y50_b8p5tj7xktkngh0000gn/T/flink-netty-shuffle-950e8c85-a420-4787-9ba8-8f9ef1539d00
11:28:33.742 [ForkJoinPool.commonPool-worker-3] INFO  org.apache.flink.runtime.io.network.buffer.NetworkBufferPool - Allocated 64 MB for network buffer pool (number of memory segments: 2048, bytes per segment: 32768).
11:28:33.753 [ForkJoinPool.commonPool-worker-3] INFO  org.apache.flink.runtime.io.network.NettyShuffleEnvironment - Starting the network environment and its components.
11:28:33.755 [ForkJoinPool.commonPool-worker-3] INFO  org.apache.flink.runtime.taskexecutor.KvStateService - Starting the kvState service and its components.
11:28:33.778 [ForkJoinPool.commonPool-worker-3] INFO  org.apache.flink.configuration.Configuration - Config uses fallback configuration key 'akka.ask.timeout' instead of key 'taskmanager.slot.timeout'
11:28:33.788 [ForkJoinPool.commonPool-worker-3] INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService - Starting RPC endpoint for org.apache.flink.runtime.taskexecutor.TaskExecutor at akka://flink/user/rpc/taskmanager_0 .
11:28:33.824 [flink-akka.actor.default-dispatcher-5] INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService - Start job leader service.
11:28:33.825 [flink-akka.actor.default-dispatcher-5] INFO  org.apache.flink.runtime.filecache.FileCache - User file cache uses directory /var/folders/w3/dxm786y50_b8p5tj7xktkngh0000gn/T/flink-dist-cache-2d1c916a-8c46-4782-ac19-fbc1c003d764
11:28:33.851 [ForkJoinPool.commonPool-worker-3] INFO  org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint - Starting rest endpoint.
11:28:33.852 [ForkJoinPool.commonPool-worker-3] INFO  org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint - Failed to load web based job submission extension. Probable reason: flink-runtime-web is not in the classpath.
11:28:33.904 [ForkJoinPool.commonPool-worker-3] WARN  org.apache.flink.runtime.webmonitor.WebMonitorUtils - Log file environment variable 'log.file' is not set.
11:28:33.904 [ForkJoinPool.commonPool-worker-3] WARN  org.apache.flink.runtime.webmonitor.WebMonitorUtils - JobManager log files are unavailable in the web dashboard. Log file location not found in environment variable 'log.file' or configuration key 'web.log.path'.
11:28:33.998 [ForkJoinPool.commonPool-worker-3] INFO  org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint - Rest endpoint listening at localhost:51733
11:28:33.999 [ForkJoinPool.commonPool-worker-3] INFO  org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService - Proposing leadership to contender http://localhost:51733
11:28:34.000 [mini-cluster-io-thread-1] INFO  org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint - http://localhost:51733 was granted leadership with leaderSessionID=a2aebfc8-506b-4808-897e-8a561abfec4d
11:28:34.000 [mini-cluster-io-thread-1] INFO  org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService - Received confirmation of leadership for leader http://localhost:51733 , session=a2aebfc8-506b-4808-897e-8a561abfec4d
11:28:34.006 [ForkJoinPool.commonPool-worker-3] INFO  org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService - Proposing leadership to contender LeaderContender: DefaultDispatcherRunner
11:28:34.007 [ForkJoinPool.commonPool-worker-3] INFO  org.apache.flink.runtime.resourcemanager.ResourceManagerServiceImpl - Starting resource manager service.
11:28:34.007 [ForkJoinPool.commonPool-worker-3] INFO  org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService - Proposing leadership to contender LeaderContender: ResourceManagerServiceImpl
11:28:34.007 [mini-cluster-io-thread-2] INFO  org.apache.flink.runtime.dispatcher.runner.DefaultDispatcherRunner - DefaultDispatcherRunner was granted leadership with leader id eb72648b-cac8-4e5f-b9e2-d369579dbdd2. Creating new DispatcherLeaderProcess.
11:28:34.007 [pool-10-thread-1] INFO  org.apache.flink.runtime.resourcemanager.ResourceManagerServiceImpl - Resource manager service is granted leadership with session id 351c05a4-0994-4b1a-bcaa-e748654f5964.
11:28:34.009 [ForkJoinPool.commonPool-worker-3] INFO  org.apache.flink.runtime.minicluster.MiniCluster - Flink Mini Cluster started successfully
11:28:34.010 [mini-cluster-io-thread-2] INFO  org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess - Start SessionDispatcherLeaderProcess.
11:28:34.012 [mini-cluster-io-thread-1] INFO  org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess - Recover all persisted job graphs that are not finished, yet.
11:28:34.012 [mini-cluster-io-thread-1] INFO  org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess - Successfully recovered 0 persisted job graphs.
11:28:34.019 [pool-10-thread-1] INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService - Starting RPC endpoint for org.apache.flink.runtime.resourcemanager.StandaloneResourceManager at akka://flink/user/rpc/resourcemanager_1 .
11:28:34.023 [mini-cluster-io-thread-1] INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService - Starting RPC endpoint for org.apache.flink.runtime.dispatcher.StandaloneDispatcher at akka://flink/user/rpc/dispatcher_2 .
11:28:34.027 [flink-akka.actor.default-dispatcher-5] INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager - Starting the resource manager.
11:28:34.032 [mini-cluster-io-thread-1] INFO  org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService - Received confirmation of leadership for leader akka://flink/user/rpc/dispatcher_2 , session=eb72648b-cac8-4e5f-b9e2-d369579dbdd2
11:28:34.033 [mini-cluster-io-thread-3] INFO  org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService - Received confirmation of leadership for leader akka://flink/user/rpc/resourcemanager_1 , session=351c05a4-0994-4b1a-bcaa-e748654f5964
11:28:34.050 [flink-akka.actor.default-dispatcher-8] INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor - Connecting to ResourceManager akka://flink/user/rpc/resourcemanager_1(bcaae748654f5964351c05a409944b1a).
11:28:34.054 [flink-akka.actor.default-dispatcher-5] INFO  org.apache.flink.runtime.dispatcher.StandaloneDispatcher - Received JobGraph submission 'insert-into_default_catalog.default_database.event$3,default_catalog.default_database.event$3$1,default_catalog.default_database.errors_internal_sink' (d86be44a73f13cd595ed7d7754c9e830).
11:28:34.054 [flink-akka.actor.default-dispatcher-5] INFO  org.apache.flink.runtime.dispatcher.StandaloneDispatcher - Submitting job 'insert-into_default_catalog.default_database.event$3,default_catalog.default_database.event$3$1,default_catalog.default_database.errors_internal_sink' (d86be44a73f13cd595ed7d7754c9e830).
11:28:34.056 [flink-akka.actor.default-dispatcher-9] INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor - Resolved ResourceManager address, beginning registration
11:28:34.058 [flink-akka.actor.default-dispatcher-8] INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager - Registering TaskManager with ResourceID d07bd3b6-9e42-41a8-9a36-13b767338620 (akka://flink/user/rpc/taskmanager_0) at ResourceManager
11:28:34.059 [flink-akka.actor.default-dispatcher-7] INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor - Successful registration at resource manager akka://flink/user/rpc/resourcemanager_1 under registration id 34d08e88814bff7e0571d6e4b322fedc.
11:28:34.063 [flink-akka.actor.default-dispatcher-5] INFO  org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService - Proposing leadership to contender LeaderContender: JobMasterServiceLeadershipRunner
11:28:34.070 [jobmanager-io-thread-1] INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService - Starting RPC endpoint for org.apache.flink.runtime.jobmaster.JobMaster at akka://flink/user/rpc/jobmanager_3 .
11:28:34.074 [jobmanager-io-thread-1] INFO  org.apache.flink.runtime.jobmaster.JobMaster - Initializing job 'insert-into_default_catalog.default_database.event$3,default_catalog.default_database.event$3$1,default_catalog.default_database.errors_internal_sink' (d86be44a73f13cd595ed7d7754c9e830).
11:28:34.091 [jobmanager-io-thread-1] INFO  org.apache.flink.runtime.jobmaster.JobMaster - Using restart back off time strategy NoRestartBackoffTimeStrategy for insert-into_default_catalog.default_database.event$3,default_catalog.default_database.event$3$1,default_catalog.default_database.errors_internal_sink (d86be44a73f13cd595ed7d7754c9e830).
11:28:34.110 [jobmanager-io-thread-1] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph - Created execution graph d220afe491fa016e76b29529ab091deb for job d86be44a73f13cd595ed7d7754c9e830.
11:28:34.119 [jobmanager-io-thread-1] INFO  org.apache.flink.runtime.jobmaster.JobMaster - Running initialization on master for job insert-into_default_catalog.default_database.event$3,default_catalog.default_database.event$3$1,default_catalog.default_database.errors_internal_sink (d86be44a73f13cd595ed7d7754c9e830).
11:28:34.119 [jobmanager-io-thread-1] INFO  org.apache.flink.runtime.jobmaster.JobMaster - Successfully ran initialization on master in 0 ms.
11:28:34.142 [jobmanager-io-thread-1] INFO  org.apache.flink.runtime.scheduler.adapter.DefaultExecutionTopology - Built 8 new pipelined regions in 1 ms, total 8 pipelined regions currently.
11:28:34.147 [jobmanager-io-thread-1] INFO  org.apache.flink.runtime.jobmaster.JobMaster - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@c0c5642
11:28:34.147 [jobmanager-io-thread-1] INFO  org.apache.flink.runtime.state.StateBackendLoader - State backend loader loads the state backend as HashMapStateBackend
11:28:34.148 [jobmanager-io-thread-1] INFO  org.apache.flink.runtime.jobmaster.JobMaster - Checkpoint storage is set to 'jobmanager'
11:28:34.168 [jobmanager-io-thread-1] INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator - No checkpoint found during restore.
11:28:34.173 [jobmanager-io-thread-1] INFO  org.apache.flink.runtime.jobmaster.JobMaster - Using failover strategy org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy@1aa9c85a for insert-into_default_catalog.default_database.event$3,default_catalog.default_database.event$3$1,default_catalog.default_database.errors_internal_sink (d86be44a73f13cd595ed7d7754c9e830).
11:28:34.179 [jobmanager-io-thread-1] INFO  org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService - Received confirmation of leadership for leader akka://flink/user/rpc/jobmanager_3 , session=1485b5ca-b781-47fd-8e17-ae1814d6d1ae
11:28:34.180 [flink-akka.actor.default-dispatcher-5] INFO  org.apache.flink.runtime.jobmaster.JobMaster - Starting execution of job 'insert-into_default_catalog.default_database.event$3,default_catalog.default_database.event$3$1,default_catalog.default_database.errors_internal_sink' (d86be44a73f13cd595ed7d7754c9e830) under job master id 8e17ae1814d6d1ae1485b5cab78147fd.
11:28:34.182 [flink-akka.actor.default-dispatcher-5] INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator - Starting split enumerator for source Source: createevent.
11:28:34.183 [flink-akka.actor.default-dispatcher-5] INFO  org.apache.flink.runtime.jobmaster.JobMaster - Starting scheduling with scheduling strategy [org.apache.flink.runtime.scheduler.strategy.PipelinedRegionSchedulingStrategy]
11:28:34.183 [flink-akka.actor.default-dispatcher-5] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph - Job insert-into_default_catalog.default_database.event$3,default_catalog.default_database.event$3$1,default_catalog.default_database.errors_internal_sink (d86be44a73f13cd595ed7d7754c9e830) switched from state CREATED to RUNNING.
11:28:34.184 [SourceCoordinator-Source: createevent] INFO  org.apache.kafka.clients.admin.AdminClientConfig - AdminClientConfig values:
	auto.include.jmx.reporter = true
	bootstrap.servers = [localhost:51724]
	client.dns.lookup = use_all_dns_ips
	client.id = createevent-45f40691-1392-43a5-9cce-4fec91cfd2c4-enumerator-admin-client
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

11:28:34.185 [SourceCoordinator-Source: createevent] WARN  org.apache.kafka.clients.admin.AdminClientConfig - These configurations '[key.deserializer, value.deserializer, enable.auto.commit, client.id.prefix, group.id, partition.discovery.interval.ms, auto.offset.reset]' were supplied but are not used yet.
11:28:34.185 [SourceCoordinator-Source: createevent] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 3.4.0
11:28:34.185 [SourceCoordinator-Source: createevent] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 2e1947d240607d53
11:28:34.185 [SourceCoordinator-Source: createevent] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1687544914185
11:28:34.185 [SourceCoordinator-Source: createevent] INFO  org.apache.flink.connector.kafka.source.enumerator.KafkaSourceEnumerator - Starting the KafkaSourceEnumerator for consumer group createevent-45f40691-1392-43a5-9cce-4fec91cfd2c4 without periodic partition discovery.
11:28:34.187 [flink-akka.actor.default-dispatcher-5] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph - Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (1/8) (d220afe491fa016e76b29529ab091deb_cbc357ccb763df2852fee8c4fc7d55f2_0_0) switched from CREATED to SCHEDULED.
11:28:34.187 [flink-akka.actor.default-dispatcher-5] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph - Flat Map -> *anonymous_datastream_source$1*[5] -> Sink: errors_internal_sink[6] (1/8) (d220afe491fa016e76b29529ab091deb_5042f7303f448a048cdf298e40198247_0_0) switched from CREATED to SCHEDULED.
11:28:34.198 [flink-akka.actor.default-dispatcher-5] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph - Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (2/8) (d220afe491fa016e76b29529ab091deb_cbc357ccb763df2852fee8c4fc7d55f2_1_0) switched from CREATED to SCHEDULED.
11:28:34.198 [flink-akka.actor.default-dispatcher-5] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph - Flat Map -> *anonymous_datastream_source$1*[5] -> Sink: errors_internal_sink[6] (2/8) (d220afe491fa016e76b29529ab091deb_5042f7303f448a048cdf298e40198247_1_0) switched from CREATED to SCHEDULED.
11:28:34.198 [SourceCoordinator-Source: createevent] INFO  org.apache.flink.connector.kafka.source.enumerator.KafkaSourceEnumerator - Discovered new partitions: [schema-createevent-0]
11:28:34.198 [flink-akka.actor.default-dispatcher-5] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph - Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (3/8) (d220afe491fa016e76b29529ab091deb_cbc357ccb763df2852fee8c4fc7d55f2_2_0) switched from CREATED to SCHEDULED.
11:28:34.198 [flink-akka.actor.default-dispatcher-5] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph - Flat Map -> *anonymous_datastream_source$1*[5] -> Sink: errors_internal_sink[6] (3/8) (d220afe491fa016e76b29529ab091deb_5042f7303f448a048cdf298e40198247_2_0) switched from CREATED to SCHEDULED.
11:28:34.198 [flink-akka.actor.default-dispatcher-5] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph - Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (4/8) (d220afe491fa016e76b29529ab091deb_cbc357ccb763df2852fee8c4fc7d55f2_3_0) switched from CREATED to SCHEDULED.
11:28:34.198 [flink-akka.actor.default-dispatcher-5] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph - Flat Map -> *anonymous_datastream_source$1*[5] -> Sink: errors_internal_sink[6] (4/8) (d220afe491fa016e76b29529ab091deb_5042f7303f448a048cdf298e40198247_3_0) switched from CREATED to SCHEDULED.
11:28:34.198 [flink-akka.actor.default-dispatcher-5] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph - Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (5/8) (d220afe491fa016e76b29529ab091deb_cbc357ccb763df2852fee8c4fc7d55f2_4_0) switched from CREATED to SCHEDULED.
11:28:34.199 [flink-akka.actor.default-dispatcher-5] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph - Flat Map -> *anonymous_datastream_source$1*[5] -> Sink: errors_internal_sink[6] (5/8) (d220afe491fa016e76b29529ab091deb_5042f7303f448a048cdf298e40198247_4_0) switched from CREATED to SCHEDULED.
11:28:34.199 [flink-akka.actor.default-dispatcher-5] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph - Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (6/8) (d220afe491fa016e76b29529ab091deb_cbc357ccb763df2852fee8c4fc7d55f2_5_0) switched from CREATED to SCHEDULED.
11:28:34.199 [flink-akka.actor.default-dispatcher-5] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph - Flat Map -> *anonymous_datastream_source$1*[5] -> Sink: errors_internal_sink[6] (6/8) (d220afe491fa016e76b29529ab091deb_5042f7303f448a048cdf298e40198247_5_0) switched from CREATED to SCHEDULED.
11:28:34.199 [flink-akka.actor.default-dispatcher-5] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph - Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (7/8) (d220afe491fa016e76b29529ab091deb_cbc357ccb763df2852fee8c4fc7d55f2_6_0) switched from CREATED to SCHEDULED.
11:28:34.199 [flink-akka.actor.default-dispatcher-5] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph - Flat Map -> *anonymous_datastream_source$1*[5] -> Sink: errors_internal_sink[6] (7/8) (d220afe491fa016e76b29529ab091deb_5042f7303f448a048cdf298e40198247_6_0) switched from CREATED to SCHEDULED.
11:28:34.199 [flink-akka.actor.default-dispatcher-5] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph - Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (8/8) (d220afe491fa016e76b29529ab091deb_cbc357ccb763df2852fee8c4fc7d55f2_7_0) switched from CREATED to SCHEDULED.
11:28:34.199 [flink-akka.actor.default-dispatcher-5] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph - Flat Map -> *anonymous_datastream_source$1*[5] -> Sink: errors_internal_sink[6] (8/8) (d220afe491fa016e76b29529ab091deb_5042f7303f448a048cdf298e40198247_7_0) switched from CREATED to SCHEDULED.
11:28:34.199 [flink-akka.actor.default-dispatcher-5] INFO  org.apache.flink.runtime.jobmaster.JobMaster - Connecting to ResourceManager akka://flink/user/rpc/resourcemanager_1(bcaae748654f5964351c05a409944b1a)
11:28:34.201 [flink-akka.actor.default-dispatcher-5] INFO  org.apache.flink.runtime.jobmaster.JobMaster - Resolved ResourceManager address, beginning registration
11:28:34.202 [flink-akka.actor.default-dispatcher-7] INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager - Registering job manager 8e17ae1814d6d1ae1485b5cab78147fd@akka://flink/user/rpc/jobmanager_3 for job d86be44a73f13cd595ed7d7754c9e830.
11:28:34.205 [flink-akka.actor.default-dispatcher-7] INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager - Registered job manager 8e17ae1814d6d1ae1485b5cab78147fd@akka://flink/user/rpc/jobmanager_3 for job d86be44a73f13cd595ed7d7754c9e830.
11:28:34.206 [flink-akka.actor.default-dispatcher-7] INFO  org.apache.flink.runtime.jobmaster.JobMaster - JobManager successfully registered at ResourceManager, leader id: bcaae748654f5964351c05a409944b1a.
11:28:34.207 [flink-akka.actor.default-dispatcher-9] INFO  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager - Received resource requirements from job d86be44a73f13cd595ed7d7754c9e830: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=8}]
11:28:34.277 [flink-akka.actor.default-dispatcher-7] INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor - Receive slot request 3f8389866173750901ea45cc1a40df7e for job d86be44a73f13cd595ed7d7754c9e830 from resource manager with leader id bcaae748654f5964351c05a409944b1a.
11:28:34.281 [flink-akka.actor.default-dispatcher-7] INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor - Allocated slot for 3f8389866173750901ea45cc1a40df7e.
11:28:34.282 [flink-akka.actor.default-dispatcher-7] INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService - Add job d86be44a73f13cd595ed7d7754c9e830 for job leader monitoring.
11:28:34.283 [mini-cluster-io-thread-4] INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService - Try to register at job manager akka://flink/user/rpc/jobmanager_3 with leader id 1485b5ca-b781-47fd-8e17-ae1814d6d1ae.
11:28:34.283 [flink-akka.actor.default-dispatcher-7] INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor - Receive slot request 17b28bd5da27f3580d7f3117b1f4ac5a for job d86be44a73f13cd595ed7d7754c9e830 from resource manager with leader id bcaae748654f5964351c05a409944b1a.
11:28:34.283 [flink-akka.actor.default-dispatcher-7] INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor - Allocated slot for 17b28bd5da27f3580d7f3117b1f4ac5a.
11:28:34.284 [flink-akka.actor.default-dispatcher-7] INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor - Receive slot request 83639796df6d89c97d603a928bd3a4de for job d86be44a73f13cd595ed7d7754c9e830 from resource manager with leader id bcaae748654f5964351c05a409944b1a.
11:28:34.284 [flink-akka.actor.default-dispatcher-7] INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor - Allocated slot for 83639796df6d89c97d603a928bd3a4de.
11:28:34.284 [flink-akka.actor.default-dispatcher-7] INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor - Receive slot request 1f3278fa2fd8460416138b341c977e29 for job d86be44a73f13cd595ed7d7754c9e830 from resource manager with leader id bcaae748654f5964351c05a409944b1a.
11:28:34.284 [flink-akka.actor.default-dispatcher-7] INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor - Allocated slot for 1f3278fa2fd8460416138b341c977e29.
11:28:34.284 [flink-akka.actor.default-dispatcher-7] INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor - Receive slot request 40a113d8b1aa3eb00443a7445bad81be for job d86be44a73f13cd595ed7d7754c9e830 from resource manager with leader id bcaae748654f5964351c05a409944b1a.
11:28:34.284 [flink-akka.actor.default-dispatcher-7] INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor - Allocated slot for 40a113d8b1aa3eb00443a7445bad81be.
11:28:34.284 [flink-akka.actor.default-dispatcher-5] INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService - Resolved JobManager address, beginning registration
11:28:34.284 [flink-akka.actor.default-dispatcher-7] INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor - Receive slot request dc440c1e87511e0479246764199e2c2f for job d86be44a73f13cd595ed7d7754c9e830 from resource manager with leader id bcaae748654f5964351c05a409944b1a.
11:28:34.284 [flink-akka.actor.default-dispatcher-7] INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor - Allocated slot for dc440c1e87511e0479246764199e2c2f.
11:28:34.284 [flink-akka.actor.default-dispatcher-7] INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor - Receive slot request ce6710659f99419d2e3ffdd423c38317 for job d86be44a73f13cd595ed7d7754c9e830 from resource manager with leader id bcaae748654f5964351c05a409944b1a.
11:28:34.284 [flink-akka.actor.default-dispatcher-7] INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor - Allocated slot for ce6710659f99419d2e3ffdd423c38317.
11:28:34.284 [flink-akka.actor.default-dispatcher-7] INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor - Receive slot request b5d8aae146528392beb88bf17f3a564c for job d86be44a73f13cd595ed7d7754c9e830 from resource manager with leader id bcaae748654f5964351c05a409944b1a.
11:28:34.284 [flink-akka.actor.default-dispatcher-7] INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor - Allocated slot for b5d8aae146528392beb88bf17f3a564c.
11:28:34.289 [flink-akka.actor.default-dispatcher-8] INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService - Successful registration at job manager akka://flink/user/rpc/jobmanager_3 for job d86be44a73f13cd595ed7d7754c9e830.
11:28:34.290 [flink-akka.actor.default-dispatcher-8] INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor - Establish JobManager connection for job d86be44a73f13cd595ed7d7754c9e830.
11:28:34.291 [flink-akka.actor.default-dispatcher-8] INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor - Offer reserved slots to the leader of job d86be44a73f13cd595ed7d7754c9e830.
11:28:34.296 [flink-akka.actor.default-dispatcher-7] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph - Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (1/8) (d220afe491fa016e76b29529ab091deb_cbc357ccb763df2852fee8c4fc7d55f2_0_0) switched from SCHEDULED to DEPLOYING.
11:28:34.296 [flink-akka.actor.default-dispatcher-7] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph - Deploying Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (1/8) (attempt #0) with attempt id d220afe491fa016e76b29529ab091deb_cbc357ccb763df2852fee8c4fc7d55f2_0_0 and vertex id cbc357ccb763df2852fee8c4fc7d55f2_0 to d07bd3b6-9e42-41a8-9a36-13b767338620 @ localhost (dataPort=-1) with allocation id b5d8aae146528392beb88bf17f3a564c
11:28:34.299 [flink-akka.actor.default-dispatcher-7] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph - Flat Map -> *anonymous_datastream_source$1*[5] -> Sink: errors_internal_sink[6] (1/8) (d220afe491fa016e76b29529ab091deb_5042f7303f448a048cdf298e40198247_0_0) switched from SCHEDULED to DEPLOYING.
11:28:34.300 [flink-akka.actor.default-dispatcher-7] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph - Deploying Flat Map -> *anonymous_datastream_source$1*[5] -> Sink: errors_internal_sink[6] (1/8) (attempt #0) with attempt id d220afe491fa016e76b29529ab091deb_5042f7303f448a048cdf298e40198247_0_0 and vertex id 5042f7303f448a048cdf298e40198247_0 to d07bd3b6-9e42-41a8-9a36-13b767338620 @ localhost (dataPort=-1) with allocation id b5d8aae146528392beb88bf17f3a564c
11:28:34.300 [flink-akka.actor.default-dispatcher-8] INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl - Activate slot b5d8aae146528392beb88bf17f3a564c.
11:28:34.304 [flink-akka.actor.default-dispatcher-7] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph - Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (2/8) (d220afe491fa016e76b29529ab091deb_cbc357ccb763df2852fee8c4fc7d55f2_1_0) switched from SCHEDULED to DEPLOYING.
11:28:34.304 [flink-akka.actor.default-dispatcher-7] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph - Deploying Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (2/8) (attempt #0) with attempt id d220afe491fa016e76b29529ab091deb_cbc357ccb763df2852fee8c4fc7d55f2_1_0 and vertex id cbc357ccb763df2852fee8c4fc7d55f2_1 to d07bd3b6-9e42-41a8-9a36-13b767338620 @ localhost (dataPort=-1) with allocation id 3f8389866173750901ea45cc1a40df7e
11:28:34.305 [flink-akka.actor.default-dispatcher-7] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph - Flat Map -> *anonymous_datastream_source$1*[5] -> Sink: errors_internal_sink[6] (2/8) (d220afe491fa016e76b29529ab091deb_5042f7303f448a048cdf298e40198247_1_0) switched from SCHEDULED to DEPLOYING.
11:28:34.305 [flink-akka.actor.default-dispatcher-7] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph - Deploying Flat Map -> *anonymous_datastream_source$1*[5] -> Sink: errors_internal_sink[6] (2/8) (attempt #0) with attempt id d220afe491fa016e76b29529ab091deb_5042f7303f448a048cdf298e40198247_1_0 and vertex id 5042f7303f448a048cdf298e40198247_1 to d07bd3b6-9e42-41a8-9a36-13b767338620 @ localhost (dataPort=-1) with allocation id 3f8389866173750901ea45cc1a40df7e
11:28:34.305 [flink-akka.actor.default-dispatcher-7] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph - Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (3/8) (d220afe491fa016e76b29529ab091deb_cbc357ccb763df2852fee8c4fc7d55f2_2_0) switched from SCHEDULED to DEPLOYING.
11:28:34.305 [flink-akka.actor.default-dispatcher-7] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph - Deploying Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (3/8) (attempt #0) with attempt id d220afe491fa016e76b29529ab091deb_cbc357ccb763df2852fee8c4fc7d55f2_2_0 and vertex id cbc357ccb763df2852fee8c4fc7d55f2_2 to d07bd3b6-9e42-41a8-9a36-13b767338620 @ localhost (dataPort=-1) with allocation id 40a113d8b1aa3eb00443a7445bad81be
11:28:34.306 [flink-akka.actor.default-dispatcher-7] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph - Flat Map -> *anonymous_datastream_source$1*[5] -> Sink: errors_internal_sink[6] (3/8) (d220afe491fa016e76b29529ab091deb_5042f7303f448a048cdf298e40198247_2_0) switched from SCHEDULED to DEPLOYING.
11:28:34.306 [flink-akka.actor.default-dispatcher-7] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph - Deploying Flat Map -> *anonymous_datastream_source$1*[5] -> Sink: errors_internal_sink[6] (3/8) (attempt #0) with attempt id d220afe491fa016e76b29529ab091deb_5042f7303f448a048cdf298e40198247_2_0 and vertex id 5042f7303f448a048cdf298e40198247_2 to d07bd3b6-9e42-41a8-9a36-13b767338620 @ localhost (dataPort=-1) with allocation id 40a113d8b1aa3eb00443a7445bad81be
11:28:34.306 [flink-akka.actor.default-dispatcher-7] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph - Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (4/8) (d220afe491fa016e76b29529ab091deb_cbc357ccb763df2852fee8c4fc7d55f2_3_0) switched from SCHEDULED to DEPLOYING.
11:28:34.306 [flink-akka.actor.default-dispatcher-7] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph - Deploying Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (4/8) (attempt #0) with attempt id d220afe491fa016e76b29529ab091deb_cbc357ccb763df2852fee8c4fc7d55f2_3_0 and vertex id cbc357ccb763df2852fee8c4fc7d55f2_3 to d07bd3b6-9e42-41a8-9a36-13b767338620 @ localhost (dataPort=-1) with allocation id ce6710659f99419d2e3ffdd423c38317
11:28:34.306 [flink-akka.actor.default-dispatcher-7] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph - Flat Map -> *anonymous_datastream_source$1*[5] -> Sink: errors_internal_sink[6] (4/8) (d220afe491fa016e76b29529ab091deb_5042f7303f448a048cdf298e40198247_3_0) switched from SCHEDULED to DEPLOYING.
11:28:34.306 [flink-akka.actor.default-dispatcher-7] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph - Deploying Flat Map -> *anonymous_datastream_source$1*[5] -> Sink: errors_internal_sink[6] (4/8) (attempt #0) with attempt id d220afe491fa016e76b29529ab091deb_5042f7303f448a048cdf298e40198247_3_0 and vertex id 5042f7303f448a048cdf298e40198247_3 to d07bd3b6-9e42-41a8-9a36-13b767338620 @ localhost (dataPort=-1) with allocation id ce6710659f99419d2e3ffdd423c38317
11:28:34.307 [flink-akka.actor.default-dispatcher-7] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph - Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (5/8) (d220afe491fa016e76b29529ab091deb_cbc357ccb763df2852fee8c4fc7d55f2_4_0) switched from SCHEDULED to DEPLOYING.
11:28:34.307 [flink-akka.actor.default-dispatcher-7] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph - Deploying Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (5/8) (attempt #0) with attempt id d220afe491fa016e76b29529ab091deb_cbc357ccb763df2852fee8c4fc7d55f2_4_0 and vertex id cbc357ccb763df2852fee8c4fc7d55f2_4 to d07bd3b6-9e42-41a8-9a36-13b767338620 @ localhost (dataPort=-1) with allocation id 1f3278fa2fd8460416138b341c977e29
11:28:34.308 [flink-akka.actor.default-dispatcher-7] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph - Flat Map -> *anonymous_datastream_source$1*[5] -> Sink: errors_internal_sink[6] (5/8) (d220afe491fa016e76b29529ab091deb_5042f7303f448a048cdf298e40198247_4_0) switched from SCHEDULED to DEPLOYING.
11:28:34.308 [flink-akka.actor.default-dispatcher-7] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph - Deploying Flat Map -> *anonymous_datastream_source$1*[5] -> Sink: errors_internal_sink[6] (5/8) (attempt #0) with attempt id d220afe491fa016e76b29529ab091deb_5042f7303f448a048cdf298e40198247_4_0 and vertex id 5042f7303f448a048cdf298e40198247_4 to d07bd3b6-9e42-41a8-9a36-13b767338620 @ localhost (dataPort=-1) with allocation id 1f3278fa2fd8460416138b341c977e29
11:28:34.308 [flink-akka.actor.default-dispatcher-7] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph - Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (6/8) (d220afe491fa016e76b29529ab091deb_cbc357ccb763df2852fee8c4fc7d55f2_5_0) switched from SCHEDULED to DEPLOYING.
11:28:34.308 [flink-akka.actor.default-dispatcher-7] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph - Deploying Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (6/8) (attempt #0) with attempt id d220afe491fa016e76b29529ab091deb_cbc357ccb763df2852fee8c4fc7d55f2_5_0 and vertex id cbc357ccb763df2852fee8c4fc7d55f2_5 to d07bd3b6-9e42-41a8-9a36-13b767338620 @ localhost (dataPort=-1) with allocation id 17b28bd5da27f3580d7f3117b1f4ac5a
11:28:34.308 [flink-akka.actor.default-dispatcher-7] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph - Flat Map -> *anonymous_datastream_source$1*[5] -> Sink: errors_internal_sink[6] (6/8) (d220afe491fa016e76b29529ab091deb_5042f7303f448a048cdf298e40198247_5_0) switched from SCHEDULED to DEPLOYING.
11:28:34.308 [flink-akka.actor.default-dispatcher-7] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph - Deploying Flat Map -> *anonymous_datastream_source$1*[5] -> Sink: errors_internal_sink[6] (6/8) (attempt #0) with attempt id d220afe491fa016e76b29529ab091deb_5042f7303f448a048cdf298e40198247_5_0 and vertex id 5042f7303f448a048cdf298e40198247_5 to d07bd3b6-9e42-41a8-9a36-13b767338620 @ localhost (dataPort=-1) with allocation id 17b28bd5da27f3580d7f3117b1f4ac5a
11:28:34.308 [flink-akka.actor.default-dispatcher-7] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph - Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (7/8) (d220afe491fa016e76b29529ab091deb_cbc357ccb763df2852fee8c4fc7d55f2_6_0) switched from SCHEDULED to DEPLOYING.
11:28:34.308 [flink-akka.actor.default-dispatcher-7] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph - Deploying Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (7/8) (attempt #0) with attempt id d220afe491fa016e76b29529ab091deb_cbc357ccb763df2852fee8c4fc7d55f2_6_0 and vertex id cbc357ccb763df2852fee8c4fc7d55f2_6 to d07bd3b6-9e42-41a8-9a36-13b767338620 @ localhost (dataPort=-1) with allocation id 83639796df6d89c97d603a928bd3a4de
11:28:34.309 [flink-akka.actor.default-dispatcher-7] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph - Flat Map -> *anonymous_datastream_source$1*[5] -> Sink: errors_internal_sink[6] (7/8) (d220afe491fa016e76b29529ab091deb_5042f7303f448a048cdf298e40198247_6_0) switched from SCHEDULED to DEPLOYING.
11:28:34.309 [flink-akka.actor.default-dispatcher-7] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph - Deploying Flat Map -> *anonymous_datastream_source$1*[5] -> Sink: errors_internal_sink[6] (7/8) (attempt #0) with attempt id d220afe491fa016e76b29529ab091deb_5042f7303f448a048cdf298e40198247_6_0 and vertex id 5042f7303f448a048cdf298e40198247_6 to d07bd3b6-9e42-41a8-9a36-13b767338620 @ localhost (dataPort=-1) with allocation id 83639796df6d89c97d603a928bd3a4de
11:28:34.309 [flink-akka.actor.default-dispatcher-7] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph - Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (8/8) (d220afe491fa016e76b29529ab091deb_cbc357ccb763df2852fee8c4fc7d55f2_7_0) switched from SCHEDULED to DEPLOYING.
11:28:34.309 [flink-akka.actor.default-dispatcher-7] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph - Deploying Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (8/8) (attempt #0) with attempt id d220afe491fa016e76b29529ab091deb_cbc357ccb763df2852fee8c4fc7d55f2_7_0 and vertex id cbc357ccb763df2852fee8c4fc7d55f2_7 to d07bd3b6-9e42-41a8-9a36-13b767338620 @ localhost (dataPort=-1) with allocation id dc440c1e87511e0479246764199e2c2f
11:28:34.309 [flink-akka.actor.default-dispatcher-7] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph - Flat Map -> *anonymous_datastream_source$1*[5] -> Sink: errors_internal_sink[6] (8/8) (d220afe491fa016e76b29529ab091deb_5042f7303f448a048cdf298e40198247_7_0) switched from SCHEDULED to DEPLOYING.
11:28:34.309 [flink-akka.actor.default-dispatcher-7] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph - Deploying Flat Map -> *anonymous_datastream_source$1*[5] -> Sink: errors_internal_sink[6] (8/8) (attempt #0) with attempt id d220afe491fa016e76b29529ab091deb_5042f7303f448a048cdf298e40198247_7_0 and vertex id 5042f7303f448a048cdf298e40198247_7 to d07bd3b6-9e42-41a8-9a36-13b767338620 @ localhost (dataPort=-1) with allocation id dc440c1e87511e0479246764199e2c2f
11:28:34.310 [flink-akka.actor.default-dispatcher-8] INFO  org.apache.flink.runtime.state.changelog.StateChangelogStorageLoader - StateChangelogStorageLoader initialized with shortcut names {memory}.
11:28:34.310 [flink-akka.actor.default-dispatcher-8] INFO  org.apache.flink.runtime.state.changelog.StateChangelogStorageLoader - Creating a changelog storage with name 'memory'.
11:28:34.327 [flink-akka.actor.default-dispatcher-8] INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor - Received task Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (1/8)#0 (d220afe491fa016e76b29529ab091deb_cbc357ccb763df2852fee8c4fc7d55f2_0_0), deploy into slot with allocation id b5d8aae146528392beb88bf17f3a564c.
11:28:34.327 [Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (1/8)#0] INFO  org.apache.flink.runtime.taskmanager.Task - Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (1/8)#0 (d220afe491fa016e76b29529ab091deb_cbc357ccb763df2852fee8c4fc7d55f2_0_0) switched from CREATED to DEPLOYING.
11:28:34.329 [flink-akka.actor.default-dispatcher-8] INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl - Activate slot b5d8aae146528392beb88bf17f3a564c.
11:28:34.334 [Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (1/8)#0] INFO  org.apache.flink.runtime.taskmanager.Task - Loading JAR files for task Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (1/8)#0 (d220afe491fa016e76b29529ab091deb_cbc357ccb763df2852fee8c4fc7d55f2_0_0) [DEPLOYING].
11:28:34.339 [flink-akka.actor.default-dispatcher-8] INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor - Received task Flat Map -> *anonymous_datastream_source$1*[5] -> Sink: errors_internal_sink[6] (1/8)#0 (d220afe491fa016e76b29529ab091deb_5042f7303f448a048cdf298e40198247_0_0), deploy into slot with allocation id b5d8aae146528392beb88bf17f3a564c.
11:28:34.339 [flink-akka.actor.default-dispatcher-8] INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl - Activate slot 3f8389866173750901ea45cc1a40df7e.
11:28:34.339 [Flat Map -> *anonymous_datastream_source$1*[5] -> Sink: errors_internal_sink[6] (1/8)#0] INFO  org.apache.flink.runtime.taskmanager.Task - Flat Map -> *anonymous_datastream_source$1*[5] -> Sink: errors_internal_sink[6] (1/8)#0 (d220afe491fa016e76b29529ab091deb_5042f7303f448a048cdf298e40198247_0_0) switched from CREATED to DEPLOYING.
11:28:34.340 [Flat Map -> *anonymous_datastream_source$1*[5] -> Sink: errors_internal_sink[6] (1/8)#0] INFO  org.apache.flink.runtime.taskmanager.Task - Loading JAR files for task Flat Map -> *anonymous_datastream_source$1*[5] -> Sink: errors_internal_sink[6] (1/8)#0 (d220afe491fa016e76b29529ab091deb_5042f7303f448a048cdf298e40198247_0_0) [DEPLOYING].
11:28:34.340 [flink-akka.actor.default-dispatcher-8] INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor - Received task Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (2/8)#0 (d220afe491fa016e76b29529ab091deb_cbc357ccb763df2852fee8c4fc7d55f2_1_0), deploy into slot with allocation id 3f8389866173750901ea45cc1a40df7e.
11:28:34.341 [Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (2/8)#0] INFO  org.apache.flink.runtime.taskmanager.Task - Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (2/8)#0 (d220afe491fa016e76b29529ab091deb_cbc357ccb763df2852fee8c4fc7d55f2_1_0) switched from CREATED to DEPLOYING.
11:28:34.341 [flink-akka.actor.default-dispatcher-8] INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl - Activate slot 3f8389866173750901ea45cc1a40df7e.
11:28:34.341 [Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (2/8)#0] INFO  org.apache.flink.runtime.taskmanager.Task - Loading JAR files for task Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (2/8)#0 (d220afe491fa016e76b29529ab091deb_cbc357ccb763df2852fee8c4fc7d55f2_1_0) [DEPLOYING].
11:28:34.346 [flink-akka.actor.default-dispatcher-8] INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor - Received task Flat Map -> *anonymous_datastream_source$1*[5] -> Sink: errors_internal_sink[6] (2/8)#0 (d220afe491fa016e76b29529ab091deb_5042f7303f448a048cdf298e40198247_1_0), deploy into slot with allocation id 3f8389866173750901ea45cc1a40df7e.
11:28:34.346 [flink-akka.actor.default-dispatcher-8] INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl - Activate slot 40a113d8b1aa3eb00443a7445bad81be.
11:28:34.346 [Flat Map -> *anonymous_datastream_source$1*[5] -> Sink: errors_internal_sink[6] (2/8)#0] INFO  org.apache.flink.runtime.taskmanager.Task - Flat Map -> *anonymous_datastream_source$1*[5] -> Sink: errors_internal_sink[6] (2/8)#0 (d220afe491fa016e76b29529ab091deb_5042f7303f448a048cdf298e40198247_1_0) switched from CREATED to DEPLOYING.
11:28:34.346 [Flat Map -> *anonymous_datastream_source$1*[5] -> Sink: errors_internal_sink[6] (2/8)#0] INFO  org.apache.flink.runtime.taskmanager.Task - Loading JAR files for task Flat Map -> *anonymous_datastream_source$1*[5] -> Sink: errors_internal_sink[6] (2/8)#0 (d220afe491fa016e76b29529ab091deb_5042f7303f448a048cdf298e40198247_1_0) [DEPLOYING].
11:28:34.347 [flink-akka.actor.default-dispatcher-8] INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor - Received task Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (3/8)#0 (d220afe491fa016e76b29529ab091deb_cbc357ccb763df2852fee8c4fc7d55f2_2_0), deploy into slot with allocation id 40a113d8b1aa3eb00443a7445bad81be.
11:28:34.347 [Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (3/8)#0] INFO  org.apache.flink.runtime.taskmanager.Task - Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (3/8)#0 (d220afe491fa016e76b29529ab091deb_cbc357ccb763df2852fee8c4fc7d55f2_2_0) switched from CREATED to DEPLOYING.
11:28:34.347 [Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (3/8)#0] INFO  org.apache.flink.runtime.taskmanager.Task - Loading JAR files for task Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (3/8)#0 (d220afe491fa016e76b29529ab091deb_cbc357ccb763df2852fee8c4fc7d55f2_2_0) [DEPLOYING].
11:28:34.348 [flink-akka.actor.default-dispatcher-8] INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl - Activate slot 40a113d8b1aa3eb00443a7445bad81be.
11:28:34.351 [flink-akka.actor.default-dispatcher-8] INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor - Received task Flat Map -> *anonymous_datastream_source$1*[5] -> Sink: errors_internal_sink[6] (3/8)#0 (d220afe491fa016e76b29529ab091deb_5042f7303f448a048cdf298e40198247_2_0), deploy into slot with allocation id 40a113d8b1aa3eb00443a7445bad81be.
11:28:34.351 [flink-akka.actor.default-dispatcher-8] INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl - Activate slot ce6710659f99419d2e3ffdd423c38317.
11:28:34.351 [Flat Map -> *anonymous_datastream_source$1*[5] -> Sink: errors_internal_sink[6] (3/8)#0] INFO  org.apache.flink.runtime.taskmanager.Task - Flat Map -> *anonymous_datastream_source$1*[5] -> Sink: errors_internal_sink[6] (3/8)#0 (d220afe491fa016e76b29529ab091deb_5042f7303f448a048cdf298e40198247_2_0) switched from CREATED to DEPLOYING.
11:28:34.352 [Flat Map -> *anonymous_datastream_source$1*[5] -> Sink: errors_internal_sink[6] (3/8)#0] INFO  org.apache.flink.runtime.taskmanager.Task - Loading JAR files for task Flat Map -> *anonymous_datastream_source$1*[5] -> Sink: errors_internal_sink[6] (3/8)#0 (d220afe491fa016e76b29529ab091deb_5042f7303f448a048cdf298e40198247_2_0) [DEPLOYING].
11:28:34.352 [flink-akka.actor.default-dispatcher-8] INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor - Received task Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (4/8)#0 (d220afe491fa016e76b29529ab091deb_cbc357ccb763df2852fee8c4fc7d55f2_3_0), deploy into slot with allocation id ce6710659f99419d2e3ffdd423c38317.
11:28:34.353 [Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (4/8)#0] INFO  org.apache.flink.runtime.taskmanager.Task - Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (4/8)#0 (d220afe491fa016e76b29529ab091deb_cbc357ccb763df2852fee8c4fc7d55f2_3_0) switched from CREATED to DEPLOYING.
11:28:34.353 [Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (4/8)#0] INFO  org.apache.flink.runtime.taskmanager.Task - Loading JAR files for task Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (4/8)#0 (d220afe491fa016e76b29529ab091deb_cbc357ccb763df2852fee8c4fc7d55f2_3_0) [DEPLOYING].
11:28:34.353 [flink-akka.actor.default-dispatcher-8] INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl - Activate slot ce6710659f99419d2e3ffdd423c38317.
11:28:34.354 [flink-akka.actor.default-dispatcher-8] INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor - Received task Flat Map -> *anonymous_datastream_source$1*[5] -> Sink: errors_internal_sink[6] (4/8)#0 (d220afe491fa016e76b29529ab091deb_5042f7303f448a048cdf298e40198247_3_0), deploy into slot with allocation id ce6710659f99419d2e3ffdd423c38317.
11:28:34.355 [Flat Map -> *anonymous_datastream_source$1*[5] -> Sink: errors_internal_sink[6] (4/8)#0] INFO  org.apache.flink.runtime.taskmanager.Task - Flat Map -> *anonymous_datastream_source$1*[5] -> Sink: errors_internal_sink[6] (4/8)#0 (d220afe491fa016e76b29529ab091deb_5042f7303f448a048cdf298e40198247_3_0) switched from CREATED to DEPLOYING.
11:28:34.355 [Flat Map -> *anonymous_datastream_source$1*[5] -> Sink: errors_internal_sink[6] (4/8)#0] INFO  org.apache.flink.runtime.taskmanager.Task - Loading JAR files for task Flat Map -> *anonymous_datastream_source$1*[5] -> Sink: errors_internal_sink[6] (4/8)#0 (d220afe491fa016e76b29529ab091deb_5042f7303f448a048cdf298e40198247_3_0) [DEPLOYING].
11:28:34.355 [flink-akka.actor.default-dispatcher-8] INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl - Activate slot 1f3278fa2fd8460416138b341c977e29.
11:28:34.356 [flink-akka.actor.default-dispatcher-8] INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor - Received task Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (5/8)#0 (d220afe491fa016e76b29529ab091deb_cbc357ccb763df2852fee8c4fc7d55f2_4_0), deploy into slot with allocation id 1f3278fa2fd8460416138b341c977e29.
11:28:34.356 [flink-akka.actor.default-dispatcher-8] INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl - Activate slot 1f3278fa2fd8460416138b341c977e29.
11:28:34.357 [Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (5/8)#0] INFO  org.apache.flink.runtime.taskmanager.Task - Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (5/8)#0 (d220afe491fa016e76b29529ab091deb_cbc357ccb763df2852fee8c4fc7d55f2_4_0) switched from CREATED to DEPLOYING.
11:28:34.357 [Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (5/8)#0] INFO  org.apache.flink.runtime.taskmanager.Task - Loading JAR files for task Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (5/8)#0 (d220afe491fa016e76b29529ab091deb_cbc357ccb763df2852fee8c4fc7d55f2_4_0) [DEPLOYING].
11:28:34.358 [flink-akka.actor.default-dispatcher-8] INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor - Received task Flat Map -> *anonymous_datastream_source$1*[5] -> Sink: errors_internal_sink[6] (5/8)#0 (d220afe491fa016e76b29529ab091deb_5042f7303f448a048cdf298e40198247_4_0), deploy into slot with allocation id 1f3278fa2fd8460416138b341c977e29.
11:28:34.359 [Flat Map -> *anonymous_datastream_source$1*[5] -> Sink: errors_internal_sink[6] (5/8)#0] INFO  org.apache.flink.runtime.taskmanager.Task - Flat Map -> *anonymous_datastream_source$1*[5] -> Sink: errors_internal_sink[6] (5/8)#0 (d220afe491fa016e76b29529ab091deb_5042f7303f448a048cdf298e40198247_4_0) switched from CREATED to DEPLOYING.
11:28:34.359 [flink-akka.actor.default-dispatcher-8] INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl - Activate slot 17b28bd5da27f3580d7f3117b1f4ac5a.
11:28:34.359 [Flat Map -> *anonymous_datastream_source$1*[5] -> Sink: errors_internal_sink[6] (5/8)#0] INFO  org.apache.flink.runtime.taskmanager.Task - Loading JAR files for task Flat Map -> *anonymous_datastream_source$1*[5] -> Sink: errors_internal_sink[6] (5/8)#0 (d220afe491fa016e76b29529ab091deb_5042f7303f448a048cdf298e40198247_4_0) [DEPLOYING].
11:28:34.360 [Flat Map -> *anonymous_datastream_source$1*[5] -> Sink: errors_internal_sink[6] (2/8)#0] INFO  org.apache.flink.streaming.runtime.tasks.StreamTask - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@43901367
11:28:34.360 [Flat Map -> *anonymous_datastream_source$1*[5] -> Sink: errors_internal_sink[6] (2/8)#0] INFO  org.apache.flink.runtime.state.StateBackendLoader - State backend loader loads the state backend as HashMapStateBackend
11:28:34.360 [Flat Map -> *anonymous_datastream_source$1*[5] -> Sink: errors_internal_sink[6] (3/8)#0] INFO  org.apache.flink.streaming.runtime.tasks.StreamTask - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@698a5546
11:28:34.360 [Flat Map -> *anonymous_datastream_source$1*[5] -> Sink: errors_internal_sink[6] (2/8)#0] INFO  org.apache.flink.streaming.runtime.tasks.StreamTask - Checkpoint storage is set to 'jobmanager'
11:28:34.360 [Flat Map -> *anonymous_datastream_source$1*[5] -> Sink: errors_internal_sink[6] (4/8)#0] INFO  org.apache.flink.streaming.runtime.tasks.StreamTask - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@69d84df6
11:28:34.361 [Flat Map -> *anonymous_datastream_source$1*[5] -> Sink: errors_internal_sink[6] (3/8)#0] INFO  org.apache.flink.runtime.state.StateBackendLoader - State backend loader loads the state backend as HashMapStateBackend
11:28:34.361 [Flat Map -> *anonymous_datastream_source$1*[5] -> Sink: errors_internal_sink[6] (1/8)#0] INFO  org.apache.flink.streaming.runtime.tasks.StreamTask - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@26cc447d
11:28:34.361 [Flat Map -> *anonymous_datastream_source$1*[5] -> Sink: errors_internal_sink[6] (1/8)#0] INFO  org.apache.flink.runtime.state.StateBackendLoader - State backend loader loads the state backend as HashMapStateBackend
11:28:34.361 [Flat Map -> *anonymous_datastream_source$1*[5] -> Sink: errors_internal_sink[6] (1/8)#0] INFO  org.apache.flink.streaming.runtime.tasks.StreamTask - Checkpoint storage is set to 'jobmanager'
11:28:34.361 [Flat Map -> *anonymous_datastream_source$1*[5] -> Sink: errors_internal_sink[6] (4/8)#0] INFO  org.apache.flink.runtime.state.StateBackendLoader - State backend loader loads the state backend as HashMapStateBackend
11:28:34.361 [Flat Map -> *anonymous_datastream_source$1*[5] -> Sink: errors_internal_sink[6] (5/8)#0] INFO  org.apache.flink.streaming.runtime.tasks.StreamTask - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@591d1b1
11:28:34.361 [Flat Map -> *anonymous_datastream_source$1*[5] -> Sink: errors_internal_sink[6] (5/8)#0] INFO  org.apache.flink.runtime.state.StateBackendLoader - State backend loader loads the state backend as HashMapStateBackend
11:28:34.361 [Flat Map -> *anonymous_datastream_source$1*[5] -> Sink: errors_internal_sink[6] (5/8)#0] INFO  org.apache.flink.streaming.runtime.tasks.StreamTask - Checkpoint storage is set to 'jobmanager'
11:28:34.361 [Flat Map -> *anonymous_datastream_source$1*[5] -> Sink: errors_internal_sink[6] (3/8)#0] INFO  org.apache.flink.streaming.runtime.tasks.StreamTask - Checkpoint storage is set to 'jobmanager'
11:28:34.361 [Flat Map -> *anonymous_datastream_source$1*[5] -> Sink: errors_internal_sink[6] (4/8)#0] INFO  org.apache.flink.streaming.runtime.tasks.StreamTask - Checkpoint storage is set to 'jobmanager'
11:28:34.362 [flink-akka.actor.default-dispatcher-8] INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor - Received task Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (6/8)#0 (d220afe491fa016e76b29529ab091deb_cbc357ccb763df2852fee8c4fc7d55f2_5_0), deploy into slot with allocation id 17b28bd5da27f3580d7f3117b1f4ac5a.
11:28:34.362 [Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (6/8)#0] INFO  org.apache.flink.runtime.taskmanager.Task - Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (6/8)#0 (d220afe491fa016e76b29529ab091deb_cbc357ccb763df2852fee8c4fc7d55f2_5_0) switched from CREATED to DEPLOYING.
11:28:34.362 [Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (6/8)#0] INFO  org.apache.flink.runtime.taskmanager.Task - Loading JAR files for task Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (6/8)#0 (d220afe491fa016e76b29529ab091deb_cbc357ccb763df2852fee8c4fc7d55f2_5_0) [DEPLOYING].
11:28:34.362 [flink-akka.actor.default-dispatcher-8] INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl - Activate slot 17b28bd5da27f3580d7f3117b1f4ac5a.
11:28:34.365 [Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (4/8)#0] INFO  org.apache.flink.streaming.runtime.tasks.StreamTask - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@1fd74d23
11:28:34.365 [Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (4/8)#0] INFO  org.apache.flink.runtime.state.StateBackendLoader - State backend loader loads the state backend as HashMapStateBackend
11:28:34.365 [Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (4/8)#0] INFO  org.apache.flink.streaming.runtime.tasks.StreamTask - Checkpoint storage is set to 'jobmanager'
11:28:34.365 [Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (3/8)#0] INFO  org.apache.flink.streaming.runtime.tasks.StreamTask - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@1fd9a0c5
11:28:34.365 [Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (3/8)#0] INFO  org.apache.flink.runtime.state.StateBackendLoader - State backend loader loads the state backend as HashMapStateBackend
11:28:34.366 [Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (3/8)#0] INFO  org.apache.flink.streaming.runtime.tasks.StreamTask - Checkpoint storage is set to 'jobmanager'
11:28:34.366 [Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (1/8)#0] INFO  org.apache.flink.streaming.runtime.tasks.StreamTask - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@6573dad
11:28:34.366 [Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (1/8)#0] INFO  org.apache.flink.runtime.state.StateBackendLoader - State backend loader loads the state backend as HashMapStateBackend
11:28:34.366 [Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (2/8)#0] INFO  org.apache.flink.streaming.runtime.tasks.StreamTask - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@11431503
11:28:34.366 [Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (1/8)#0] INFO  org.apache.flink.streaming.runtime.tasks.StreamTask - Checkpoint storage is set to 'jobmanager'
11:28:34.366 [Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (5/8)#0] INFO  org.apache.flink.streaming.runtime.tasks.StreamTask - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@4314cf9
11:28:34.366 [Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (2/8)#0] INFO  org.apache.flink.runtime.state.StateBackendLoader - State backend loader loads the state backend as HashMapStateBackend
11:28:34.366 [Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (2/8)#0] INFO  org.apache.flink.streaming.runtime.tasks.StreamTask - Checkpoint storage is set to 'jobmanager'
11:28:34.366 [Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (5/8)#0] INFO  org.apache.flink.runtime.state.StateBackendLoader - State backend loader loads the state backend as HashMapStateBackend
11:28:34.366 [Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (5/8)#0] INFO  org.apache.flink.streaming.runtime.tasks.StreamTask - Checkpoint storage is set to 'jobmanager'
11:28:34.366 [Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (6/8)#0] INFO  org.apache.flink.streaming.runtime.tasks.StreamTask - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@a4a1987
11:28:34.366 [Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (6/8)#0] INFO  org.apache.flink.runtime.state.StateBackendLoader - State backend loader loads the state backend as HashMapStateBackend
11:28:34.366 [Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (6/8)#0] INFO  org.apache.flink.streaming.runtime.tasks.StreamTask - Checkpoint storage is set to 'jobmanager'
11:28:34.367 [flink-akka.actor.default-dispatcher-8] INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor - Received task Flat Map -> *anonymous_datastream_source$1*[5] -> Sink: errors_internal_sink[6] (6/8)#0 (d220afe491fa016e76b29529ab091deb_5042f7303f448a048cdf298e40198247_5_0), deploy into slot with allocation id 17b28bd5da27f3580d7f3117b1f4ac5a.
11:28:34.367 [flink-akka.actor.default-dispatcher-8] INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl - Activate slot 83639796df6d89c97d603a928bd3a4de.
11:28:34.367 [Flat Map -> *anonymous_datastream_source$1*[5] -> Sink: errors_internal_sink[6] (6/8)#0] INFO  org.apache.flink.runtime.taskmanager.Task - Flat Map -> *anonymous_datastream_source$1*[5] -> Sink: errors_internal_sink[6] (6/8)#0 (d220afe491fa016e76b29529ab091deb_5042f7303f448a048cdf298e40198247_5_0) switched from CREATED to DEPLOYING.
11:28:34.368 [Flat Map -> *anonymous_datastream_source$1*[5] -> Sink: errors_internal_sink[6] (6/8)#0] INFO  org.apache.flink.runtime.taskmanager.Task - Loading JAR files for task Flat Map -> *anonymous_datastream_source$1*[5] -> Sink: errors_internal_sink[6] (6/8)#0 (d220afe491fa016e76b29529ab091deb_5042f7303f448a048cdf298e40198247_5_0) [DEPLOYING].
11:28:34.369 [Flat Map -> *anonymous_datastream_source$1*[5] -> Sink: errors_internal_sink[6] (6/8)#0] INFO  org.apache.flink.streaming.runtime.tasks.StreamTask - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@11b703db
11:28:34.369 [Flat Map -> *anonymous_datastream_source$1*[5] -> Sink: errors_internal_sink[6] (6/8)#0] INFO  org.apache.flink.runtime.state.StateBackendLoader - State backend loader loads the state backend as HashMapStateBackend
11:28:34.369 [Flat Map -> *anonymous_datastream_source$1*[5] -> Sink: errors_internal_sink[6] (6/8)#0] INFO  org.apache.flink.streaming.runtime.tasks.StreamTask - Checkpoint storage is set to 'jobmanager'
11:28:34.369 [Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (1/8)#0] INFO  org.apache.flink.runtime.taskmanager.Task - Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (1/8)#0 (d220afe491fa016e76b29529ab091deb_cbc357ccb763df2852fee8c4fc7d55f2_0_0) switched from DEPLOYING to INITIALIZING.
11:28:34.369 [Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (6/8)#0] INFO  org.apache.flink.runtime.taskmanager.Task - Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (6/8)#0 (d220afe491fa016e76b29529ab091deb_cbc357ccb763df2852fee8c4fc7d55f2_5_0) switched from DEPLOYING to INITIALIZING.
11:28:34.369 [Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (4/8)#0] INFO  org.apache.flink.runtime.taskmanager.Task - Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (4/8)#0 (d220afe491fa016e76b29529ab091deb_cbc357ccb763df2852fee8c4fc7d55f2_3_0) switched from DEPLOYING to INITIALIZING.
11:28:34.370 [Flat Map -> *anonymous_datastream_source$1*[5] -> Sink: errors_internal_sink[6] (6/8)#0] INFO  org.apache.flink.runtime.taskmanager.Task - Flat Map -> *anonymous_datastream_source$1*[5] -> Sink: errors_internal_sink[6] (6/8)#0 (d220afe491fa016e76b29529ab091deb_5042f7303f448a048cdf298e40198247_5_0) switched from DEPLOYING to INITIALIZING.
11:28:34.370 [Flat Map -> *anonymous_datastream_source$1*[5] -> Sink: errors_internal_sink[6] (3/8)#0] INFO  org.apache.flink.runtime.taskmanager.Task - Flat Map -> *anonymous_datastream_source$1*[5] -> Sink: errors_internal_sink[6] (3/8)#0 (d220afe491fa016e76b29529ab091deb_5042f7303f448a048cdf298e40198247_2_0) switched from DEPLOYING to INITIALIZING.
11:28:34.370 [Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (2/8)#0] INFO  org.apache.flink.runtime.taskmanager.Task - Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (2/8)#0 (d220afe491fa016e76b29529ab091deb_cbc357ccb763df2852fee8c4fc7d55f2_1_0) switched from DEPLOYING to INITIALIZING.
11:28:34.370 [Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (3/8)#0] INFO  org.apache.flink.runtime.taskmanager.Task - Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (3/8)#0 (d220afe491fa016e76b29529ab091deb_cbc357ccb763df2852fee8c4fc7d55f2_2_0) switched from DEPLOYING to INITIALIZING.
11:28:34.370 [Flat Map -> *anonymous_datastream_source$1*[5] -> Sink: errors_internal_sink[6] (5/8)#0] INFO  org.apache.flink.runtime.taskmanager.Task - Flat Map -> *anonymous_datastream_source$1*[5] -> Sink: errors_internal_sink[6] (5/8)#0 (d220afe491fa016e76b29529ab091deb_5042f7303f448a048cdf298e40198247_4_0) switched from DEPLOYING to INITIALIZING.
11:28:34.370 [Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (5/8)#0] INFO  org.apache.flink.runtime.taskmanager.Task - Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (5/8)#0 (d220afe491fa016e76b29529ab091deb_cbc357ccb763df2852fee8c4fc7d55f2_4_0) switched from DEPLOYING to INITIALIZING.
11:28:34.370 [Flat Map -> *anonymous_datastream_source$1*[5] -> Sink: errors_internal_sink[6] (2/8)#0] INFO  org.apache.flink.runtime.taskmanager.Task - Flat Map -> *anonymous_datastream_source$1*[5] -> Sink: errors_internal_sink[6] (2/8)#0 (d220afe491fa016e76b29529ab091deb_5042f7303f448a048cdf298e40198247_1_0) switched from DEPLOYING to INITIALIZING.
11:28:34.371 [Flat Map -> *anonymous_datastream_source$1*[5] -> Sink: errors_internal_sink[6] (4/8)#0] INFO  org.apache.flink.runtime.taskmanager.Task - Flat Map -> *anonymous_datastream_source$1*[5] -> Sink: errors_internal_sink[6] (4/8)#0 (d220afe491fa016e76b29529ab091deb_5042f7303f448a048cdf298e40198247_3_0) switched from DEPLOYING to INITIALIZING.
11:28:34.371 [Flat Map -> *anonymous_datastream_source$1*[5] -> Sink: errors_internal_sink[6] (1/8)#0] INFO  org.apache.flink.runtime.taskmanager.Task - Flat Map -> *anonymous_datastream_source$1*[5] -> Sink: errors_internal_sink[6] (1/8)#0 (d220afe491fa016e76b29529ab091deb_5042f7303f448a048cdf298e40198247_0_0) switched from DEPLOYING to INITIALIZING.
11:28:34.371 [flink-akka.actor.default-dispatcher-8] INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor - Received task Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (7/8)#0 (d220afe491fa016e76b29529ab091deb_cbc357ccb763df2852fee8c4fc7d55f2_6_0), deploy into slot with allocation id 83639796df6d89c97d603a928bd3a4de.
11:28:34.371 [flink-akka.actor.default-dispatcher-7] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph - Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (1/8) (d220afe491fa016e76b29529ab091deb_cbc357ccb763df2852fee8c4fc7d55f2_0_0) switched from DEPLOYING to INITIALIZING.
11:28:34.372 [flink-akka.actor.default-dispatcher-8] INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl - Activate slot 83639796df6d89c97d603a928bd3a4de.
11:28:34.372 [Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (7/8)#0] INFO  org.apache.flink.runtime.taskmanager.Task - Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (7/8)#0 (d220afe491fa016e76b29529ab091deb_cbc357ccb763df2852fee8c4fc7d55f2_6_0) switched from CREATED to DEPLOYING.
11:28:34.372 [Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (7/8)#0] INFO  org.apache.flink.runtime.taskmanager.Task - Loading JAR files for task Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (7/8)#0 (d220afe491fa016e76b29529ab091deb_cbc357ccb763df2852fee8c4fc7d55f2_6_0) [DEPLOYING].
11:28:34.374 [flink-akka.actor.default-dispatcher-8] INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor - Received task Flat Map -> *anonymous_datastream_source$1*[5] -> Sink: errors_internal_sink[6] (7/8)#0 (d220afe491fa016e76b29529ab091deb_5042f7303f448a048cdf298e40198247_6_0), deploy into slot with allocation id 83639796df6d89c97d603a928bd3a4de.
11:28:34.374 [flink-akka.actor.default-dispatcher-7] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph - Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (6/8) (d220afe491fa016e76b29529ab091deb_cbc357ccb763df2852fee8c4fc7d55f2_5_0) switched from DEPLOYING to INITIALIZING.
11:28:34.374 [flink-akka.actor.default-dispatcher-7] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph - Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (4/8) (d220afe491fa016e76b29529ab091deb_cbc357ccb763df2852fee8c4fc7d55f2_3_0) switched from DEPLOYING to INITIALIZING.
11:28:34.374 [flink-akka.actor.default-dispatcher-8] INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl - Activate slot dc440c1e87511e0479246764199e2c2f.
11:28:34.374 [flink-akka.actor.default-dispatcher-7] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph - Flat Map -> *anonymous_datastream_source$1*[5] -> Sink: errors_internal_sink[6] (6/8) (d220afe491fa016e76b29529ab091deb_5042f7303f448a048cdf298e40198247_5_0) switched from DEPLOYING to INITIALIZING.
11:28:34.374 [flink-akka.actor.default-dispatcher-7] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph - Flat Map -> *anonymous_datastream_source$1*[5] -> Sink: errors_internal_sink[6] (3/8) (d220afe491fa016e76b29529ab091deb_5042f7303f448a048cdf298e40198247_2_0) switched from DEPLOYING to INITIALIZING.
11:28:34.374 [flink-akka.actor.default-dispatcher-7] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph - Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (2/8) (d220afe491fa016e76b29529ab091deb_cbc357ccb763df2852fee8c4fc7d55f2_1_0) switched from DEPLOYING to INITIALIZING.
11:28:34.375 [flink-akka.actor.default-dispatcher-7] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph - Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (3/8) (d220afe491fa016e76b29529ab091deb_cbc357ccb763df2852fee8c4fc7d55f2_2_0) switched from DEPLOYING to INITIALIZING.
11:28:34.375 [flink-akka.actor.default-dispatcher-7] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph - Flat Map -> *anonymous_datastream_source$1*[5] -> Sink: errors_internal_sink[6] (5/8) (d220afe491fa016e76b29529ab091deb_5042f7303f448a048cdf298e40198247_4_0) switched from DEPLOYING to INITIALIZING.
11:28:34.375 [flink-akka.actor.default-dispatcher-7] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph - Flat Map -> *anonymous_datastream_source$1*[5] -> Sink: errors_internal_sink[6] (2/8) (d220afe491fa016e76b29529ab091deb_5042f7303f448a048cdf298e40198247_1_0) switched from DEPLOYING to INITIALIZING.
11:28:34.375 [flink-akka.actor.default-dispatcher-7] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph - Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (5/8) (d220afe491fa016e76b29529ab091deb_cbc357ccb763df2852fee8c4fc7d55f2_4_0) switched from DEPLOYING to INITIALIZING.
11:28:34.375 [Flat Map -> *anonymous_datastream_source$1*[5] -> Sink: errors_internal_sink[6] (7/8)#0] INFO  org.apache.flink.runtime.taskmanager.Task - Flat Map -> *anonymous_datastream_source$1*[5] -> Sink: errors_internal_sink[6] (7/8)#0 (d220afe491fa016e76b29529ab091deb_5042f7303f448a048cdf298e40198247_6_0) switched from CREATED to DEPLOYING.
11:28:34.375 [flink-akka.actor.default-dispatcher-7] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph - Flat Map -> *anonymous_datastream_source$1*[5] -> Sink: errors_internal_sink[6] (4/8) (d220afe491fa016e76b29529ab091deb_5042f7303f448a048cdf298e40198247_3_0) switched from DEPLOYING to INITIALIZING.
11:28:34.375 [flink-akka.actor.default-dispatcher-7] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph - Flat Map -> *anonymous_datastream_source$1*[5] -> Sink: errors_internal_sink[6] (1/8) (d220afe491fa016e76b29529ab091deb_5042f7303f448a048cdf298e40198247_0_0) switched from DEPLOYING to INITIALIZING.
11:28:34.375 [Flat Map -> *anonymous_datastream_source$1*[5] -> Sink: errors_internal_sink[6] (7/8)#0] INFO  org.apache.flink.runtime.taskmanager.Task - Loading JAR files for task Flat Map -> *anonymous_datastream_source$1*[5] -> Sink: errors_internal_sink[6] (7/8)#0 (d220afe491fa016e76b29529ab091deb_5042f7303f448a048cdf298e40198247_6_0) [DEPLOYING].
11:28:34.376 [Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (7/8)#0] INFO  org.apache.flink.streaming.runtime.tasks.StreamTask - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@72353a8c
11:28:34.376 [Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (7/8)#0] INFO  org.apache.flink.runtime.state.StateBackendLoader - State backend loader loads the state backend as HashMapStateBackend
11:28:34.376 [Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (7/8)#0] INFO  org.apache.flink.streaming.runtime.tasks.StreamTask - Checkpoint storage is set to 'jobmanager'
11:28:34.376 [Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (7/8)#0] INFO  org.apache.flink.runtime.taskmanager.Task - Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (7/8)#0 (d220afe491fa016e76b29529ab091deb_cbc357ccb763df2852fee8c4fc7d55f2_6_0) switched from DEPLOYING to INITIALIZING.
11:28:34.377 [Flat Map -> *anonymous_datastream_source$1*[5] -> Sink: errors_internal_sink[6] (7/8)#0] INFO  org.apache.flink.streaming.runtime.tasks.StreamTask - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@c8e7658
11:28:34.377 [Flat Map -> *anonymous_datastream_source$1*[5] -> Sink: errors_internal_sink[6] (7/8)#0] INFO  org.apache.flink.runtime.state.StateBackendLoader - State backend loader loads the state backend as HashMapStateBackend
11:28:34.377 [Flat Map -> *anonymous_datastream_source$1*[5] -> Sink: errors_internal_sink[6] (7/8)#0] INFO  org.apache.flink.streaming.runtime.tasks.StreamTask - Checkpoint storage is set to 'jobmanager'
11:28:34.377 [flink-akka.actor.default-dispatcher-8] INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor - Received task Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (8/8)#0 (d220afe491fa016e76b29529ab091deb_cbc357ccb763df2852fee8c4fc7d55f2_7_0), deploy into slot with allocation id dc440c1e87511e0479246764199e2c2f.
11:28:34.377 [Flat Map -> *anonymous_datastream_source$1*[5] -> Sink: errors_internal_sink[6] (7/8)#0] INFO  org.apache.flink.runtime.taskmanager.Task - Flat Map -> *anonymous_datastream_source$1*[5] -> Sink: errors_internal_sink[6] (7/8)#0 (d220afe491fa016e76b29529ab091deb_5042f7303f448a048cdf298e40198247_6_0) switched from DEPLOYING to INITIALIZING.
11:28:34.378 [flink-akka.actor.default-dispatcher-9] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph - Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (7/8) (d220afe491fa016e76b29529ab091deb_cbc357ccb763df2852fee8c4fc7d55f2_6_0) switched from DEPLOYING to INITIALIZING.
11:28:34.379 [flink-akka.actor.default-dispatcher-9] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph - Flat Map -> *anonymous_datastream_source$1*[5] -> Sink: errors_internal_sink[6] (7/8) (d220afe491fa016e76b29529ab091deb_5042f7303f448a048cdf298e40198247_6_0) switched from DEPLOYING to INITIALIZING.
11:28:34.379 [flink-akka.actor.default-dispatcher-9] INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl - Activate slot dc440c1e87511e0479246764199e2c2f.
11:28:34.379 [Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (8/8)#0] INFO  org.apache.flink.runtime.taskmanager.Task - Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (8/8)#0 (d220afe491fa016e76b29529ab091deb_cbc357ccb763df2852fee8c4fc7d55f2_7_0) switched from CREATED to DEPLOYING.
11:28:34.379 [Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (8/8)#0] INFO  org.apache.flink.runtime.taskmanager.Task - Loading JAR files for task Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (8/8)#0 (d220afe491fa016e76b29529ab091deb_cbc357ccb763df2852fee8c4fc7d55f2_7_0) [DEPLOYING].
11:28:34.382 [flink-akka.actor.default-dispatcher-9] INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor - Received task Flat Map -> *anonymous_datastream_source$1*[5] -> Sink: errors_internal_sink[6] (8/8)#0 (d220afe491fa016e76b29529ab091deb_5042f7303f448a048cdf298e40198247_7_0), deploy into slot with allocation id dc440c1e87511e0479246764199e2c2f.
11:28:34.382 [Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (8/8)#0] INFO  org.apache.flink.streaming.runtime.tasks.StreamTask - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@7102325d
11:28:34.383 [Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (8/8)#0] INFO  org.apache.flink.runtime.state.StateBackendLoader - State backend loader loads the state backend as HashMapStateBackend
11:28:34.383 [Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (8/8)#0] INFO  org.apache.flink.streaming.runtime.tasks.StreamTask - Checkpoint storage is set to 'jobmanager'
11:28:34.383 [Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (8/8)#0] INFO  org.apache.flink.runtime.taskmanager.Task - Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (8/8)#0 (d220afe491fa016e76b29529ab091deb_cbc357ccb763df2852fee8c4fc7d55f2_7_0) switched from DEPLOYING to INITIALIZING.
11:28:34.384 [flink-akka.actor.default-dispatcher-7] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph - Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (8/8) (d220afe491fa016e76b29529ab091deb_cbc357ccb763df2852fee8c4fc7d55f2_7_0) switched from DEPLOYING to INITIALIZING.
11:28:34.385 [flink-akka.actor.default-dispatcher-9] INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl - Activate slot b5d8aae146528392beb88bf17f3a564c.
11:28:34.385 [flink-akka.actor.default-dispatcher-9] INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl - Activate slot 3f8389866173750901ea45cc1a40df7e.
11:28:34.385 [flink-akka.actor.default-dispatcher-9] INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl - Activate slot 40a113d8b1aa3eb00443a7445bad81be.
11:28:34.385 [flink-akka.actor.default-dispatcher-9] INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl - Activate slot ce6710659f99419d2e3ffdd423c38317.
11:28:34.385 [flink-akka.actor.default-dispatcher-9] INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl - Activate slot 1f3278fa2fd8460416138b341c977e29.
11:28:34.385 [flink-akka.actor.default-dispatcher-9] INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl - Activate slot 17b28bd5da27f3580d7f3117b1f4ac5a.
11:28:34.385 [flink-akka.actor.default-dispatcher-9] INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl - Activate slot 83639796df6d89c97d603a928bd3a4de.
11:28:34.385 [flink-akka.actor.default-dispatcher-9] INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl - Activate slot dc440c1e87511e0479246764199e2c2f.
11:28:34.385 [Flat Map -> *anonymous_datastream_source$1*[5] -> Sink: errors_internal_sink[6] (8/8)#0] INFO  org.apache.flink.runtime.taskmanager.Task - Flat Map -> *anonymous_datastream_source$1*[5] -> Sink: errors_internal_sink[6] (8/8)#0 (d220afe491fa016e76b29529ab091deb_5042f7303f448a048cdf298e40198247_7_0) switched from CREATED to DEPLOYING.
11:28:34.385 [Flat Map -> *anonymous_datastream_source$1*[5] -> Sink: errors_internal_sink[6] (8/8)#0] INFO  org.apache.flink.runtime.taskmanager.Task - Loading JAR files for task Flat Map -> *anonymous_datastream_source$1*[5] -> Sink: errors_internal_sink[6] (8/8)#0 (d220afe491fa016e76b29529ab091deb_5042f7303f448a048cdf298e40198247_7_0) [DEPLOYING].
11:28:34.390 [Flat Map -> *anonymous_datastream_source$1*[5] -> Sink: errors_internal_sink[6] (8/8)#0] INFO  org.apache.flink.streaming.runtime.tasks.StreamTask - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@5522c4eb
11:28:34.390 [Flat Map -> *anonymous_datastream_source$1*[5] -> Sink: errors_internal_sink[6] (8/8)#0] INFO  org.apache.flink.runtime.state.StateBackendLoader - State backend loader loads the state backend as HashMapStateBackend
11:28:34.390 [Flat Map -> *anonymous_datastream_source$1*[5] -> Sink: errors_internal_sink[6] (8/8)#0] INFO  org.apache.flink.streaming.runtime.tasks.StreamTask - Checkpoint storage is set to 'jobmanager'
11:28:34.390 [Flat Map -> *anonymous_datastream_source$1*[5] -> Sink: errors_internal_sink[6] (8/8)#0] INFO  org.apache.flink.runtime.taskmanager.Task - Flat Map -> *anonymous_datastream_source$1*[5] -> Sink: errors_internal_sink[6] (8/8)#0 (d220afe491fa016e76b29529ab091deb_5042f7303f448a048cdf298e40198247_7_0) switched from DEPLOYING to INITIALIZING.
11:28:34.393 [flink-akka.actor.default-dispatcher-8] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph - Flat Map -> *anonymous_datastream_source$1*[5] -> Sink: errors_internal_sink[6] (8/8) (d220afe491fa016e76b29529ab091deb_5042f7303f448a048cdf298e40198247_7_0) switched from DEPLOYING to INITIALIZING.
11:28:34.484 [Flat Map -> *anonymous_datastream_source$1*[5] -> Sink: errors_internal_sink[6] (5/8)#0] INFO  org.apache.flink.runtime.taskmanager.Task - Flat Map -> *anonymous_datastream_source$1*[5] -> Sink: errors_internal_sink[6] (5/8)#0 (d220afe491fa016e76b29529ab091deb_5042f7303f448a048cdf298e40198247_4_0) switched from INITIALIZING to RUNNING.
11:28:34.484 [Flat Map -> *anonymous_datastream_source$1*[5] -> Sink: errors_internal_sink[6] (1/8)#0] INFO  org.apache.flink.runtime.taskmanager.Task - Flat Map -> *anonymous_datastream_source$1*[5] -> Sink: errors_internal_sink[6] (1/8)#0 (d220afe491fa016e76b29529ab091deb_5042f7303f448a048cdf298e40198247_0_0) switched from INITIALIZING to RUNNING.
11:28:34.484 [Flat Map -> *anonymous_datastream_source$1*[5] -> Sink: errors_internal_sink[6] (3/8)#0] INFO  org.apache.flink.runtime.taskmanager.Task - Flat Map -> *anonymous_datastream_source$1*[5] -> Sink: errors_internal_sink[6] (3/8)#0 (d220afe491fa016e76b29529ab091deb_5042f7303f448a048cdf298e40198247_2_0) switched from INITIALIZING to RUNNING.
11:28:34.484 [Flat Map -> *anonymous_datastream_source$1*[5] -> Sink: errors_internal_sink[6] (6/8)#0] INFO  org.apache.flink.runtime.taskmanager.Task - Flat Map -> *anonymous_datastream_source$1*[5] -> Sink: errors_internal_sink[6] (6/8)#0 (d220afe491fa016e76b29529ab091deb_5042f7303f448a048cdf298e40198247_5_0) switched from INITIALIZING to RUNNING.
11:28:34.484 [Flat Map -> *anonymous_datastream_source$1*[5] -> Sink: errors_internal_sink[6] (2/8)#0] INFO  org.apache.flink.runtime.taskmanager.Task - Flat Map -> *anonymous_datastream_source$1*[5] -> Sink: errors_internal_sink[6] (2/8)#0 (d220afe491fa016e76b29529ab091deb_5042f7303f448a048cdf298e40198247_1_0) switched from INITIALIZING to RUNNING.
11:28:34.484 [Flat Map -> *anonymous_datastream_source$1*[5] -> Sink: errors_internal_sink[6] (7/8)#0] INFO  org.apache.flink.runtime.taskmanager.Task - Flat Map -> *anonymous_datastream_source$1*[5] -> Sink: errors_internal_sink[6] (7/8)#0 (d220afe491fa016e76b29529ab091deb_5042f7303f448a048cdf298e40198247_6_0) switched from INITIALIZING to RUNNING.
11:28:34.485 [Flat Map -> *anonymous_datastream_source$1*[5] -> Sink: errors_internal_sink[6] (8/8)#0] INFO  org.apache.flink.runtime.taskmanager.Task - Flat Map -> *anonymous_datastream_source$1*[5] -> Sink: errors_internal_sink[6] (8/8)#0 (d220afe491fa016e76b29529ab091deb_5042f7303f448a048cdf298e40198247_7_0) switched from INITIALIZING to RUNNING.
11:28:34.485 [Flat Map -> *anonymous_datastream_source$1*[5] -> Sink: errors_internal_sink[6] (4/8)#0] INFO  org.apache.flink.runtime.taskmanager.Task - Flat Map -> *anonymous_datastream_source$1*[5] -> Sink: errors_internal_sink[6] (4/8)#0 (d220afe491fa016e76b29529ab091deb_5042f7303f448a048cdf298e40198247_3_0) switched from INITIALIZING to RUNNING.
11:28:34.485 [flink-akka.actor.default-dispatcher-8] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph - Flat Map -> *anonymous_datastream_source$1*[5] -> Sink: errors_internal_sink[6] (5/8) (d220afe491fa016e76b29529ab091deb_5042f7303f448a048cdf298e40198247_4_0) switched from INITIALIZING to RUNNING.
11:28:34.486 [flink-akka.actor.default-dispatcher-8] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph - Flat Map -> *anonymous_datastream_source$1*[5] -> Sink: errors_internal_sink[6] (3/8) (d220afe491fa016e76b29529ab091deb_5042f7303f448a048cdf298e40198247_2_0) switched from INITIALIZING to RUNNING.
11:28:34.486 [flink-akka.actor.default-dispatcher-8] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph - Flat Map -> *anonymous_datastream_source$1*[5] -> Sink: errors_internal_sink[6] (1/8) (d220afe491fa016e76b29529ab091deb_5042f7303f448a048cdf298e40198247_0_0) switched from INITIALIZING to RUNNING.
11:28:34.486 [flink-akka.actor.default-dispatcher-8] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph - Flat Map -> *anonymous_datastream_source$1*[5] -> Sink: errors_internal_sink[6] (6/8) (d220afe491fa016e76b29529ab091deb_5042f7303f448a048cdf298e40198247_5_0) switched from INITIALIZING to RUNNING.
11:28:34.486 [flink-akka.actor.default-dispatcher-8] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph - Flat Map -> *anonymous_datastream_source$1*[5] -> Sink: errors_internal_sink[6] (2/8) (d220afe491fa016e76b29529ab091deb_5042f7303f448a048cdf298e40198247_1_0) switched from INITIALIZING to RUNNING.
11:28:34.486 [flink-akka.actor.default-dispatcher-8] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph - Flat Map -> *anonymous_datastream_source$1*[5] -> Sink: errors_internal_sink[6] (7/8) (d220afe491fa016e76b29529ab091deb_5042f7303f448a048cdf298e40198247_6_0) switched from INITIALIZING to RUNNING.
11:28:34.486 [flink-akka.actor.default-dispatcher-8] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph - Flat Map -> *anonymous_datastream_source$1*[5] -> Sink: errors_internal_sink[6] (8/8) (d220afe491fa016e76b29529ab091deb_5042f7303f448a048cdf298e40198247_7_0) switched from INITIALIZING to RUNNING.
11:28:34.486 [flink-akka.actor.default-dispatcher-8] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph - Flat Map -> *anonymous_datastream_source$1*[5] -> Sink: errors_internal_sink[6] (4/8) (d220afe491fa016e76b29529ab091deb_5042f7303f448a048cdf298e40198247_3_0) switched from INITIALIZING to RUNNING.
11:28:34.579 [Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (4/8)#0] INFO  org.apache.kafka.clients.producer.ProducerConfig - ProducerConfig values:
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:51724]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-2
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

11:28:34.579 [Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (7/8)#0] INFO  org.apache.kafka.clients.producer.ProducerConfig - ProducerConfig values:
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:51724]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-4
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

11:28:34.579 [Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (8/8)#0] INFO  org.apache.kafka.clients.producer.ProducerConfig - ProducerConfig values:
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:51724]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-8
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

11:28:34.579 [Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (6/8)#0] INFO  org.apache.kafka.clients.producer.ProducerConfig - ProducerConfig values:
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:51724]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-7
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

11:28:34.579 [Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (3/8)#0] INFO  org.apache.kafka.clients.producer.ProducerConfig - ProducerConfig values:
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:51724]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-3
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

11:28:34.579 [Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (2/8)#0] INFO  org.apache.kafka.clients.producer.ProducerConfig - ProducerConfig values:
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:51724]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-6
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

11:28:34.579 [Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (5/8)#0] INFO  org.apache.kafka.clients.producer.ProducerConfig - ProducerConfig values:
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:51724]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-5
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

11:28:34.580 [Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (1/8)#0] INFO  org.apache.kafka.clients.producer.ProducerConfig - ProducerConfig values:
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:51724]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-9
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

11:28:34.581 [Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (4/8)#0] INFO  org.apache.kafka.clients.producer.KafkaProducer - [Producer clientId=producer-2] Instantiated an idempotent producer.
11:28:34.581 [Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (5/8)#0] INFO  org.apache.kafka.clients.producer.KafkaProducer - [Producer clientId=producer-5] Instantiated an idempotent producer.
11:28:34.582 [Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (6/8)#0] INFO  org.apache.kafka.clients.producer.KafkaProducer - [Producer clientId=producer-7] Instantiated an idempotent producer.
11:28:34.583 [Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (3/8)#0] INFO  org.apache.kafka.clients.producer.KafkaProducer - [Producer clientId=producer-3] Instantiated an idempotent producer.
11:28:34.584 [Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (2/8)#0] INFO  org.apache.kafka.clients.producer.KafkaProducer - [Producer clientId=producer-6] Instantiated an idempotent producer.
11:28:34.585 [Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (1/8)#0] INFO  org.apache.kafka.clients.producer.KafkaProducer - [Producer clientId=producer-9] Instantiated an idempotent producer.
11:28:34.586 [Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (7/8)#0] INFO  org.apache.kafka.clients.producer.KafkaProducer - [Producer clientId=producer-4] Instantiated an idempotent producer.
11:28:34.587 [Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (8/8)#0] INFO  org.apache.kafka.clients.producer.KafkaProducer - [Producer clientId=producer-8] Instantiated an idempotent producer.
11:28:34.592 [Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (5/8)#0] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 3.4.0
11:28:34.592 [Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (5/8)#0] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 2e1947d240607d53
11:28:34.592 [Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (5/8)#0] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1687544914591
11:28:34.598 [Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (8/8)#0] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 3.4.0
11:28:34.598 [Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (8/8)#0] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 2e1947d240607d53
11:28:34.598 [Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (8/8)#0] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1687544914598
11:28:34.598 [kafka-producer-network-thread | producer-5] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-5] Cluster ID: D0jK8KEKRqCvHg2y7XT-hQ
11:28:34.600 [Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (7/8)#0] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 3.4.0
11:28:34.600 [Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (7/8)#0] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 2e1947d240607d53
11:28:34.600 [Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (7/8)#0] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1687544914600
11:28:34.600 [kafka-producer-network-thread | producer-5] INFO  org.apache.kafka.clients.producer.internals.TransactionManager - [Producer clientId=producer-5] ProducerId set to 1 with epoch 0
11:28:34.601 [Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (6/8)#0] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 3.4.0
11:28:34.601 [Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (6/8)#0] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 2e1947d240607d53
11:28:34.601 [Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (6/8)#0] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1687544914600
11:28:34.601 [Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (2/8)#0] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 3.4.0
11:28:34.601 [Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (2/8)#0] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 2e1947d240607d53
11:28:34.601 [Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (2/8)#0] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1687544914601
11:28:34.602 [Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (4/8)#0] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 3.4.0
11:28:34.602 [Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (4/8)#0] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 2e1947d240607d53
11:28:34.602 [Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (4/8)#0] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1687544914602
11:28:34.603 [Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (3/8)#0] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 3.4.0
11:28:34.603 [Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (3/8)#0] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 2e1947d240607d53
11:28:34.603 [Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (3/8)#0] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1687544914602
11:28:34.604 [Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (7/8)#0] INFO  org.apache.flink.runtime.taskmanager.Task - Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (7/8)#0 (d220afe491fa016e76b29529ab091deb_cbc357ccb763df2852fee8c4fc7d55f2_6_0) switched from INITIALIZING to RUNNING.
11:28:34.605 [Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (5/8)#0] INFO  org.apache.flink.runtime.taskmanager.Task - Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (5/8)#0 (d220afe491fa016e76b29529ab091deb_cbc357ccb763df2852fee8c4fc7d55f2_4_0) switched from INITIALIZING to RUNNING.
11:28:34.606 [Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (8/8)#0] INFO  org.apache.flink.runtime.taskmanager.Task - Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (8/8)#0 (d220afe491fa016e76b29529ab091deb_cbc357ccb763df2852fee8c4fc7d55f2_7_0) switched from INITIALIZING to RUNNING.
11:28:34.608 [flink-akka.actor.default-dispatcher-7] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph - Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (7/8) (d220afe491fa016e76b29529ab091deb_cbc357ccb763df2852fee8c4fc7d55f2_6_0) switched from INITIALIZING to RUNNING.
11:28:34.608 [kafka-producer-network-thread | producer-4] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-4] Cluster ID: D0jK8KEKRqCvHg2y7XT-hQ
11:28:34.608 [flink-akka.actor.default-dispatcher-7] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph - Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (5/8) (d220afe491fa016e76b29529ab091deb_cbc357ccb763df2852fee8c4fc7d55f2_4_0) switched from INITIALIZING to RUNNING.
11:28:34.608 [Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (1/8)#0] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 3.4.0
11:28:34.608 [Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (1/8)#0] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 2e1947d240607d53
11:28:34.608 [Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (1/8)#0] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1687544914608
11:28:34.608 [SourceCoordinator-Source: createevent] INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator - Source Source: createevent registering reader for parallel task 4 (#0) @
11:28:34.608 [flink-akka.actor.default-dispatcher-7] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph - Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (8/8) (d220afe491fa016e76b29529ab091deb_cbc357ccb763df2852fee8c4fc7d55f2_7_0) switched from INITIALIZING to RUNNING.
11:28:34.609 [kafka-producer-network-thread | producer-8] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-8] Cluster ID: D0jK8KEKRqCvHg2y7XT-hQ
11:28:34.609 [Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (2/8)#0] INFO  org.apache.flink.runtime.taskmanager.Task - Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (2/8)#0 (d220afe491fa016e76b29529ab091deb_cbc357ccb763df2852fee8c4fc7d55f2_1_0) switched from INITIALIZING to RUNNING.
11:28:34.609 [kafka-producer-network-thread | producer-4] INFO  org.apache.kafka.clients.producer.internals.TransactionManager - [Producer clientId=producer-4] ProducerId set to 2 with epoch 0
11:28:34.609 [flink-akka.actor.default-dispatcher-8] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph - Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (2/8) (d220afe491fa016e76b29529ab091deb_cbc357ccb763df2852fee8c4fc7d55f2_1_0) switched from INITIALIZING to RUNNING.
11:28:34.610 [SourceCoordinator-Source: createevent] INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator - Source Source: createevent registering reader for parallel task 6 (#0) @
11:28:34.610 [kafka-producer-network-thread | producer-8] INFO  org.apache.kafka.clients.producer.internals.TransactionManager - [Producer clientId=producer-8] ProducerId set to 3 with epoch 0
11:28:34.610 [Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (6/8)#0] INFO  org.apache.flink.runtime.taskmanager.Task - Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (6/8)#0 (d220afe491fa016e76b29529ab091deb_cbc357ccb763df2852fee8c4fc7d55f2_5_0) switched from INITIALIZING to RUNNING.
11:28:34.610 [flink-akka.actor.default-dispatcher-5] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph - Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (6/8) (d220afe491fa016e76b29529ab091deb_cbc357ccb763df2852fee8c4fc7d55f2_5_0) switched from INITIALIZING to RUNNING.
11:28:34.611 [SourceCoordinator-Source: createevent] INFO  org.apache.flink.connector.kafka.source.enumerator.KafkaSourceEnumerator - Assigning splits to readers {6=[[Partition: schema-createevent-0, StartingOffset: -2, StoppingOffset: -9223372036854775808]]}
11:28:34.611 [Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (4/8)#0] INFO  org.apache.flink.runtime.taskmanager.Task - Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (4/8)#0 (d220afe491fa016e76b29529ab091deb_cbc357ccb763df2852fee8c4fc7d55f2_3_0) switched from INITIALIZING to RUNNING.
11:28:34.612 [flink-akka.actor.default-dispatcher-7] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph - Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (4/8) (d220afe491fa016e76b29529ab091deb_cbc357ccb763df2852fee8c4fc7d55f2_3_0) switched from INITIALIZING to RUNNING.
11:28:34.612 [Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (3/8)#0] INFO  org.apache.flink.runtime.taskmanager.Task - Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (3/8)#0 (d220afe491fa016e76b29529ab091deb_cbc357ccb763df2852fee8c4fc7d55f2_2_0) switched from INITIALIZING to RUNNING.
11:28:34.613 [flink-akka.actor.default-dispatcher-5] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph - Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (3/8) (d220afe491fa016e76b29529ab091deb_cbc357ccb763df2852fee8c4fc7d55f2_2_0) switched from INITIALIZING to RUNNING.
11:28:34.614 [Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (1/8)#0] INFO  org.apache.flink.runtime.taskmanager.Task - Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (1/8)#0 (d220afe491fa016e76b29529ab091deb_cbc357ccb763df2852fee8c4fc7d55f2_0_0) switched from INITIALIZING to RUNNING.
11:28:34.614 [flink-akka.actor.default-dispatcher-9] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph - Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (1/8) (d220afe491fa016e76b29529ab091deb_cbc357ccb763df2852fee8c4fc7d55f2_0_0) switched from INITIALIZING to RUNNING.
11:28:34.615 [kafka-producer-network-thread | producer-7] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-7] Cluster ID: D0jK8KEKRqCvHg2y7XT-hQ
11:28:34.615 [kafka-producer-network-thread | producer-7] INFO  org.apache.kafka.clients.producer.internals.TransactionManager - [Producer clientId=producer-7] ProducerId set to 4 with epoch 0
11:28:34.615 [kafka-producer-network-thread | producer-2] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-2] Cluster ID: D0jK8KEKRqCvHg2y7XT-hQ
11:28:34.616 [kafka-producer-network-thread | producer-2] INFO  org.apache.kafka.clients.producer.internals.TransactionManager - [Producer clientId=producer-2] ProducerId set to 5 with epoch 0
11:28:34.616 [kafka-producer-network-thread | producer-3] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-3] Cluster ID: D0jK8KEKRqCvHg2y7XT-hQ
11:28:34.616 [kafka-producer-network-thread | producer-6] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-6] Cluster ID: D0jK8KEKRqCvHg2y7XT-hQ
11:28:34.616 [SourceCoordinator-Source: createevent] INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator - Source Source: createevent registering reader for parallel task 5 (#0) @
11:28:34.616 [SourceCoordinator-Source: createevent] INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator - Source Source: createevent registering reader for parallel task 7 (#0) @
11:28:34.616 [SourceCoordinator-Source: createevent] INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator - Source Source: createevent registering reader for parallel task 1 (#0) @
11:28:34.616 [SourceCoordinator-Source: createevent] INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator - Source Source: createevent registering reader for parallel task 3 (#0) @
11:28:34.616 [kafka-producer-network-thread | producer-3] INFO  org.apache.kafka.clients.producer.internals.TransactionManager - [Producer clientId=producer-3] ProducerId set to 6 with epoch 0
11:28:34.616 [SourceCoordinator-Source: createevent] INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator - Source Source: createevent registering reader for parallel task 0 (#0) @
11:28:34.616 [SourceCoordinator-Source: createevent] INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator - Source Source: createevent registering reader for parallel task 2 (#0) @
11:28:34.616 [kafka-producer-network-thread | producer-6] INFO  org.apache.kafka.clients.producer.internals.TransactionManager - [Producer clientId=producer-6] ProducerId set to 7 with epoch 0
11:28:34.617 [kafka-producer-network-thread | producer-9] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-9] Cluster ID: D0jK8KEKRqCvHg2y7XT-hQ
11:28:34.617 [kafka-producer-network-thread | producer-9] INFO  org.apache.kafka.clients.producer.internals.TransactionManager - [Producer clientId=producer-9] ProducerId set to 8 with epoch 0
11:28:34.618 [Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (7/8)#0] INFO  org.apache.flink.connector.base.source.reader.SourceReaderBase - Adding split(s) to reader: [[Partition: schema-createevent-0, StartingOffset: -2, StoppingOffset: -9223372036854775808]]
11:28:34.619 [Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (7/8)#0] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values:
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:51724]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = createevent-45f40691-1392-43a5-9cce-4fec91cfd2c4-6
	client.rack =
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = createevent-45f40691-1392-43a5-9cce-4fec91cfd2c4
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

11:28:34.620 [Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (7/8)#0] WARN  org.apache.kafka.clients.consumer.ConsumerConfig - These configurations '[client.id.prefix, partition.discovery.interval.ms]' were supplied but are not used yet.
11:28:34.620 [Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (7/8)#0] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 3.4.0
11:28:34.620 [Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (7/8)#0] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 2e1947d240607d53
11:28:34.620 [Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (7/8)#0] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1687544914620
11:28:34.623 [Source Data Fetcher for Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (7/8)#0] INFO  org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher - Starting split fetcher 0
11:28:34.626 [Source Data Fetcher for Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (7/8)#0] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=createevent-45f40691-1392-43a5-9cce-4fec91cfd2c4-6, groupId=createevent-45f40691-1392-43a5-9cce-4fec91cfd2c4] Assigned to partition(s): schema-createevent-0
11:28:34.629 [Source Data Fetcher for Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (7/8)#0] INFO  org.apache.kafka.clients.consumer.internals.SubscriptionState - [Consumer clientId=createevent-45f40691-1392-43a5-9cce-4fec91cfd2c4-6, groupId=createevent-45f40691-1392-43a5-9cce-4fec91cfd2c4] Seeking to earliest offset of partition schema-createevent-0
11:28:34.636 [Source Data Fetcher for Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (7/8)#0] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=createevent-45f40691-1392-43a5-9cce-4fec91cfd2c4-6, groupId=createevent-45f40691-1392-43a5-9cce-4fec91cfd2c4] Resetting the last seen epoch of partition schema-createevent-0 to 0 since the associated topicId changed from null to p1MvE0ssR9mnYo7ybouNhQ
11:28:34.636 [Source Data Fetcher for Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (7/8)#0] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=createevent-45f40691-1392-43a5-9cce-4fec91cfd2c4-6, groupId=createevent-45f40691-1392-43a5-9cce-4fec91cfd2c4] Cluster ID: D0jK8KEKRqCvHg2y7XT-hQ
11:28:34.648 [Source Data Fetcher for Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (7/8)#0] INFO  org.apache.kafka.clients.consumer.internals.SubscriptionState - [Consumer clientId=createevent-45f40691-1392-43a5-9cce-4fec91cfd2c4-6, groupId=createevent-45f40691-1392-43a5-9cce-4fec91cfd2c4] Resetting offset for partition schema-createevent-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:51724 (id: 0 rack: null)], epoch=0}}.
11:28:36.446 [vert.x-eventloop-thread-1] INFO  com.datasqrl.flink.AbstractSubscriptionTest - Connection acknowledged
11:28:36.446 [vert.x-eventloop-thread-1] INFO  com.datasqrl.flink.AbstractSubscriptionTest - Received message: {"type":"ka"}
11:28:36.480 [kafka-producer-network-thread | producer-1] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-1] Resetting the last seen epoch of partition schema-createevent-0 to 0 since the associated topicId changed from null to p1MvE0ssR9mnYo7ybouNhQ
11:28:36.483 [vert.x-kafka-consumer-thread-0] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-bcb673c0-fcf5-4124-98a8-1773e55d92e4-1, groupId=bcb673c0-fcf5-4124-98a8-1773e55d92e4] Resetting the last seen epoch of partition event-3-0 to 0 since the associated topicId changed from null to 3m77TBFRQdqdP8Xjdo77rg
11:28:36.484 [vert.x-kafka-consumer-thread-0] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-bcb673c0-fcf5-4124-98a8-1773e55d92e4-1, groupId=bcb673c0-fcf5-4124-98a8-1773e55d92e4] Cluster ID: D0jK8KEKRqCvHg2y7XT-hQ
11:28:36.489 [data-plane-kafka-request-handler-0] INFO  kafka.zk.AdminZkClient - Creating topic __consumer_offsets with configuration {compression.type=producer, cleanup.policy=compact, segment.bytes=104857600} and initial partition assignment Map(2 -> ArrayBuffer(0), 4 -> ArrayBuffer(0), 1 -> ArrayBuffer(0), 3 -> ArrayBuffer(0), 0 -> ArrayBuffer(0))
11:28:36.495 [controller-event-thread] INFO  kafka.controller.KafkaController - [Controller id=0] New topics: [Set(__consumer_offsets)], deleted topics: [Set()], new partition replica assignment [Set(TopicIdReplicaAssignment(__consumer_offsets,Some(1MIKqyDjQ06gb6ThryGNrg),Map(__consumer_offsets-4 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-3 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-2 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-1 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=))))]
11:28:36.495 [controller-event-thread] INFO  kafka.controller.KafkaController - [Controller id=0] New partition creation callback for __consumer_offsets-4,__consumer_offsets-3,__consumer_offsets-2,__consumer_offsets-0,__consumer_offsets-1
11:28:36.495 [controller-event-thread] INFO  state.change.logger - [Controller id=0 epoch=1] Changed partition __consumer_offsets-4 state from NonExistentPartition to NewPartition with assigned replicas 0
11:28:36.495 [controller-event-thread] INFO  state.change.logger - [Controller id=0 epoch=1] Changed partition __consumer_offsets-3 state from NonExistentPartition to NewPartition with assigned replicas 0
11:28:36.495 [controller-event-thread] INFO  state.change.logger - [Controller id=0 epoch=1] Changed partition __consumer_offsets-2 state from NonExistentPartition to NewPartition with assigned replicas 0
11:28:36.495 [controller-event-thread] INFO  state.change.logger - [Controller id=0 epoch=1] Changed partition __consumer_offsets-0 state from NonExistentPartition to NewPartition with assigned replicas 0
11:28:36.495 [controller-event-thread] INFO  state.change.logger - [Controller id=0 epoch=1] Changed partition __consumer_offsets-1 state from NonExistentPartition to NewPartition with assigned replicas 0
11:28:36.495 [controller-event-thread] INFO  state.change.logger - [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers Set() for 0 partitions
11:28:36.496 [controller-event-thread] INFO  state.change.logger - [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers Set() for 0 partitions
11:28:36.504 [controller-event-thread] INFO  state.change.logger - [Controller id=0 epoch=1] Changed partition __consumer_offsets-4 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), leaderRecoveryState=RECOVERED, partitionEpoch=0)
11:28:36.504 [controller-event-thread] INFO  state.change.logger - [Controller id=0 epoch=1] Changed partition __consumer_offsets-3 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), leaderRecoveryState=RECOVERED, partitionEpoch=0)
11:28:36.504 [controller-event-thread] INFO  state.change.logger - [Controller id=0 epoch=1] Changed partition __consumer_offsets-2 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), leaderRecoveryState=RECOVERED, partitionEpoch=0)
11:28:36.504 [controller-event-thread] INFO  state.change.logger - [Controller id=0 epoch=1] Changed partition __consumer_offsets-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), leaderRecoveryState=RECOVERED, partitionEpoch=0)
11:28:36.504 [controller-event-thread] INFO  state.change.logger - [Controller id=0 epoch=1] Changed partition __consumer_offsets-1 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), leaderRecoveryState=RECOVERED, partitionEpoch=0)
11:28:36.504 [controller-event-thread] INFO  state.change.logger - [Controller id=0 epoch=1] Sending LeaderAndIsr request to broker 0 with 5 become-leader and 0 become-follower partitions
11:28:36.504 [controller-event-thread] INFO  state.change.logger - [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers Set(0) for 5 partitions
11:28:36.505 [controller-event-thread] INFO  state.change.logger - [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers Set() for 0 partitions
11:28:36.505 [data-plane-kafka-request-handler-4] INFO  state.change.logger - [Broker id=0] Handling LeaderAndIsr request correlationId 5 from controller 0 for 5 partitions
11:28:36.506 [data-plane-kafka-request-handler-4] INFO  kafka.server.ReplicaFetcherManager - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(__consumer_offsets-4, __consumer_offsets-3, __consumer_offsets-2, __consumer_offsets-0, __consumer_offsets-1)
11:28:36.506 [data-plane-kafka-request-handler-4] INFO  state.change.logger - [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 5 from controller 0 epoch 1 as part of the become-leader transition for 5 partitions
11:28:36.510 [data-plane-kafka-request-handler-4] INFO  kafka.log.UnifiedLog$ - [LogLoader partition=__consumer_offsets-0, dir=/var/folders/w3/dxm786y50_b8p5tj7xktkngh0000gn/T/kafka-5258867200498972615/log7884909346953798293] Loading producer state till offset 0 with message format version 2
11:28:36.510 [data-plane-kafka-request-handler-4] INFO  kafka.log.LogManager - Created log for partition __consumer_offsets-0 in /var/folders/w3/dxm786y50_b8p5tj7xktkngh0000gn/T/kafka-5258867200498972615/log7884909346953798293/__consumer_offsets-0 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600}
11:28:36.510 [data-plane-kafka-request-handler-4] INFO  kafka.cluster.Partition - [Partition __consumer_offsets-0 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-0
11:28:36.510 [data-plane-kafka-request-handler-4] INFO  kafka.cluster.Partition - [Partition __consumer_offsets-0 broker=0] Log loaded for partition __consumer_offsets-0 with initial high watermark 0
11:28:36.511 [data-plane-kafka-request-handler-4] INFO  state.change.logger - [Broker id=0] Leader __consumer_offsets-0 with topic id Some(1MIKqyDjQ06gb6ThryGNrg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [0], adding replicas [] and removing replicas []. Previous leader epoch was -1.
11:28:36.522 [data-plane-kafka-request-handler-4] INFO  kafka.log.UnifiedLog$ - [LogLoader partition=__consumer_offsets-4, dir=/var/folders/w3/dxm786y50_b8p5tj7xktkngh0000gn/T/kafka-5258867200498972615/log7884909346953798293] Loading producer state till offset 0 with message format version 2
11:28:36.522 [data-plane-kafka-request-handler-4] INFO  kafka.log.LogManager - Created log for partition __consumer_offsets-4 in /var/folders/w3/dxm786y50_b8p5tj7xktkngh0000gn/T/kafka-5258867200498972615/log7884909346953798293/__consumer_offsets-4 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600}
11:28:36.522 [data-plane-kafka-request-handler-4] INFO  kafka.cluster.Partition - [Partition __consumer_offsets-4 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-4
11:28:36.522 [data-plane-kafka-request-handler-4] INFO  kafka.cluster.Partition - [Partition __consumer_offsets-4 broker=0] Log loaded for partition __consumer_offsets-4 with initial high watermark 0
11:28:36.522 [data-plane-kafka-request-handler-4] INFO  state.change.logger - [Broker id=0] Leader __consumer_offsets-4 with topic id Some(1MIKqyDjQ06gb6ThryGNrg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [0], adding replicas [] and removing replicas []. Previous leader epoch was -1.
11:28:36.532 [data-plane-kafka-request-handler-4] INFO  kafka.log.UnifiedLog$ - [LogLoader partition=__consumer_offsets-1, dir=/var/folders/w3/dxm786y50_b8p5tj7xktkngh0000gn/T/kafka-5258867200498972615/log7884909346953798293] Loading producer state till offset 0 with message format version 2
11:28:36.533 [data-plane-kafka-request-handler-4] INFO  kafka.log.LogManager - Created log for partition __consumer_offsets-1 in /var/folders/w3/dxm786y50_b8p5tj7xktkngh0000gn/T/kafka-5258867200498972615/log7884909346953798293/__consumer_offsets-1 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600}
11:28:36.533 [data-plane-kafka-request-handler-4] INFO  kafka.cluster.Partition - [Partition __consumer_offsets-1 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-1
11:28:36.533 [data-plane-kafka-request-handler-4] INFO  kafka.cluster.Partition - [Partition __consumer_offsets-1 broker=0] Log loaded for partition __consumer_offsets-1 with initial high watermark 0
11:28:36.533 [data-plane-kafka-request-handler-4] INFO  state.change.logger - [Broker id=0] Leader __consumer_offsets-1 with topic id Some(1MIKqyDjQ06gb6ThryGNrg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [0], adding replicas [] and removing replicas []. Previous leader epoch was -1.
11:28:36.539 [data-plane-kafka-request-handler-4] INFO  kafka.log.UnifiedLog$ - [LogLoader partition=__consumer_offsets-2, dir=/var/folders/w3/dxm786y50_b8p5tj7xktkngh0000gn/T/kafka-5258867200498972615/log7884909346953798293] Loading producer state till offset 0 with message format version 2
11:28:36.540 [data-plane-kafka-request-handler-4] INFO  kafka.log.LogManager - Created log for partition __consumer_offsets-2 in /var/folders/w3/dxm786y50_b8p5tj7xktkngh0000gn/T/kafka-5258867200498972615/log7884909346953798293/__consumer_offsets-2 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600}
11:28:36.540 [data-plane-kafka-request-handler-4] INFO  kafka.cluster.Partition - [Partition __consumer_offsets-2 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-2
11:28:36.540 [data-plane-kafka-request-handler-4] INFO  kafka.cluster.Partition - [Partition __consumer_offsets-2 broker=0] Log loaded for partition __consumer_offsets-2 with initial high watermark 0
11:28:36.540 [data-plane-kafka-request-handler-4] INFO  state.change.logger - [Broker id=0] Leader __consumer_offsets-2 with topic id Some(1MIKqyDjQ06gb6ThryGNrg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [0], adding replicas [] and removing replicas []. Previous leader epoch was -1.
11:28:36.549 [data-plane-kafka-request-handler-4] INFO  kafka.log.UnifiedLog$ - [LogLoader partition=__consumer_offsets-3, dir=/var/folders/w3/dxm786y50_b8p5tj7xktkngh0000gn/T/kafka-5258867200498972615/log7884909346953798293] Loading producer state till offset 0 with message format version 2
11:28:36.550 [data-plane-kafka-request-handler-4] INFO  kafka.log.LogManager - Created log for partition __consumer_offsets-3 in /var/folders/w3/dxm786y50_b8p5tj7xktkngh0000gn/T/kafka-5258867200498972615/log7884909346953798293/__consumer_offsets-3 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600}
11:28:36.550 [data-plane-kafka-request-handler-4] INFO  kafka.cluster.Partition - [Partition __consumer_offsets-3 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-3
11:28:36.550 [data-plane-kafka-request-handler-4] INFO  kafka.cluster.Partition - [Partition __consumer_offsets-3 broker=0] Log loaded for partition __consumer_offsets-3 with initial high watermark 0
11:28:36.550 [data-plane-kafka-request-handler-4] INFO  state.change.logger - [Broker id=0] Leader __consumer_offsets-3 with topic id Some(1MIKqyDjQ06gb6ThryGNrg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [0], adding replicas [] and removing replicas []. Previous leader epoch was -1.
11:28:36.557 [data-plane-kafka-request-handler-4] INFO  kafka.coordinator.group.GroupCoordinator - [GroupCoordinator 0]: Elected as the group coordinator for partition 3 in epoch 0
11:28:36.558 [data-plane-kafka-request-handler-4] INFO  kafka.coordinator.group.GroupMetadataManager - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-3 for epoch 0
11:28:36.559 [data-plane-kafka-request-handler-4] INFO  kafka.coordinator.group.GroupCoordinator - [GroupCoordinator 0]: Elected as the group coordinator for partition 2 in epoch 0
11:28:36.559 [data-plane-kafka-request-handler-4] INFO  kafka.coordinator.group.GroupMetadataManager - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-2 for epoch 0
11:28:36.559 [data-plane-kafka-request-handler-4] INFO  kafka.coordinator.group.GroupCoordinator - [GroupCoordinator 0]: Elected as the group coordinator for partition 1 in epoch 0
11:28:36.559 [data-plane-kafka-request-handler-4] INFO  kafka.coordinator.group.GroupMetadataManager - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-1 for epoch 0
11:28:36.559 [data-plane-kafka-request-handler-4] INFO  kafka.coordinator.group.GroupCoordinator - [GroupCoordinator 0]: Elected as the group coordinator for partition 4 in epoch 0
11:28:36.559 [data-plane-kafka-request-handler-4] INFO  kafka.coordinator.group.GroupMetadataManager - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-4 for epoch 0
11:28:36.559 [data-plane-kafka-request-handler-4] INFO  kafka.coordinator.group.GroupCoordinator - [GroupCoordinator 0]: Elected as the group coordinator for partition 0 in epoch 0
11:28:36.559 [data-plane-kafka-request-handler-4] INFO  kafka.coordinator.group.GroupMetadataManager - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-0 for epoch 0
11:28:36.559 [data-plane-kafka-request-handler-4] INFO  state.change.logger - [Broker id=0] Finished LeaderAndIsr request in 0ms correlationId 5 from controller 0 for 5 partitions
11:28:36.562 [data-plane-kafka-request-handler-1] INFO  state.change.logger - [Broker id=0] Add 5 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 6
11:28:36.564 [group-metadata-manager-0] INFO  kafka.coordinator.group.GroupMetadataManager - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-3 in 6 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler.
11:28:36.565 [group-metadata-manager-0] INFO  kafka.coordinator.group.GroupMetadataManager - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-2 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler.
11:28:36.568 [group-metadata-manager-0] INFO  kafka.coordinator.group.GroupMetadataManager - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-1 in 9 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler.
11:28:36.569 [group-metadata-manager-0] INFO  kafka.coordinator.group.GroupMetadataManager - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-4 in 10 milliseconds for epoch 0, of which 10 milliseconds was spent in the scheduler.
11:28:36.569 [group-metadata-manager-0] INFO  kafka.coordinator.group.GroupMetadataManager - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-0 in 10 milliseconds for epoch 0, of which 10 milliseconds was spent in the scheduler.
11:28:36.585 [vert.x-eventloop-thread-0] INFO  io.vertx.ext.web.handler.impl.LoggerHandlerImpl - 127.0.0.1 - - [Fri, 23 Jun 2023 18:28:36 GMT] "POST /graphql HTTP/1.1" 200 37 "-" "Vert.x-WebClient/4.3.5"
11:28:36.585 [vert.x-eventloop-thread-0] INFO  io.vertx.ext.web.handler.impl.LoggerHandlerImpl - 127.0.0.1 - - [Fri, 23 Jun 2023 18:28:36 GMT] "POST /graphql HTTP/1.1" 200 37 "-" "Vert.x-WebClient/4.3.5"
11:28:36.586 [vert.x-eventloop-thread-0] INFO  io.vertx.ext.web.handler.impl.LoggerHandlerImpl - 127.0.0.1 - - [Fri, 23 Jun 2023 18:28:36 GMT] "POST /graphql HTTP/1.1" 200 37 "-" "Vert.x-WebClient/4.3.5"
11:28:36.589 [vert.x-kafka-consumer-thread-0] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-bcb673c0-fcf5-4124-98a8-1773e55d92e4-1, groupId=bcb673c0-fcf5-4124-98a8-1773e55d92e4] Discovered group coordinator localhost:51724 (id: 2147483647 rack: null)
11:28:36.592 [vert.x-eventloop-thread-1] INFO  com.datasqrl.flink.AbstractSubscriptionTest - Received response with status code200
11:28:36.593 [vert.x-eventloop-thread-1] INFO  com.datasqrl.flink.AbstractSubscriptionTest - Response Body: {"data":{"createEvent":{"id":"id1"}}}
11:28:36.593 [vert.x-kafka-consumer-thread-0] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-bcb673c0-fcf5-4124-98a8-1773e55d92e4-1, groupId=bcb673c0-fcf5-4124-98a8-1773e55d92e4] (Re-)joining group
11:28:36.593 [vert.x-eventloop-thread-1] INFO  com.datasqrl.flink.AbstractSubscriptionTest - Received response with status code200
11:28:36.593 [vert.x-eventloop-thread-1] INFO  com.datasqrl.flink.AbstractSubscriptionTest - Response Body: {"data":{"createEvent":{"id":"id2"}}}
11:28:36.593 [vert.x-eventloop-thread-1] INFO  com.datasqrl.flink.AbstractSubscriptionTest - Received response with status code200
11:28:36.593 [vert.x-eventloop-thread-1] INFO  com.datasqrl.flink.AbstractSubscriptionTest - Response Body: {"data":{"createEvent":{"id":"id3"}}}
11:28:36.606 [data-plane-kafka-request-handler-1] INFO  kafka.coordinator.group.GroupCoordinator - [GroupCoordinator 0]: Dynamic member with unknown member id joins group bcb673c0-fcf5-4124-98a8-1773e55d92e4 in Empty state. Created a new member id consumer-bcb673c0-fcf5-4124-98a8-1773e55d92e4-1-ead94214-df5b-4c09-9b2f-48f0e63eb98f and request the member to rejoin with this id.
11:28:36.609 [vert.x-kafka-consumer-thread-0] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-bcb673c0-fcf5-4124-98a8-1773e55d92e4-1, groupId=bcb673c0-fcf5-4124-98a8-1773e55d92e4] Request joining group due to: need to re-join with the given member-id: consumer-bcb673c0-fcf5-4124-98a8-1773e55d92e4-1-ead94214-df5b-4c09-9b2f-48f0e63eb98f
11:28:36.609 [vert.x-kafka-consumer-thread-0] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-bcb673c0-fcf5-4124-98a8-1773e55d92e4-1, groupId=bcb673c0-fcf5-4124-98a8-1773e55d92e4] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
11:28:36.609 [vert.x-kafka-consumer-thread-0] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-bcb673c0-fcf5-4124-98a8-1773e55d92e4-1, groupId=bcb673c0-fcf5-4124-98a8-1773e55d92e4] (Re-)joining group
11:28:36.614 [data-plane-kafka-request-handler-2] INFO  kafka.coordinator.group.GroupCoordinator - [GroupCoordinator 0]: Preparing to rebalance group bcb673c0-fcf5-4124-98a8-1773e55d92e4 in state PreparingRebalance with old generation 0 (__consumer_offsets-1) (reason: Adding new member consumer-bcb673c0-fcf5-4124-98a8-1773e55d92e4-1-ead94214-df5b-4c09-9b2f-48f0e63eb98f with group instance id None; client reason: rebalance failed due to MemberIdRequiredException)
11:28:36.618 [executor-Rebalance] INFO  kafka.coordinator.group.GroupCoordinator - [GroupCoordinator 0]: Stabilized group bcb673c0-fcf5-4124-98a8-1773e55d92e4 generation 1 (__consumer_offsets-1) with 1 members
11:28:36.620 [vert.x-kafka-consumer-thread-0] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-bcb673c0-fcf5-4124-98a8-1773e55d92e4-1, groupId=bcb673c0-fcf5-4124-98a8-1773e55d92e4] Successfully joined group with generation Generation{generationId=1, memberId='consumer-bcb673c0-fcf5-4124-98a8-1773e55d92e4-1-ead94214-df5b-4c09-9b2f-48f0e63eb98f', protocol='range'}
11:28:36.621 [vert.x-kafka-consumer-thread-0] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-bcb673c0-fcf5-4124-98a8-1773e55d92e4-1, groupId=bcb673c0-fcf5-4124-98a8-1773e55d92e4] Finished assignment for group at generation 1: {consumer-bcb673c0-fcf5-4124-98a8-1773e55d92e4-1-ead94214-df5b-4c09-9b2f-48f0e63eb98f=Assignment(partitions=[event-3-0])}
11:28:36.632 [data-plane-kafka-request-handler-4] INFO  kafka.coordinator.group.GroupCoordinator - [GroupCoordinator 0]: Assignment received from leader consumer-bcb673c0-fcf5-4124-98a8-1773e55d92e4-1-ead94214-df5b-4c09-9b2f-48f0e63eb98f for group bcb673c0-fcf5-4124-98a8-1773e55d92e4 for generation 1. The group has 1 members, 0 of which are static.
11:28:36.644 [vert.x-kafka-consumer-thread-0] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-bcb673c0-fcf5-4124-98a8-1773e55d92e4-1, groupId=bcb673c0-fcf5-4124-98a8-1773e55d92e4] Successfully synced group in generation Generation{generationId=1, memberId='consumer-bcb673c0-fcf5-4124-98a8-1773e55d92e4-1-ead94214-df5b-4c09-9b2f-48f0e63eb98f', protocol='range'}
11:28:36.644 [vert.x-kafka-consumer-thread-0] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-bcb673c0-fcf5-4124-98a8-1773e55d92e4-1, groupId=bcb673c0-fcf5-4124-98a8-1773e55d92e4] Notifying assignor about the new Assignment(partitions=[event-3-0])
11:28:36.644 [vert.x-kafka-consumer-thread-0] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-bcb673c0-fcf5-4124-98a8-1773e55d92e4-1, groupId=bcb673c0-fcf5-4124-98a8-1773e55d92e4] Adding newly assigned partitions: event-3-0
11:28:36.652 [vert.x-kafka-consumer-thread-0] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-bcb673c0-fcf5-4124-98a8-1773e55d92e4-1, groupId=bcb673c0-fcf5-4124-98a8-1773e55d92e4] Found no committed offset for partition event-3-0
11:28:36.661 [vert.x-kafka-consumer-thread-0] INFO  org.apache.kafka.clients.consumer.internals.SubscriptionState - [Consumer clientId=consumer-bcb673c0-fcf5-4124-98a8-1773e55d92e4-1, groupId=bcb673c0-fcf5-4124-98a8-1773e55d92e4] Resetting offset for partition event-3-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:51724 (id: 0 rack: null)], epoch=0}}.
11:28:36.726 [Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (7/8)#0] INFO  org.apache.kafka.clients.producer.ProducerConfig - ProducerConfig values:
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:51724]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-10
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 3600000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

11:28:36.727 [Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (7/8)#0] INFO  org.apache.kafka.clients.producer.KafkaProducer - [Producer clientId=producer-10] Instantiated an idempotent producer.
11:28:36.728 [Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (7/8)#0] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 3.4.0
11:28:36.728 [Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (7/8)#0] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 2e1947d240607d53
11:28:36.728 [Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (7/8)#0] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1687544916728
11:28:36.732 [kafka-producer-network-thread | producer-10] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-10] Resetting the last seen epoch of partition event-3-0 to 0 since the associated topicId changed from null to 3m77TBFRQdqdP8Xjdo77rg
11:28:36.733 [kafka-producer-network-thread | producer-10] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-10] Cluster ID: D0jK8KEKRqCvHg2y7XT-hQ
11:28:36.733 [kafka-producer-network-thread | producer-10] INFO  org.apache.kafka.clients.producer.internals.TransactionManager - [Producer clientId=producer-10] ProducerId set to 9 with epoch 0
11:28:36.733 [Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (7/8)#0] INFO  org.apache.kafka.clients.producer.KafkaProducer - [Producer clientId=producer-10] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
11:28:36.734 [Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (7/8)#0] INFO  org.apache.kafka.common.metrics.Metrics - Metrics scheduler closed
11:28:36.734 [Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (7/8)#0] INFO  org.apache.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
11:28:36.734 [Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (7/8)#0] INFO  org.apache.kafka.common.metrics.Metrics - Metrics reporters closed
11:28:36.734 [Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (7/8)#0] INFO  org.apache.kafka.common.utils.AppInfoParser - App info kafka.producer for producer-10 unregistered
11:28:36.736 [kafka-producer-network-thread | producer-4] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-4] Resetting the last seen epoch of partition event-3-0 to 0 since the associated topicId changed from null to 3m77TBFRQdqdP8Xjdo77rg
11:28:36.747 [vert.x-eventloop-thread-1] INFO  com.datasqrl.flink.AbstractSubscriptionTest - Data: {"receiveEvent":{"id":"id1","name":"name1"}}
11:28:36.748 [vert.x-eventloop-thread-1] INFO  com.datasqrl.flink.AbstractSubscriptionTest - Data: {"receiveEvent":{"id":"id2","name":"name2"}}
11:28:36.749 [vert.x-eventloop-thread-1] INFO  com.datasqrl.flink.AbstractSubscriptionTest - Data: {"receiveEvent":{"id":"id3","name":"name3"}}
11:28:37.106 [main] INFO  kafka.server.KafkaServer - [KafkaServer id=0] shutting down
11:28:37.106 [main] INFO  kafka.server.KafkaServer - [KafkaServer id=0] Starting controlled shutdown
11:28:37.110 [controller-event-thread] INFO  kafka.controller.KafkaController - [Controller id=0] Shutting down broker 0
11:28:37.113 [controller-event-thread] INFO  state.change.logger - [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers Set() for 0 partitions
11:28:37.117 [main] INFO  kafka.server.KafkaServer - [KafkaServer id=0] Controlled shutdown request returned successfully after 0ms
11:28:37.118 [main] INFO  kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread - [/config/changes-event-process-thread]: Shutting down
11:28:37.118 [/config/changes-event-process-thread] INFO  kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread - [/config/changes-event-process-thread]: Stopped
11:28:37.118 [main] INFO  kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread - [/config/changes-event-process-thread]: Shutdown completed
11:28:37.119 [main] INFO  kafka.network.SocketServer - [SocketServer listenerType=ZK_BROKER, nodeId=0] Stopping socket server request processors
11:28:37.120 [kafka-producer-network-thread | producer-2] INFO  org.apache.kafka.clients.NetworkClient - [Producer clientId=producer-2] Node -1 disconnected.
11:28:37.121 [kafka-producer-network-thread | producer-6] INFO  org.apache.kafka.clients.NetworkClient - [Producer clientId=producer-6] Node -1 disconnected.
11:28:37.121 [kafka-producer-network-thread | producer-3] INFO  org.apache.kafka.clients.NetworkClient - [Producer clientId=producer-3] Node -1 disconnected.
11:28:37.121 [kafka-producer-network-thread | producer-4] INFO  org.apache.kafka.clients.NetworkClient - [Producer clientId=producer-4] Node -1 disconnected.
11:28:37.121 [Source Data Fetcher for Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (7/8)#0] INFO  org.apache.kafka.clients.NetworkClient - [Consumer clientId=createevent-45f40691-1392-43a5-9cce-4fec91cfd2c4-6, groupId=createevent-45f40691-1392-43a5-9cce-4fec91cfd2c4] Node -1 disconnected.
11:28:37.121 [kafka-producer-network-thread | producer-1] INFO  org.apache.kafka.clients.NetworkClient - [Producer clientId=producer-1] Node 0 disconnected.
11:28:37.121 [kafka-producer-network-thread | producer-1] INFO  org.apache.kafka.clients.NetworkClient - [Producer clientId=producer-1] Node -1 disconnected.
11:28:37.121 [vert.x-kafka-consumer-thread-0] INFO  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-bcb673c0-fcf5-4124-98a8-1773e55d92e4-1, groupId=bcb673c0-fcf5-4124-98a8-1773e55d92e4] Node 0 disconnected.
11:28:37.121 [kafka-admin-client-thread | createevent-45f40691-1392-43a5-9cce-4fec91cfd2c4-enumerator-admin-client] INFO  org.apache.kafka.clients.NetworkClient - [AdminClient clientId=createevent-45f40691-1392-43a5-9cce-4fec91cfd2c4-enumerator-admin-client] Node 0 disconnected.
11:28:37.122 [kafka-producer-network-thread | producer-9] INFO  org.apache.kafka.clients.NetworkClient - [Producer clientId=producer-9] Node -1 disconnected.
11:28:37.122 [vert.x-kafka-consumer-thread-0] INFO  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-bcb673c0-fcf5-4124-98a8-1773e55d92e4-1, groupId=bcb673c0-fcf5-4124-98a8-1773e55d92e4] Cancelled in-flight FETCH request with correlation id 15 due to node 0 being disconnected (elapsed time since creation: 373ms, elapsed time since send: 373ms, request timeout: 30000ms)
11:28:37.122 [vert.x-kafka-consumer-thread-0] INFO  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-bcb673c0-fcf5-4124-98a8-1773e55d92e4-1, groupId=bcb673c0-fcf5-4124-98a8-1773e55d92e4] Node 2147483647 disconnected.
11:28:37.122 [vert.x-kafka-consumer-thread-0] INFO  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-bcb673c0-fcf5-4124-98a8-1773e55d92e4-1, groupId=bcb673c0-fcf5-4124-98a8-1773e55d92e4] Node -1 disconnected.
11:28:37.122 [kafka-admin-client-thread | createevent-45f40691-1392-43a5-9cce-4fec91cfd2c4-enumerator-admin-client] INFO  org.apache.kafka.clients.NetworkClient - [AdminClient clientId=createevent-45f40691-1392-43a5-9cce-4fec91cfd2c4-enumerator-admin-client] Node -1 disconnected.
11:28:37.122 [kafka-producer-network-thread | producer-4] INFO  org.apache.kafka.clients.NetworkClient - [Producer clientId=producer-4] Node 0 disconnected.
11:28:37.122 [kafka-producer-network-thread | producer-4] INFO  org.apache.kafka.clients.NetworkClient - [Producer clientId=producer-4] Cancelled in-flight METADATA request with correlation id 8 due to node 0 being disconnected (elapsed time since creation: 0ms, elapsed time since send: 0ms, request timeout: 30000ms)
11:28:37.122 [kafka-producer-network-thread | producer-7] INFO  org.apache.kafka.clients.NetworkClient - [Producer clientId=producer-7] Node -1 disconnected.
11:28:37.122 [kafka-producer-network-thread | producer-5] INFO  org.apache.kafka.clients.NetworkClient - [Producer clientId=producer-5] Node -1 disconnected.
11:28:37.122 [Source Data Fetcher for Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (7/8)#0] INFO  org.apache.kafka.clients.NetworkClient - [Consumer clientId=createevent-45f40691-1392-43a5-9cce-4fec91cfd2c4-6, groupId=createevent-45f40691-1392-43a5-9cce-4fec91cfd2c4] Node 0 disconnected.
11:28:37.122 [Source Data Fetcher for Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (7/8)#0] INFO  org.apache.kafka.clients.NetworkClient - [Consumer clientId=createevent-45f40691-1392-43a5-9cce-4fec91cfd2c4-6, groupId=createevent-45f40691-1392-43a5-9cce-4fec91cfd2c4] Cancelled in-flight FETCH request with correlation id 9 due to node 0 being disconnected (elapsed time since creation: 47ms, elapsed time since send: 47ms, request timeout: 30000ms)
11:28:37.122 [Source Data Fetcher for Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (7/8)#0] INFO  org.apache.kafka.clients.NetworkClient - [Consumer clientId=createevent-45f40691-1392-43a5-9cce-4fec91cfd2c4-6, groupId=createevent-45f40691-1392-43a5-9cce-4fec91cfd2c4] Cancelled in-flight METADATA request with correlation id 10 due to node 0 being disconnected (elapsed time since creation: 0ms, elapsed time since send: 0ms, request timeout: 30000ms)
11:28:37.122 [BrokerToControllerChannelManager broker=0 name=forwarding] INFO  org.apache.kafka.clients.NetworkClient - [BrokerToControllerChannelManager broker=0 name=forwarding] Node 0 disconnected.
11:28:37.122 [kafka-producer-network-thread | producer-8] INFO  org.apache.kafka.clients.NetworkClient - [Producer clientId=producer-8] Node -1 disconnected.
11:28:37.123 [kafka-coordinator-heartbeat-thread | bcb673c0-fcf5-4124-98a8-1773e55d92e4] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-bcb673c0-fcf5-4124-98a8-1773e55d92e4-1, groupId=bcb673c0-fcf5-4124-98a8-1773e55d92e4] Group coordinator localhost:51724 (id: 2147483647 rack: null) is unavailable or invalid due to cause: coordinator unavailable. isDisconnected: true. Rediscovery will be attempted.
11:28:37.124 [kafka-producer-network-thread | producer-9] INFO  org.apache.kafka.clients.NetworkClient - [Producer clientId=producer-9] Node 0 disconnected.
11:28:37.125 [kafka-producer-network-thread | producer-9] WARN  org.apache.kafka.clients.NetworkClient - [Producer clientId=producer-9] Connection to node 0 (localhost/127.0.0.1:51724) could not be established. Broker may not be available.
11:28:37.125 [kafka-producer-network-thread | producer-6] INFO  org.apache.kafka.clients.NetworkClient - [Producer clientId=producer-6] Node 0 disconnected.
11:28:37.125 [kafka-producer-network-thread | producer-3] INFO  org.apache.kafka.clients.NetworkClient - [Producer clientId=producer-3] Node 0 disconnected.
11:28:37.125 [kafka-producer-network-thread | producer-6] WARN  org.apache.kafka.clients.NetworkClient - [Producer clientId=producer-6] Connection to node 0 (localhost/127.0.0.1:51724) could not be established. Broker may not be available.
11:28:37.125 [kafka-producer-network-thread | producer-3] WARN  org.apache.kafka.clients.NetworkClient - [Producer clientId=producer-3] Connection to node 0 (localhost/127.0.0.1:51724) could not be established. Broker may not be available.
11:28:37.125 [kafka-producer-network-thread | producer-7] INFO  org.apache.kafka.clients.NetworkClient - [Producer clientId=producer-7] Node 0 disconnected.
11:28:37.125 [kafka-producer-network-thread | producer-7] WARN  org.apache.kafka.clients.NetworkClient - [Producer clientId=producer-7] Connection to node 0 (localhost/127.0.0.1:51724) could not be established. Broker may not be available.
11:28:37.125 [kafka-producer-network-thread | producer-5] INFO  org.apache.kafka.clients.NetworkClient - [Producer clientId=producer-5] Node 0 disconnected.
11:28:37.125 [kafka-producer-network-thread | producer-5] WARN  org.apache.kafka.clients.NetworkClient - [Producer clientId=producer-5] Connection to node 0 (localhost/127.0.0.1:51724) could not be established. Broker may not be available.
11:28:37.125 [kafka-producer-network-thread | producer-8] INFO  org.apache.kafka.clients.NetworkClient - [Producer clientId=producer-8] Node 0 disconnected.
11:28:37.125 [kafka-producer-network-thread | producer-8] WARN  org.apache.kafka.clients.NetworkClient - [Producer clientId=producer-8] Connection to node 0 (localhost/127.0.0.1:51724) could not be established. Broker may not be available.
11:28:37.125 [kafka-producer-network-thread | producer-2] INFO  org.apache.kafka.clients.NetworkClient - [Producer clientId=producer-2] Node 0 disconnected.
11:28:37.125 [kafka-producer-network-thread | producer-2] WARN  org.apache.kafka.clients.NetworkClient - [Producer clientId=producer-2] Connection to node 0 (localhost/127.0.0.1:51724) could not be established. Broker may not be available.
11:28:37.126 [main] INFO  kafka.network.SocketServer - [SocketServer listenerType=ZK_BROKER, nodeId=0] Stopped socket server request processors
11:28:37.122 [vert.x-kafka-consumer-thread-0] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=consumer-bcb673c0-fcf5-4124-98a8-1773e55d92e4-1, groupId=bcb673c0-fcf5-4124-98a8-1773e55d92e4] Error sending fetch request (sessionId=752704801, epoch=3) to node 0:
org.apache.kafka.common.errors.DisconnectException: null
11:28:37.122 [Source Data Fetcher for Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (7/8)#0] INFO  org.apache.kafka.clients.FetchSessionHandler - [Consumer clientId=createevent-45f40691-1392-43a5-9cce-4fec91cfd2c4-6, groupId=createevent-45f40691-1392-43a5-9cce-4fec91cfd2c4] Error sending fetch request (sessionId=1603144779, epoch=5) to node 0:
org.apache.kafka.common.errors.DisconnectException: null
11:28:37.127 [main] INFO  kafka.server.KafkaRequestHandlerPool - [data-plane Kafka Request Handler on Broker 0], shutting down
11:28:37.129 [main] INFO  kafka.server.KafkaRequestHandlerPool - [data-plane Kafka Request Handler on Broker 0], shut down completely
11:28:37.133 [main] INFO  kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper - [ExpirationReaper-0-AlterAcls]: Shutting down
11:28:37.135 [main] INFO  kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper - [ExpirationReaper-0-AlterAcls]: Shutdown completed
11:28:37.135 [ExpirationReaper-0-AlterAcls] INFO  kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper - [ExpirationReaper-0-AlterAcls]: Stopped
11:28:37.136 [main] INFO  kafka.server.KafkaApis - [KafkaApi-0] Shutdown complete.
11:28:37.136 [main] INFO  kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper - [ExpirationReaper-0-topic]: Shutting down
11:28:37.137 [main] INFO  kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper - [ExpirationReaper-0-topic]: Shutdown completed
11:28:37.137 [ExpirationReaper-0-topic] INFO  kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper - [ExpirationReaper-0-topic]: Stopped
11:28:37.140 [main] INFO  kafka.coordinator.transaction.TransactionCoordinator - [TransactionCoordinator id=0] Shutting down.
11:28:37.141 [main] INFO  kafka.coordinator.transaction.TransactionStateManager - [Transaction State Manager 0]: Shutdown complete
11:28:37.141 [main] INFO  kafka.coordinator.transaction.TransactionMarkerChannelManager - [Transaction Marker Channel Manager 0]: Shutting down
11:28:37.142 [TxnMarkerSenderThread-0] INFO  kafka.coordinator.transaction.TransactionMarkerChannelManager - [Transaction Marker Channel Manager 0]: Stopped
11:28:37.142 [main] INFO  kafka.coordinator.transaction.TransactionMarkerChannelManager - [Transaction Marker Channel Manager 0]: Shutdown completed
11:28:37.143 [main] INFO  kafka.coordinator.transaction.TransactionCoordinator - [TransactionCoordinator id=0] Shutdown complete.
11:28:37.144 [main] INFO  kafka.coordinator.group.GroupCoordinator - [GroupCoordinator 0]: Shutting down.
11:28:37.144 [main] INFO  kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper - [ExpirationReaper-0-Heartbeat]: Shutting down
11:28:37.145 [ExpirationReaper-0-Heartbeat] INFO  kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper - [ExpirationReaper-0-Heartbeat]: Stopped
11:28:37.145 [main] INFO  kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper - [ExpirationReaper-0-Heartbeat]: Shutdown completed
11:28:37.147 [main] INFO  kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper - [ExpirationReaper-0-Rebalance]: Shutting down
11:28:37.148 [ExpirationReaper-0-Rebalance] INFO  kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper - [ExpirationReaper-0-Rebalance]: Stopped
11:28:37.148 [main] INFO  kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper - [ExpirationReaper-0-Rebalance]: Shutdown completed
11:28:37.149 [main] INFO  kafka.coordinator.group.GroupCoordinator - [GroupCoordinator 0]: Shutdown complete.
11:28:37.149 [main] INFO  kafka.server.ReplicaManager - [ReplicaManager broker=0] Shutting down
11:28:37.150 [main] INFO  kafka.server.ReplicaManager$LogDirFailureHandler - [LogDirFailureHandler]: Shutting down
11:28:37.151 [LogDirFailureHandler] INFO  kafka.server.ReplicaManager$LogDirFailureHandler - [LogDirFailureHandler]: Stopped
11:28:37.151 [main] INFO  kafka.server.ReplicaManager$LogDirFailureHandler - [LogDirFailureHandler]: Shutdown completed
11:28:37.151 [main] INFO  kafka.server.ReplicaFetcherManager - [ReplicaFetcherManager on broker 0] shutting down
11:28:37.152 [main] INFO  kafka.server.ReplicaFetcherManager - [ReplicaFetcherManager on broker 0] shutdown completed
11:28:37.152 [main] INFO  kafka.server.ReplicaAlterLogDirsManager - [ReplicaAlterLogDirsManager on broker 0] shutting down
11:28:37.152 [main] INFO  kafka.server.ReplicaAlterLogDirsManager - [ReplicaAlterLogDirsManager on broker 0] shutdown completed
11:28:37.152 [main] INFO  kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper - [ExpirationReaper-0-Fetch]: Shutting down
11:28:37.153 [ExpirationReaper-0-Fetch] INFO  kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper - [ExpirationReaper-0-Fetch]: Stopped
11:28:37.153 [main] INFO  kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper - [ExpirationReaper-0-Fetch]: Shutdown completed
11:28:37.154 [main] INFO  kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper - [ExpirationReaper-0-Produce]: Shutting down
11:28:37.156 [main] INFO  kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper - [ExpirationReaper-0-Produce]: Shutdown completed
11:28:37.156 [ExpirationReaper-0-Produce] INFO  kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper - [ExpirationReaper-0-Produce]: Stopped
11:28:37.156 [main] INFO  kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper - [ExpirationReaper-0-DeleteRecords]: Shutting down
11:28:37.159 [ExpirationReaper-0-DeleteRecords] INFO  kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper - [ExpirationReaper-0-DeleteRecords]: Stopped
11:28:37.159 [main] INFO  kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper - [ExpirationReaper-0-DeleteRecords]: Shutdown completed
11:28:37.160 [main] INFO  kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper - [ExpirationReaper-0-ElectLeader]: Shutting down
11:28:37.163 [ExpirationReaper-0-ElectLeader] INFO  kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper - [ExpirationReaper-0-ElectLeader]: Stopped
11:28:37.163 [main] INFO  kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper - [ExpirationReaper-0-ElectLeader]: Shutdown completed
11:28:37.171 [main] INFO  kafka.server.ReplicaManager - [ReplicaManager broker=0] Shut down completely
11:28:37.171 [main] INFO  kafka.server.BrokerToControllerRequestThread - [BrokerToControllerChannelManager broker=0 name=alterPartition]: Shutting down
11:28:37.171 [main] INFO  kafka.server.BrokerToControllerRequestThread - [BrokerToControllerChannelManager broker=0 name=alterPartition]: Shutdown completed
11:28:37.171 [BrokerToControllerChannelManager broker=0 name=alterPartition] INFO  kafka.server.BrokerToControllerRequestThread - [BrokerToControllerChannelManager broker=0 name=alterPartition]: Stopped
11:28:37.172 [main] INFO  kafka.server.BrokerToControllerChannelManagerImpl - Broker to controller channel manager for alterPartition shutdown
11:28:37.172 [main] INFO  kafka.server.BrokerToControllerRequestThread - [BrokerToControllerChannelManager broker=0 name=forwarding]: Shutting down
11:28:37.172 [BrokerToControllerChannelManager broker=0 name=forwarding] INFO  kafka.server.BrokerToControllerRequestThread - [BrokerToControllerChannelManager broker=0 name=forwarding]: Stopped
11:28:37.172 [main] INFO  kafka.server.BrokerToControllerRequestThread - [BrokerToControllerChannelManager broker=0 name=forwarding]: Shutdown completed
11:28:37.173 [main] INFO  kafka.server.BrokerToControllerChannelManagerImpl - Broker to controller channel manager for forwarding shutdown
11:28:37.173 [main] INFO  kafka.log.LogManager - Shutting down.
11:28:37.174 [main] INFO  kafka.log.LogCleaner - Shutting down the log cleaner.
11:28:37.174 [main] INFO  kafka.log.LogCleaner - [kafka-log-cleaner-thread-0]: Shutting down
11:28:37.175 [kafka-log-cleaner-thread-0] INFO  kafka.log.LogCleaner - [kafka-log-cleaner-thread-0]: Stopped
11:28:37.175 [main] INFO  kafka.log.LogCleaner - [kafka-log-cleaner-thread-0]: Shutdown completed
11:28:37.215 [log-closing-/var/folders/w3/dxm786y50_b8p5tj7xktkngh0000gn/T/kafka-5258867200498972615/log7884909346953798293] INFO  kafka.log.ProducerStateManager - [ProducerStateManager partition=__consumer_offsets-1] Wrote producer snapshot at offset 1 with 0 producer ids in 0 ms.
11:28:37.223 [vert.x-kafka-consumer-thread-0] INFO  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-bcb673c0-fcf5-4124-98a8-1773e55d92e4-1, groupId=bcb673c0-fcf5-4124-98a8-1773e55d92e4] Node 0 disconnected.
11:28:37.223 [kafka-producer-network-thread | producer-1] INFO  org.apache.kafka.clients.NetworkClient - [Producer clientId=producer-1] Node 0 disconnected.
11:28:37.223 [vert.x-kafka-consumer-thread-0] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-bcb673c0-fcf5-4124-98a8-1773e55d92e4-1, groupId=bcb673c0-fcf5-4124-98a8-1773e55d92e4] Connection to node 0 (localhost/127.0.0.1:51724) could not be established. Broker may not be available.
11:28:37.223 [kafka-producer-network-thread | producer-4] INFO  org.apache.kafka.clients.NetworkClient - [Producer clientId=producer-4] Node 0 disconnected.
11:28:37.223 [kafka-producer-network-thread | producer-1] WARN  org.apache.kafka.clients.NetworkClient - [Producer clientId=producer-1] Connection to node 0 (localhost/127.0.0.1:51724) could not be established. Broker may not be available.
11:28:37.223 [kafka-producer-network-thread | producer-4] WARN  org.apache.kafka.clients.NetworkClient - [Producer clientId=producer-4] Connection to node 0 (localhost/127.0.0.1:51724) could not be established. Broker may not be available.
11:28:37.223 [kafka-admin-client-thread | createevent-45f40691-1392-43a5-9cce-4fec91cfd2c4-enumerator-admin-client] INFO  org.apache.kafka.clients.NetworkClient - [AdminClient clientId=createevent-45f40691-1392-43a5-9cce-4fec91cfd2c4-enumerator-admin-client] Node 0 disconnected.
11:28:37.223 [Source Data Fetcher for Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (7/8)#0] INFO  org.apache.kafka.clients.NetworkClient - [Consumer clientId=createevent-45f40691-1392-43a5-9cce-4fec91cfd2c4-6, groupId=createevent-45f40691-1392-43a5-9cce-4fec91cfd2c4] Node 0 disconnected.
11:28:37.223 [kafka-admin-client-thread | createevent-45f40691-1392-43a5-9cce-4fec91cfd2c4-enumerator-admin-client] WARN  org.apache.kafka.clients.NetworkClient - [AdminClient clientId=createevent-45f40691-1392-43a5-9cce-4fec91cfd2c4-enumerator-admin-client] Connection to node 0 (localhost/127.0.0.1:51724) could not be established. Broker may not be available.
11:28:37.223 [Source Data Fetcher for Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (7/8)#0] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=createevent-45f40691-1392-43a5-9cce-4fec91cfd2c4-6, groupId=createevent-45f40691-1392-43a5-9cce-4fec91cfd2c4] Connection to node 0 (localhost/127.0.0.1:51724) could not be established. Broker may not be available.
11:28:37.225 [kafka-producer-network-thread | producer-9] INFO  org.apache.kafka.clients.NetworkClient - [Producer clientId=producer-9] Node 0 disconnected.
11:28:37.225 [kafka-producer-network-thread | producer-9] WARN  org.apache.kafka.clients.NetworkClient - [Producer clientId=producer-9] Connection to node 0 (localhost/127.0.0.1:51724) could not be established. Broker may not be available.
11:28:37.225 [kafka-producer-network-thread | producer-8] INFO  org.apache.kafka.clients.NetworkClient - [Producer clientId=producer-8] Node 0 disconnected.
11:28:37.225 [kafka-producer-network-thread | producer-8] WARN  org.apache.kafka.clients.NetworkClient - [Producer clientId=producer-8] Connection to node 0 (localhost/127.0.0.1:51724) could not be established. Broker may not be available.
11:28:37.225 [kafka-producer-network-thread | producer-3] INFO  org.apache.kafka.clients.NetworkClient - [Producer clientId=producer-3] Node 0 disconnected.
11:28:37.225 [kafka-producer-network-thread | producer-3] WARN  org.apache.kafka.clients.NetworkClient - [Producer clientId=producer-3] Connection to node 0 (localhost/127.0.0.1:51724) could not be established. Broker may not be available.
11:28:37.226 [kafka-producer-network-thread | producer-7] INFO  org.apache.kafka.clients.NetworkClient - [Producer clientId=producer-7] Node 0 disconnected.
11:28:37.226 [kafka-producer-network-thread | producer-7] WARN  org.apache.kafka.clients.NetworkClient - [Producer clientId=producer-7] Connection to node 0 (localhost/127.0.0.1:51724) could not be established. Broker may not be available.
11:28:37.226 [kafka-producer-network-thread | producer-2] INFO  org.apache.kafka.clients.NetworkClient - [Producer clientId=producer-2] Node 0 disconnected.
11:28:37.226 [kafka-producer-network-thread | producer-2] WARN  org.apache.kafka.clients.NetworkClient - [Producer clientId=producer-2] Connection to node 0 (localhost/127.0.0.1:51724) could not be established. Broker may not be available.
11:28:37.226 [kafka-producer-network-thread | producer-6] INFO  org.apache.kafka.clients.NetworkClient - [Producer clientId=producer-6] Node 0 disconnected.
11:28:37.226 [kafka-producer-network-thread | producer-6] WARN  org.apache.kafka.clients.NetworkClient - [Producer clientId=producer-6] Connection to node 0 (localhost/127.0.0.1:51724) could not be established. Broker may not be available.
11:28:37.226 [kafka-producer-network-thread | producer-5] INFO  org.apache.kafka.clients.NetworkClient - [Producer clientId=producer-5] Node 0 disconnected.
11:28:37.226 [kafka-producer-network-thread | producer-5] WARN  org.apache.kafka.clients.NetworkClient - [Producer clientId=producer-5] Connection to node 0 (localhost/127.0.0.1:51724) could not be established. Broker may not be available.
11:28:37.236 [log-closing-/var/folders/w3/dxm786y50_b8p5tj7xktkngh0000gn/T/kafka-5258867200498972615/log7884909346953798293] INFO  kafka.log.ProducerStateManager - [ProducerStateManager partition=schema-createevent-0] Wrote producer snapshot at offset 3 with 1 producer ids in 0 ms.
11:28:37.250 [log-closing-/var/folders/w3/dxm786y50_b8p5tj7xktkngh0000gn/T/kafka-5258867200498972615/log7884909346953798293] INFO  kafka.log.ProducerStateManager - [ProducerStateManager partition=event-3-0] Wrote producer snapshot at offset 3 with 1 producer ids in 0 ms.
11:28:37.289 [main] INFO  kafka.log.LogManager - Shutdown complete.
11:28:37.289 [main] INFO  kafka.controller.ControllerEventManager$ControllerEventThread - [ControllerEventThread controllerId=0] Shutting down
11:28:37.289 [main] INFO  kafka.controller.ControllerEventManager$ControllerEventThread - [ControllerEventThread controllerId=0] Shutdown completed
11:28:37.289 [controller-event-thread] INFO  kafka.controller.ControllerEventManager$ControllerEventThread - [ControllerEventThread controllerId=0] Stopped
11:28:37.291 [main] INFO  kafka.controller.ZkPartitionStateMachine - [PartitionStateMachine controllerId=0] Stopped partition state machine
11:28:37.291 [main] INFO  kafka.controller.ZkReplicaStateMachine - [ReplicaStateMachine controllerId=0] Stopped replica state machine
11:28:37.292 [main] INFO  kafka.controller.RequestSendThread - [RequestSendThread controllerId=0] Shutting down
11:28:37.292 [main] INFO  kafka.controller.RequestSendThread - [RequestSendThread controllerId=0] Shutdown completed
11:28:37.292 [Controller-0-to-broker-0-send-thread] INFO  kafka.controller.RequestSendThread - [RequestSendThread controllerId=0] Stopped
11:28:37.293 [main] INFO  kafka.controller.KafkaController - [Controller id=0] Resigned
11:28:37.293 [main] INFO  kafka.server.FinalizedFeatureChangeListener$ChangeNotificationProcessorThread - [feature-zk-node-event-process-thread]: Shutting down
11:28:37.293 [feature-zk-node-event-process-thread] INFO  kafka.server.FinalizedFeatureChangeListener$ChangeNotificationProcessorThread - [feature-zk-node-event-process-thread]: Stopped
11:28:37.293 [main] INFO  kafka.server.FinalizedFeatureChangeListener$ChangeNotificationProcessorThread - [feature-zk-node-event-process-thread]: Shutdown completed
11:28:37.294 [main] INFO  kafka.zookeeper.ZooKeeperClient - [ZooKeeperClient Kafka server] Closing.
11:28:37.324 [kafka-producer-network-thread | producer-1] INFO  org.apache.kafka.clients.NetworkClient - [Producer clientId=producer-1] Node 0 disconnected.
11:28:37.324 [kafka-producer-network-thread | producer-4] INFO  org.apache.kafka.clients.NetworkClient - [Producer clientId=producer-4] Node 0 disconnected.
11:28:37.324 [kafka-producer-network-thread | producer-1] WARN  org.apache.kafka.clients.NetworkClient - [Producer clientId=producer-1] Connection to node 0 (localhost/127.0.0.1:51724) could not be established. Broker may not be available.
11:28:37.324 [kafka-producer-network-thread | producer-4] WARN  org.apache.kafka.clients.NetworkClient - [Producer clientId=producer-4] Connection to node 0 (localhost/127.0.0.1:51724) could not be established. Broker may not be available.
11:28:37.325 [Source Data Fetcher for Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (7/8)#0] INFO  org.apache.kafka.clients.NetworkClient - [Consumer clientId=createevent-45f40691-1392-43a5-9cce-4fec91cfd2c4-6, groupId=createevent-45f40691-1392-43a5-9cce-4fec91cfd2c4] Node 0 disconnected.
11:28:37.325 [Source Data Fetcher for Source: createevent -> Process -> Process -> Map -> event$i$1[1] -> Calc[2] -> (ConstraintEnforcer[3] -> Sink: event$3[3], ConstraintEnforcer[4] -> StreamRecordTimestampInserter[4] -> event$3$1[4]: Writer -> event$3$1[4]: Committer) (7/8)#0] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=createevent-45f40691-1392-43a5-9cce-4fec91cfd2c4-6, groupId=createevent-45f40691-1392-43a5-9cce-4fec91cfd2c4] Connection to node 0 (localhost/127.0.0.1:51724) could not be established. Broker may not be available.
11:28:37.326 [kafka-producer-network-thread | producer-8] INFO  org.apache.kafka.clients.NetworkClient - [Producer clientId=producer-8] Node 0 disconnected.
11:28:37.326 [kafka-producer-network-thread | producer-7] INFO  org.apache.kafka.clients.NetworkClient - [Producer clientId=producer-7] Node 0 disconnected.
11:28:37.326 [kafka-producer-network-thread | producer-8] WARN  org.apache.kafka.clients.NetworkClient - [Producer clientId=producer-8] Connection to node 0 (localhost/127.0.0.1:51724) could not be established. Broker may not be available.
11:28:37.326 [kafka-producer-network-thread | producer-7] WARN  org.apache.kafka.clients.NetworkClient - [Producer clientId=producer-7] Connection to node 0 (localhost/127.0.0.1:51724) could not be established. Broker may not be available.
11:28:37.327 [kafka-producer-network-thread | producer-6] INFO  org.apache.kafka.clients.NetworkClient - [Producer clientId=producer-6] Node 0 disconnected.
11:28:37.327 [kafka-producer-network-thread | producer-5] INFO  org.apache.kafka.clients.NetworkClient - [Producer clientId=producer-5] Node 0 disconnected.
11:28:37.327 [kafka-producer-network-thread | producer-6] WARN  org.apache.kafka.clients.NetworkClient - [Producer clientId=producer-6] Connection to node 0 (localhost/127.0.0.1:51724) could not be established. Broker may not be available.
11:28:37.327 [kafka-producer-network-thread | producer-5] WARN  org.apache.kafka.clients.NetworkClient - [Producer clientId=producer-5] Connection to node 0 (localhost/127.0.0.1:51724) could not be established. Broker may not be available.
11:28:37.374 [vert.x-kafka-consumer-thread-0] INFO  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-bcb673c0-fcf5-4124-98a8-1773e55d92e4-1, groupId=bcb673c0-fcf5-4124-98a8-1773e55d92e4] Node 0 disconnected.
11:28:37.374 [vert.x-kafka-consumer-thread-0] WARN  org.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-bcb673c0-fcf5-4124-98a8-1773e55d92e4-1, groupId=bcb673c0-fcf5-4124-98a8-1773e55d92e4] Connection to node 0 (localhost/127.0.0.1:51724) could not be established. Broker may not be available.
11:28:37.377 [kafka-producer-network-thread | producer-9] INFO  org.apache.kafka.clients.NetworkClient - [Producer clientId=producer-9] Node 0 disconnected.
11:28:37.377 [kafka-producer-network-thread | producer-3] INFO  org.apache.kafka.clients.NetworkClient - [Producer clientId=producer-3] Node 0 disconnected.
11:28:37.377 [kafka-producer-network-thread | producer-9] WARN  org.apache.kafka.clients.NetworkClient - [Producer clientId=producer-9] Connection to node 0 (localhost/127.0.0.1:51724) could not be established. Broker may not be available.
11:28:37.377 [kafka-producer-network-thread | producer-3] WARN  org.apache.kafka.clients.NetworkClient - [Producer clientId=producer-3] Connection to node 0 (localhost/127.0.0.1:51724) could not be established. Broker may not be available.
11:28:37.378 [kafka-producer-network-thread | producer-2] INFO  org.apache.kafka.clients.NetworkClient - [Producer clientId=producer-2] Node 0 disconnected.
11:28:37.378 [kafka-producer-network-thread | producer-2] WARN  org.apache.kafka.clients.NetworkClient - [Producer clientId=producer-2] Connection to node 0 (localhost/127.0.0.1:51724) could not be established. Broker may not be available.
11:28:37.400 [main] INFO  org.apache.zookeeper.ZooKeeper - Session: 0x1000730070e0000 closed
11:28:37.401 [main-EventThread] INFO  org.apache.zookeeper.ClientCnxn - EventThread shut down for session: 0x1000730070e0000
11:28:37.402 [main] INFO  kafka.zookeeper.ZooKeeperClient - [ZooKeeperClient Kafka server] Closed.
11:28:37.402 [main] INFO  kafka.server.ClientQuotaManager$ThrottledChannelReaper - [ThrottledChannelReaper-Fetch]: Shutting down
11:28:37.404 [ThrottledChannelReaper-Fetch] INFO  kafka.server.ClientQuotaManager$ThrottledChannelReaper - [ThrottledChannelReaper-Fetch]: Stopped
11:28:37.404 [main] INFO  kafka.server.ClientQuotaManager$ThrottledChannelReaper - [ThrottledChannelReaper-Fetch]: Shutdown completed
11:28:37.404 [main] INFO  kafka.server.ClientQuotaManager$ThrottledChannelReaper - [ThrottledChannelReaper-Produce]: Shutting down
11:28:37.404 [ThrottledChannelReaper-Produce] INFO  kafka.server.ClientQuotaManager$ThrottledChannelReaper - [ThrottledChannelReaper-Produce]: Stopped
11:28:37.404 [main] INFO  kafka.server.ClientQuotaManager$ThrottledChannelReaper - [ThrottledChannelReaper-Produce]: Shutdown completed
11:28:37.404 [main] INFO  kafka.server.ClientQuotaManager$ThrottledChannelReaper - [ThrottledChannelReaper-Request]: Shutting down
11:28:37.404 [ThrottledChannelReaper-Request] INFO  kafka.server.ClientQuotaManager$ThrottledChannelReaper - [ThrottledChannelReaper-Request]: Stopped
11:28:37.404 [main] INFO  kafka.server.ClientQuotaManager$ThrottledChannelReaper - [ThrottledChannelReaper-Request]: Shutdown completed
11:28:37.404 [main] INFO  kafka.server.ClientQuotaManager$ThrottledChannelReaper - [ThrottledChannelReaper-ControllerMutation]: Shutting down
11:28:37.404 [ThrottledChannelReaper-ControllerMutation] INFO  kafka.server.ClientQuotaManager$ThrottledChannelReaper - [ThrottledChannelReaper-ControllerMutation]: Stopped
11:28:37.404 [main] INFO  kafka.server.ClientQuotaManager$ThrottledChannelReaper - [ThrottledChannelReaper-ControllerMutation]: Shutdown completed
11:28:37.405 [main] INFO  kafka.network.SocketServer - [SocketServer listenerType=ZK_BROKER, nodeId=0] Shutting down socket server
11:28:37.418 [main] INFO  kafka.network.SocketServer - [SocketServer listenerType=ZK_BROKER, nodeId=0] Shutdown completed
11:28:37.419 [main] INFO  org.apache.kafka.common.metrics.Metrics - Metrics scheduler closed
11:28:37.419 [main] INFO  org.apache.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
11:28:37.419 [main] INFO  org.apache.kafka.common.metrics.Metrics - Metrics reporters closed
11:28:37.419 [main] INFO  kafka.server.BrokerTopicStats - Broker and topic stats closed
11:28:37.420 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - App info kafka.server for 0 unregistered
11:28:37.420 [main] INFO  kafka.server.KafkaServer - [KafkaServer id=0] shut down completed
11:28:37.425 [NIOServerCxnFactory.AcceptThread:/127.0.0.1:0] INFO  org.apache.zookeeper.server.NIOServerCnxnFactory - accept thread exitted run method
11:28:37.425 [NIOServerCxnFactory.SelectorThread-0] INFO  org.apache.zookeeper.server.NIOServerCnxnFactory - selector thread exitted run method
11:28:37.425 [ConnnectionExpirer] INFO  org.apache.zookeeper.server.NIOServerCnxnFactory - ConnnectionExpirerThread interrupted
11:28:37.425 [kafka-admin-client-thread | createevent-45f40691-1392-43a5-9cce-4fec91cfd2c4-enumerator-admin-client] INFO  org.apache.kafka.clients.NetworkClient - [AdminClient clientId=createevent-45f40691-1392-43a5-9cce-4fec91cfd2c4-enumerator-admin-client] Node 0 disconnected.
11:28:37.425 [kafka-admin-client-thread | createevent-45f40691-1392-43a5-9cce-4fec91cfd2c4-enumerator-admin-client] WARN  org.apache.kafka.clients.NetworkClient - [AdminClient clientId=createevent-45f40691-1392-43a5-9cce-4fec91cfd2c4-enumerator-admin-client] Connection to node 0 (localhost/127.0.0.1:51724) could not be established. Broker may not be available.
11:28:37.426 [NIOServerCxnFactory.SelectorThread-1] INFO  org.apache.zookeeper.server.NIOServerCnxnFactory - selector thread exitted run method
11:28:37.426 [main] INFO  org.apache.zookeeper.server.ZooKeeperServer - shutting down
11:28:37.426 [main] INFO  org.apache.zookeeper.server.RequestThrottler - Shutting down
11:28:37.426 [RequestThrottler] INFO  org.apache.zookeeper.server.RequestThrottler - Draining request throttler queue
11:28:37.426 [RequestThrottler] INFO  org.apache.zookeeper.server.RequestThrottler - RequestThrottler shutdown. Dropped 0 requests
11:28:37.426 [main] INFO  org.apache.zookeeper.server.SessionTrackerImpl - Shutting down
11:28:37.426 [main] INFO  org.apache.zookeeper.server.PrepRequestProcessor - Shutting down
11:28:37.427 [ProcessThread(sid:0 cport:51722):] INFO  org.apache.zookeeper.server.PrepRequestProcessor - PrepRequestProcessor exited loop!
11:28:37.427 [main] INFO  org.apache.zookeeper.server.SyncRequestProcessor - Shutting down
11:28:37.427 [SyncThread:0] INFO  org.apache.zookeeper.server.SyncRequestProcessor - SyncRequestProcessor exited!
11:28:37.427 [main] INFO  org.apache.zookeeper.server.FinalRequestProcessor - shutdown of request processor complete
11:28:37.456 [TaskExecutorLocalStateStoresManager shutdown hook] INFO  org.apache.flink.runtime.state.TaskExecutorLocalStateStoresManager - Shutting down TaskExecutorLocalStateStoresManager.
11:28:37.456 [TaskExecutorStateChangelogStoragesManager shutdown hook] INFO  org.apache.flink.runtime.state.TaskExecutorStateChangelogStoragesManager - Shutting down TaskExecutorStateChangelogStoragesManager.
11:28:37.457 [PermanentBlobCache shutdown hook] INFO  org.apache.flink.runtime.blob.PermanentBlobCache - Shutting down BLOB cache
11:28:37.458 [BlobServer shutdown hook] INFO  org.apache.flink.runtime.blob.BlobServer - Stopped BLOB server at 0.0.0.0:51732
11:28:37.458 [FileCache shutdown hook] INFO  org.apache.flink.runtime.filecache.FileCache - removed file cache directory /var/folders/w3/dxm786y50_b8p5tj7xktkngh0000gn/T/flink-dist-cache-2d1c916a-8c46-4782-ac19-fbc1c003d764
11:28:37.459 [TransientBlobCache shutdown hook] INFO  org.apache.flink.runtime.blob.TransientBlobCache - Shutting down BLOB cache
11:28:37.460 [FileChannelManagerImpl-io shutdown hook] INFO  org.apache.flink.runtime.io.disk.FileChannelManagerImpl - FileChannelManager removed spill file directory /var/folders/w3/dxm786y50_b8p5tj7xktkngh0000gn/T/flink-io-b23f7d97-3eae-47ce-80cd-a38fa1615b7b
11:28:37.461 [FileChannelManagerImpl-netty-shuffle shutdown hook] INFO  org.apache.flink.runtime.io.disk.FileChannelManagerImpl - FileChannelManager removed spill file directory /var/folders/w3/dxm786y50_b8p5tj7xktkngh0000gn/T/flink-netty-shuffle-950e8c85-a420-4787-9ba8-8f9ef1539d00

Process finished with exit code 0
