{"url":"https://events.bizzabo.com/468544/agenda/session/1211663","date":"September 26, 2023","time":"7:30 AM - 8:30 AM","title":"Breakfast","abstract":"N/A","location":"Hall 3","speakers":[],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1222575","date":"September 26, 2023","time":"7:30 AM - 6:00 PM","title":"Registration","abstract":"N/A","location":"Hall 1 Concourse","speakers":[],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1136821","date":"September 26, 2023 - September 27, 2023","time":"Sep 26, 11:45 PM - Sep 27, 1:15 AM","title":"Streaming into the Future: The Evolution and Impact of Data Streaming Platforms","abstract":"N/A","location":"San Jose Civic","speakers":[{"name":" Shaun Clowes ","title":" Chief Product Officer ","company":" Confluent "},{"name":" Joseph Foster ","title":" Cloud Computing Program Manager ","company":" NASA "},{"name":" Jay Kreps ","title":" Co-founder and CEO ","company":" Confluent  "},{"name":" Girish Rao ","title":" SVP, Platform Engineering & Ops ","company":"  Warner Bros. Discovery (WBD)  "},{"name":" Daniel Sternberg ","title":" Head of Data ","company":" Notion "}],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1217209","date":"September 26, 2023","time":"10:15 AM - 6:00 PM","title":"Expo","abstract":"N/A","location":"Expo Hall","speakers":[],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1136823","date":"September 26, 2023","time":"10:30 AM - 11:15 AM","title":"10 tips for enabling data discovery and governance in your organization","abstract":"Discovery is the first barrier to using data. As data platforms and systems scale so does the ability of stakeholders to create more and more data. More data means things are harder to find. Data products need to be cataloged on the go not only for discovery but also for governance purposes. Not only that, data can exist in many forms - reports, tables, files, streams, services, logs and may go through multiple hops of processing by multiple teams before it becomes a curated data product. Furthermore, a typical data organization will have a plethora of platform and infrastructure pieces - some open source, some cloud based and some custom. To build a robust discovery ecosystem, cataloging must happen continuously, at each hop and for every component in the organization. This becomes challenging without a central team overseeing the entire process.\n\nIn this presentation, I will talk about how we solved the problem of cataloging and discovery using Datahub as our discovery platform. I will cover the details of how we went about ingesting metadata from a plethora of infrastructure and platform components(such as Snowflake, Looker, Terraform, Airflow, Kinesis, custom declarative configs etc) that are involved in a typical data product lifecycle at Chime. I will also talk about the processes and design principles we used to make cataloging and data governance a part of our dna.","location":"Breakout Room 1 (Livestream)","speakers":[{"name":" Sherin Thomas ","title":" Staff Software Engineer ","company":" Sherin Thomas "}],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1136824","date":"September 26, 2023","time":"10:30 AM - 11:15 AM","title":"Analytics: The Final Data Frontier (or, Why Users Need Your Data and How Pinot Serves it to Them)","abstract":"Traditionally, analytics have served internal decision-makers‚Äîoften an exclusive group of people in high-status positions in the organization. Recently, initiatives like Data Mesh have recognized that pushing analytics products down to people at all levels of the org chart can make for a more responsive and competitive organization. But what about people outside the organization? It used to be that you wanted to see their data, but now they need to see yours. Users are the final frontier of analytics, and going forward, you may have less and less of a choice whether to expose analytics products to your customers themselves.\n\nThis requires a completely re-engineered approach to data infrastructure. Apache Pinot is a database built from the ground up to ingest streaming data from Apache Kafka and serve filtered, grouped, and aggregated results in tens of milliseconds rather than tens of seconds. Built at LinkedIn to expose the social network's data to users as game-changing application features, Pinot is now powering user-facing analytics in real-time, event-driven systems in many different businesses all over the world. Come to this talk to understand the forces that have given rise to this class of database, learn about Pinot's internals, and see some examples of it in action.","location":"Breakout Room 2","speakers":[{"name":" Tim Berglund ","title":" VP Developer Relations ","company":" StarTree "}],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1136825","date":"September 26, 2023","time":"10:30 AM - 11:15 AM","title":"Beyond a Lifetime of Data","abstract":"For most businesses, customer data has a shelf life in days, months, years. What about when the data needs to be collected today, so that you can use it decades or over a century later? The customer relationship with Social Security starts from when a Social Security Number is issued and lasts even after death to support qualified survivors. SSAs current critical data structures go back to the late 1970s, when records were king and contain only a snapshot of information at collection. In that time, the legislation around the critical data has evolved, but wide-spread dependencies restricted structure evolution. By shifting our processes to an Event Driven Architecture, one business line at a time, we hope to set the stage to revisit our critical data query models, while highlighting and correcting the data quality issues that inevitably built up over decades.","location":"Breakout Room 3","speakers":[{"name":" Vanessa Burckard ","title":" Enterprise Data Architect ","company":" Social Security Administration "}],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1136826","date":"September 26, 2023","time":"10:30 AM - 11:15 AM","title":"From üêõ to ü¶ã: Data Pipelines Evolution from Batch to Streaming","abstract":"Despite data streaming being in most companies‚Äô agenda, transitioning out of consolidated batch systems is not as simple as flipping a switch: new technologies, processes and coding frameworks need to be assessed and then adopted which can make the evolution a long and painful process. But, what if we could keep the same framework? This session explores how Apache Flink can narrow the gap between batch and streaming by keeping the same data pipelines definition while the underlying technology evolves.\n\nWe‚Äôll start the journey with a typical batch system, based on a relational database, and then showcase how to evolve it to streaming using Apache Flink and Apache Kafka with minimal changes on the data pipeline definition. We‚Äôll cover query based connectors, mimicking the batch behavior, and then move to more advanced change data capture solutions with Debezium. Finally we‚Äôll touch on critical topics like data validation and late arrival of events and expose strategies on how to minimize related risks.\n\nIf you‚Äôre thinking about migrating from batch to streaming, but are afraid of any disruption the process may cause in your organization, this session is for you!","location":"Breakout Room 5","speakers":[{"name":" Francesco Tisiot ","title":" Senior Developer Advocate ","company":" Aiven "}],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1136836","date":"September 26, 2023","time":"10:30 AM - 11:15 AM","title":"Get More from your Data: Accelerate Time-to-Value and Reduce TCO with Confluent Cloud on AWS","abstract":"In this talk, we will explore how Confluent and Amazon Web Services (AWS) work together to help you in the journey of data modernization and innovation.\nWe guide you through the migration journey to Confluent Cloud on AWS, delving into advanced features and capabilities for streamlined migration and business continuity. Gain insights from customer success stories, learn cloud modernization strategies, patterns, and best practices, and AWS resources to kickstart your initiatives.\nExplore modern app development on Confluent Cloud on AWS, alongside strategic ISV partners like MongoDB, and unlock the full potential of real-time streaming.","location":"Breakout Room 7","speakers":[{"name":" Weifan Liang ","title":" Sr. Partner Solutions Architect ","company":" AWS "},{"name":" Joseph Morais ","title":" Principal, Partner Solutions Engineer ","company":" Confluent "}],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1136827","date":"September 26, 2023","time":"10:30 AM - 11:15 AM","title":"The Wonderful World of Apache Kafka","abstract":"Apache Kafka is the core of an amazing ecosystem of tools and frameworks that enable us to get more value from our data. Let's take a tour through this wonderful world and see what we can learn.\nIn this session, we'll have a gentle introduction to Apache Kafka, and then a survey of some of the more popular components in the Kafka ecosystem. We'll look at the Kafka Producer and Consumer libraries, Kafka Connect, Kafka Streams, the Confluent Schema Registry, and more. You will leave loaded with ideas of ways to put Kafka to use in your organization, and a list of resources to help you on your journey.","location":"Breakout Room 6","speakers":[{"name":" Dave Klein ","title":" Developer Advocate ","company":" Tabular "}],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1136828","date":"September 26, 2023","time":"10:30 AM - 11:15 AM","title":"Topic (namespace) as a Service","abstract":"Running shared, centrally managed Kafka cluster(s) used by many different teams has many advantages for a company over the setup in which teams maintain their clusters.\n\nEven when using a SaaS solution, sharing clusters is much more cost-effective. But what about data governance in a scenario like this? Teams need to have ownership over specific topics and the ability to grant access to other teams and request access to other teams' topics. The Gitops approach might be a good enough solution if the number of teams is relatively small, but as that number grows, you want to automate these processes to keep your Kafka users happy.\n\nIn this talk, we will look deeper into what it takes to provide such a platform.","location":"Breakout Room 4","speakers":[{"name":" Igor Buzatoviƒá ","title":" Principal Software Engineer ","company":" Porsche Digital Croatia "}],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1180209","date":"September 26, 2023","time":"11:00 AM - 12:30 PM","title":"Build A Gaming Analytics Pipeline: An AWS GameDay","abstract":"Join us for a Confluent and Amazon Web Services (AWS) GameDay, a gamified workshop where you will build an event-driven, data-streaming pipeline to detect cheating, identify toxic chat, ban players, and update matchmaking ranking in real time. Participants will get hands-on experience with services such as AWS Lambda, Amazon GameLift, Amazon DynamoDB, and Confluent Cloud. Please bring a laptop and prepare yourself to learn, have fun, win swag, and herd Unicorns. AWS and Confluent Cloud accounts will be provided for the session.","location":"Meeting Room 211A","speakers":[],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1136855","date":"September 26, 2023","time":"11:30 AM - 12:15 PM","title":"Analyzing Streaming Data with Apache Druid","abstract":"When real-time matters, you need data in motion, you need a data pipeline, and you need subsecond, high-concurrency analytics that combine what‚Äôs happening now with what happened in the past. You need Apache KFD: Kafka, Flink, and Druid!\nJoin us for this meetup hub session on building real-time analytics for streaming data. We will demonstrate how to build analytics applications using Apache Kafka, Apache Flink, and Apache Druid, the open-source database for speed, scale, and streaming data.\nYou‚Äôll learn how KFD makes it easy to:\nIngest streaming Kafka data natively at scale\nEnhance, enrich and align data from multiple topics\nAutomatically detect data types and ingest into scalable tables\nExecute SQL queries combining real-time and historical data\nCreate interactive dashboards to visualize event data\nEmbed the dashboards into a web application","location":"Meetup Hub (Expo Hall)","speakers":[{"name":" Darin Briskman ","title":" Director of Technology ","company":" Imply "}],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1136845","date":"September 26, 2023","time":"11:30 AM - 12:15 PM","title":"Building a Dynamic Rules Engine with Kafka Streams","abstract":"The benefit of real-time data can be measured by how frequently the data in question changes, nowhere is this more apparent than threat detection. Responding to an ever changing landscape of attacks and exploits requires a system that can not only handle the scale and dynamic nature of the data but also a dynamically changing set of detection rules. We developed Confluent SIGMA, an open source project built on Kafka Streams for the open SIGMA DSL, to handle real-time rule additions and modifications. In this talk we will cover:\n* The architecture of our Kafka Streams layer that makes it possible to use external data feeds as rule input\n* How we handle dynamic criteria for joins and filters\n* Best practices for writing dynamic rule engines in Kafka Streams\n* Upcoming improvements to Kafka Streams to support versioned rules\nAlthough Confluent SIGMA focuses on cyber threat detection this same pattern can also be applied to any DSL (domain specific language) that would benefit from real-time stream processing. After attending you will have the framework to drive dynamic rules through Kafka Streams for any use case that might require it.","location":"Breakout Room 3","speakers":[{"name":" Will LaForest ","title":" Field CTO ","company":" Confluent "},{"name":" Michael Peacock ","title":" Federal Solutions Engineer ","company":" Confluent "}],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1136831","date":"September 26, 2023","time":"11:30 AM - 12:15 PM","title":"Evolution of Real-time User Engagement Event Consumption at Pinterest","abstract":"We will discuss how we at Pinterest transformed real time user engagement event consumption.\n\nEvery day, we log hundreds of billions of user engagement events across different domains to a few common Kafka topics which are consumed by hundreds of real time applications. These real time applications were built upon diverged frameworks (e.g. Spark Streaming, Storm, Flink, and internally developed frameworks using Kafka Consumer API) without standardization on processing logics. It led to repeated processing of similar logic, multiple codebases to maintain, low data quality, and inconsistency with offline datasets. These negatively impact scalability, reliability, efficiency and data accuracy of these applications and eventually affect the real-time content recommendation quality and user experience.\n\nTo address these challenges, we unified the way of consuming events in our real time applications by consolidating the compute engines to Flink, splitting events in those common topics by engagement types, generating cleansed events with standardized processing to align on business concepts. Throughout these efforts, we achieved multi-million dollar infrastructure savings and double-digit engagement gain after applications adopted those cleansed events.\n\nMoving forward, we are implementing frameworks for better tracking and governing the Kafka events and real time use cases.","location":"Breakout Room 1 (Livestream)","speakers":[{"name":" Lu Liu ","title":" Software Engineer ","company":" Pinterest "},{"name":" Heng Zhang ","title":" Software Engineer ","company":" Pinterest "}],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1136832","date":"September 26, 2023","time":"11:30 AM - 12:15 PM","title":"From 0 to 300 mph Towards the Promised Land","abstract":"As the volume and velocity of data in motion continues to increase, the challenge becomes being able to move your team at the same pace. After implementing technology to capture data from various high-frequency sources such as financial applications, sensors and IoT devices, teams often find themselves struggling to build and roll out best practices quickly. They inevitably fall behind on the performance and scalability demands of the business.\n\nIn this session, Mike Rosam and Tun Shwe will share their experiences of building data teams at McLaren and fast growth startups. They will take you on a journey of how they navigated their way from the old batch world to the new streaming world, where real-time decisions are enabled by stream processing and are within the reach of every data team.\n\nAttendees will learn the common sources of friction against streaming technologies in organisations, be able to identify the pain points that lead to streaming adoption, prioritisation based on data maturity and how to gain a competitive edge by focusing on culture and value. So, buckle up and join us on this high-speed adventure to the promised land!","location":"Breakout Room 2","speakers":[{"name":" Michael Rosam ","title":" Co-founder & CEO ","company":" Quix "},{"name":" Tun Shwe ","title":" VP of Data ","company":" Quix "}],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1136835","date":"September 26, 2023","time":"11:30 AM - 12:15 PM","title":"Isolating Streaming Ingest and Queries Using RocksDB","abstract":"In a real-time analytics architecture, streaming data ingestion, from a source like Kafka, and query serving run on the same compute unit, so that queries can reflect newly ingested data. These two distinct competing functions invariably contend for the available compute resources, which makes it difficult to handle situations where there are unexpected bursts of either streaming ingestion or queries that can slow down the system. We will examine common approaches to the problem of compute contention, such as scaling, replication, and querying from shared storage, and discuss their tradeoffs and how they remain incomplete solutions.\n\nIn this talk, we will present a real-time analytics architecture we implemented in the Rockset database, based on RocksDB, that effectively isolates streaming data ingestion from query serving. RocksDB is a popular log-structured merge-tree storage engine that writes to an in-memory memtable and periodically flushes to disk.\n\nCore to our architecture is the separation of compute and storage. This allows multiple RocksDB instances to query from the same shared storage. We use cloud object storage to ensure durability and use SSD as a shared hot storage tier for low-latency reads. On the compute side, we designed our query processing engine to be completely separate from all the modules that perform data ingestion.\n\nFor fresh data to be available to multiple compute units, it is essential that the in-memory state of the ingester's RocksDB memtable be replicated to other RocksDB instances. We built a RocksDB memtable replicator that propagates changes to remote instances in single-digit milliseconds. This architecture enables compute isolation so that real-time streaming ingestion does not interfere with queries, while still allowing the most recent data to be queried.","location":"Breakout Room 4","speakers":[{"name":" Nathan Bronson ","title":" Principal Architect ","company":" Rockset "}],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1136822","date":"September 26, 2023","time":"11:30 AM - 11:50 AM","title":"The Top 5 Mistakes Engineers Make When Implementing Apache Flink","abstract":"Is your team looking to bring the power of full, end-to-end stream processing with Apache Flink to your organization but are concerned about the time, resources or skills required? In this talk, the PMC Chair for Apache Flink, Robert Metzger, will go through some of the top mistakes he has seen engineers make during various stages of implementation and maintenance. ¬†Sharon Xie, Decodable Founding Engineer, will reveal the best practices on how to avoid these in your environment. ¬†If you have any plans on implementing Apache Flink, then this is a session you do not want to miss.","location":"Breakout Room 7","speakers":[{"name":" Sharon Xie ","title":" Founding Engineer ","company":" Decodable "},{"name":" Robert Metzger ","title":" Software Engineer ","company":" Decodable "}],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1136833","date":"September 26, 2023","time":"11:30 AM - 12:15 PM","title":"Unlocking the Power of Apache Flink: An Introduction in 4 Acts","abstract":"Today's consumers have come to expect timely and accurate information from the companies they do business with. Whether it's being alerted that someone just used your credit card to rent a car in Prague, or checking on the balance of your mobile data plan, it's not good enough to learn about yesterday's information today. We all expect the companies managing our data to be able to provide fully up-to-the-moment reporting.\n\nApache Flink is a battle-hardened stream processor widely used for demanding applications like these. Its performance and robustness are the result of a handful of core design principles: a shared-nothing architecture with local state, event-time processing, and state snapshots (for recovery). During this talk, we'll bring these principles to life with real-world examples and demos.","location":"Breakout Room 6","speakers":[{"name":" David Anderson ","title":" Software Practice Lead ","company":" Confluent "}],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1136907","date":"September 26, 2023","time":"11:30 AM - 12:15 PM","title":"Workflow Engines & Event Streaming Brokers - Can They Work Together?","abstract":"Workflow engines and event streaming brokers offer very different solutions to the same requirement - an optimal implementation of microservices communication.\n\nAt Wix, we have a good experience with event-driven architecture for our 2500 microservices using Apache Kafka. Apache Kafka provides:\n\n* Support for very high throughput\n\n* Fault tolerance\n\n* Very loose coupling\n\n* Huge connectors eco-system\n\nTemporal workflow orchestration has interesting features:\n\n* Support for long running tasks\n\n* Business flows visual tracking\n\n* Easy to follow imperative style programming\n\nIn this talk we will learn about the tradeoffs between the two technologies and how to implement various use cases in each architecture, including those that need a little more work.","location":"Breakout Room 5","speakers":[{"name":" Natan Silnitsky ","title":" Backend Infrastructure tech lead ","company":" Wix "}],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1136829","date":"September 26, 2023","time":"12:00 PM - 12:20 PM","title":"Unifying Stream Processing with a Fast Data Store","abstract":"In this session, we will describe an architecture that addresses simplicity and performance in stream processing deployments, while also reducing cost. This architecture aims for fewer moving parts, fewer clusters and servers to manage, fewer network hops, higher throughput, and lower latency, as well as less custom code.\nWe will demonstrate the combination of stream processing and fast data store in a unified real-time data platform. We will also discuss Streaming SQL features, Python and Java support, and best practices.","location":"Breakout Room 7","speakers":[{"name":" Fawaz Ghali ","title":" Principal Data Science Architect & Head of Developer Relations ","company":" Hazelcast "},{"name":" Michael Goldverg ","title":" Managing Director, Distinguished Engineer ","company":" The Bank of New York Mellon "}],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1136838","date":"September 26, 2023","time":"12:30 PM - 1:30 PM","title":"Lunch","abstract":"Join us in Hall 3 to grab lunch, meet new friends, and relax before the afternoon sessions.","location":"Hall 3","speakers":[],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1222160","date":"September 26, 2023","time":"12:30 PM - 2:00 PM","title":"Trailblazing Women in Tech: Navigating Careers, Empowering Leadership, and Inspiring Futures","abstract":"Confluent‚Äôs DE&I team will host a Women in Tech Luncheon and Panel Discussion at Current 2023. This event will highlight women who have used their leadership to open doors, crash ceilings, and pave the way for other women. We aim to leave the audience feeling proud and inspired to be a successful woman and leader in the male-dominated industry of Technology.","location":"Grand Ballroom 220A","speakers":[{"name":" Mona Chadha ","title":" Director, Infrastructure Partnerships ","company":" AWS "},{"name":" Joey Monique Fowler ","title":" Senior Director, Technical Services ","company":" Denny's Corporation "},{"name":" Denise Hemmert ","title":" VP, Enterprise Enablement Services ","company":" Cardinal Health "},{"name":" Shruti Modi ","title":" Director, Data Platform, Penske Transportation Solutions ","company":" Penske "},{"name":" Sharmey Shah ","title":" AVP, Northeast Enterprise ","company":" Confluent "},{"name":" Stephanie Buscemi ","title":" CMO ","company":" Confluent "}],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1136840","date":"September 26, 2023","time":"1:30 PM - 2:15 PM","title":"5-minute Practical Streaming Techniques that can Save You Millions","abstract":"Companies are looking for ways to reduce streaming infrastructure costs in the current macroeconomic environment. However, this is a difficult task for two reasons. First, cutting costs without sacrificing latency or correctness requires deep knowledge of engine implementation details and a keen eye to identify opportunities. Second, optimization techniques are less accessible when working with high-level language abstraction, such as SQL. These techniques often involve engine query planning, requiring even deeper expertise. Many Data Engineers and Data Scientists prefer not to deal with Intermediate Representations (IR) and optimization rules. They also may not care too deeply about the details of applying streaming watermarks to reduce the runtime complexity for Point-In-Time-Correct join queries.\n\nIn this talk, I will share some simple optimization techniques you can apply with streaming SQL in just a few minutes that can cut costs by 10x or even 100x. Then, we‚Äôll gradually dive deeper into some novel optimization techniques that can be applied across your distributed storage and compute stacks.\n\nBy the end of this talk, if you are a Data Engineer or a Data Scientist who is looking to build real-time streaming workloads but have concerns about cost, I hope you‚Äôll be able to walk away with some tricks so you can check that box on your product ROI OKR :) If you are a platform engineer, I hope you will learn how to apply optimization abstractions across various compute and storage engines in your platform.","location":"Breakout Room 5","speakers":[{"name":" Zhenzhong Xu ","title":" Co-founder & CTO ","company":" Claypot AI, Inc. "}],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1136850","date":"September 26, 2023","time":"1:30 PM - 2:15 PM","title":"Autoscaling Confluent Cloud: Should We? How Would We?","abstract":"Although cloud-based, managed Kafka offerings abstract away most administrative responsibilities, a few admin-related concerns remain‚Äì‚Äìlike cluster scaling. When is scaling your cloud-based Kafka appropriate? And how should you set it up to auto-scale?\n\nGone are the days of over-provisioning resources to meet expected demand. Technologies like kubernetes make it relatively simple to implement strategies around both horizontal and vertical scaling. Cloud providers give users the ability to track their resource utilization and set up autoscaling groups and policies. Cloud administrators use these tools (and others) to guarantee their applications can handle the demands placed on them. With Kafka being a central pillar of our cloud-native data pipelines it requires administrators to determine if, when and how to scale Kafka as their workloads ebb and flow.\n\nIn this session, we‚Äôll explore the topic of auto-scaling by implementing a strategy for Confluent Cloud resources. We‚Äôll first discuss common use cases that dictate a need to create a scaling strategy for Confluent Cloud and introduce the approaches best suited for each use case. With a nod to both where we came from and where we are going, we will discuss the architecture of Confluent Cloud and how it affects the way we scale Kafka. Attendees will learn how to deal with ephemeral workloads, what to monitor for when creating an auto-scaling policy, and the ‚Äúgotchas‚Äù of auto-scaling in Confluent Cloud. We will also discuss best practices for scaling Kafka clients, because Kafka is only as scalable as the client applications that connect to it.\n\nWe will dive into code that examines these approaches and by the end of the session, you‚Äôll have the tools needed to design and implement your own scaling strategy for your Confluent Cloud workloads.","location":"Breakout Room 6","speakers":[{"name":" Amanda Gilbert ","title":" Staff Solutions Engineer ","company":" Confluent "}],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1180214","date":"September 26, 2023","time":"1:30 PM - 3:15 PM","title":"Cluster Linking on Confluent Cloud (Lecture & Lab)","abstract":"Cluster Linking makes it easy to build multi-datacenter, multi-region, and hybrid cloud deployments by enabling you to directly connect clusters and mirror topics from one cluster to another.\nThis session will be broken into two parts: Lecture & Lab with a 15 min break in between\nWithin the lecture, we will discuss what Confluent cluster linking is, how it is set up, and provide examples of the most common use cases for cluster linking.\nWithin the lecture, we will discuss what Confluent cluster linking is, how it is set up, and provide examples of the most common use cases for cluster linking.\nFollowing the lecture will be a demo and hands-on* lab, where we will be creating a cluster link and updating the configuration using the recommended best practices. Finally, we will also learn how to migrate Kafka clients from the source to the destination cluster.\n* Bring your own laptop to work on hands-on lab or just enjoy the demo of the hands-on lab.","location":"Meeting Room 212","speakers":[{"name":" Borja Hernandez Crespo ","title":" Staff Technical Trainer ","company":" Confluent "}],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1136834","date":"September 26, 2023","time":"1:30 PM - 2:15 PM","title":"Getting Data In and Out of Flink - Understanding Flink and Its Connector Ecosystem","abstract":"Apache Flink is a powerful open-source stream processing framework that enables real-time data processing at scale. One of the key features of Flink is its rich ecosystem of connectors that allow users to easily integrate with a wide range of data sources and sinks. However, working with connectors can be challenging, especially for users who are new to Flink or stream processing.\n\nThis talk aims to help users better understand Flink connectors, the Flink connector ecosystem, and their importance in building scalable and robust data processing pipelines. It will cover topics such as:\n\n* An introduction to Flink connectors and their role in stream processing\n\n* A deep dive into the different Flink connector APIs, including the Unified Source and Sink API, SourceReaderBase and the Async Sink API.\n\n* The benefits of using unified batch and streaming APIs in Flink\n\nBy the end of this talk, attendees will have a solid understanding of Flink connectors, the connector interface, and be better equipped to build efficient and reliable data processing pipelines with Flink.","location":"Breakout Room 1 (Livestream)","speakers":[{"name":" Martijn Visser ","title":" Senior Product Manager ","company":" Confluent "}],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1136841","date":"September 26, 2023","time":"1:30 PM - 2:15 PM","title":"Learnings of Running Kafka Tiered Storage at Scale","abstract":"KIP-405 introduced tiered storage in Apache Kafka. It introduces the separation of compute and storage in brokers that improves the scalability, efficiency, and elasticity of Kafka clusters. We implemented this feature and have been running it in production for several months in different tiers of clusters at Uber.\n\nWe will talk about the following:\n\n- The principles followed in building the feature.\n\n- The journey of deploying and running it in our production clusters with different workloads.\n\n- The learnings from running it in production at a large scale, that led to a few interesting features extended from KIP-405.\n\n- The issues encountered and how we have fixed and mitigated them.","location":"Breakout Room 3","speakers":[{"name":" Satish Duggana ","title":" Sr Staff Engineer ","company":" Uber "},{"name":" Abhijeet Kumar ","title":" Staff Software Engineer ","company":" Uber "}],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1136842","date":"September 26, 2023","time":"1:30 PM - 2:15 PM","title":"Need for Speed: Machine Learning in the Era of Real-Time","abstract":"The world is impatient! Everyone wants to be aware of everything everywhere, all at once. Growing demand for quick decision-making has propelled the fascinating world of real-time machine learning (RTML) into the spotlight.\n\nOver the years, brilliant minds developed and refined a number of fantastic ML techniques. Yet, the impatient users are boohooing: we want our data now (read: low latency), and we like our data fresh (read: dynamic data adaptability), and we also want it to be cheap (read: utilize resources efficiently).\n\nIn this talk, we'll explore the evolution of solutions to these challenges and how ML systems are advancing toward online inference and continual learning.\n\nBy the end of our talk, attendees will leave with an understanding of the latest RTML techniques and the essential factors to consider when designing and implementing real-time machine-learning solutions.","location":"Breakout Room 2","speakers":[{"name":" Oli Makhasoeva ","title":" Director of Developer Relations and Operations ","company":" Bytewax "}],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1136908","date":"September 26, 2023","time":"1:30 PM - 2:15 PM","title":"Ready Player One: Unforgettable Gaming Experiences powered by Confluent and AWS","abstract":"Join Amazon Web Services (AWS) and Confluent to discover how continuous streaming pipelines process in-game data to personalize player experiences, drive in-game advertising, cross-promotions, and prevent gameplay issues. We will show how data from the game client is used in real-time to detect cheaters. And we will dive into the solution architecture of Confluent‚Äôs native integrations with AWS Services, for example AWS Lambda for serverless computing. We will also unveil our latest joint innovations aimed at transforming the customer experience.","location":"Meetup Hub (Expo Hall)","speakers":[{"name":" Weifan Liang ","title":" Sr. Partner Solutions Architect ","company":" AWS "},{"name":" Joseph Morais ","title":" Principal, Partner Solutions Engineer ","company":" Confluent "}],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1136844","date":"September 26, 2023","time":"1:30 PM - 2:15 PM","title":"Streaming Solutions Showdown","abstract":"Stream processing feels like the obvious (and right) choice, but perhaps less obvious is which stream processing technology should be used.\nCome along as expert streaming processing panelists compare and contrast some of the most popular stream processing technologies‚Äì‚ÄìApache Kafka Streams, Apache Flink, Apache Spark, and Apache Pulsar. Each panelist will have a chance to introduce their respective technology before we into a group discussion to answer questions such as:\nWhat are the most important features of each technology?\nWhere do each of these technologies excel, and which use cases are they not so well-suited for?\nWhat can users expect on the roadmap for your technology?\nBy the end of the panel, you‚Äôll be able to make a more informed decision choosing a streaming technology for your next project!","location":"Breakout Room 4","speakers":[{"name":" Danica Fine ","title":" Senior Developer Advocate ","company":" Confluent "},{"name":" Holden Karau ","title":" Open Source Engineer ","company":" Netflix "},{"name":" Matteo Merli ","title":" CTO ","company":" StreamNative "},{"name":" Sophie Blee-Goldman ","title":" Founding Engineer ","company":" Responsive "},{"name":" Tzu-Li (Gordon) Tai ","title":" Staff Software Engineer ","company":" Confluent "}],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1136852","date":"September 26, 2023","time":"1:30 PM - 2:15 PM","title":"Unlocking Real-Time Data Insights: Leveraging Confluent Cloud on Azure for Streamlined and Scalable Data Pipelines","abstract":"Traditional data pipelines face scalability and cost challenges due to monolithic design and batch processing. They assume all data must be stored in one location, leading to time-consuming, expensive, and error-prone processes.\nConfluent Cloud on Azure provides a fully managed cloud-native data streaming platform that simplifies how you build real-time data flows with data streaming pipelines. Join our session to discover how to leverage data pipelines to capture changes to data in real-time, enrich them instantly, and send them to downstream systems.\nGain insights into data streaming use cases, including database pipelines and data warehouse pipelines. Learn how to deliver trustworthy, high-quality self-service access to data so developers can quickly innovate and bring new products to market faster.\nHighlights:\n- Use streaming data pipelines to gain real-time access to data on Azure instead of processing data in batches with extract, transform, and load (ETL) pipelines.\n- Analyze data, ensure downstream compatibility, and make data instantly usable wherever needed.\n- Simplify data flow development and iteration so you can boost developer productivity.\n- Use Confluent Cloud with Azure Cosmos DB, Azure Synapse, and Azure Functions.","location":"Breakout Room 7","speakers":[{"name":" Jacob Bozorov ","title":" Sr. Cloud Solution Architect ","company":" Microsoft "}],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1136848","date":"September 26, 2023","time":"2:30 PM - 3:15 PM","title":"4 Patterns to Jumpstart your Event-Driven Architecture Journey","abstract":"The shift from monolithic applications to microservices is anything but easy. Since services usually don't operate in isolation, it's vital to implement proper communication models among them. A crucial aspect in this regard is to avoid tight coupling and numerous point-to-point connections between any two services. One effective approach is to build upon messaging infrastructure as a decoupling element and employ an event-driven application architecture.\n\nDuring this session, we explore selected event-driven architecture patterns commonly found in the field: the claim-check pattern, the content enricher pattern, the message translator pattern, and the outbox pattern. For each of the four patterns, we look into a live demo scenario based on Apache Kafka and discuss some variations and trade-offs regarding the chosen implementation.\n\nYou will walk away with a solid understanding of how the discussed event-driven architecture patterns help you with building robust and decoupled service-to-service communication and how to apply them in your next Apache Kafka-based project.","location":"Breakout Room 5","speakers":[{"name":" Hans-Peter Grahsl ","title":" Developer Advocate ","company":" Red Hat "}],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1136849","date":"September 26, 2023","time":"2:30 PM - 3:15 PM","title":"Anomaly Detection on Time Series Data Using Apache Flink","abstract":"The emerging need to be cloud native caused a fast adoption of applications to be redesigned. However, many are managing the cloud native deployments without taking any precautions against the intrusions in network security. Therefore, the need to detect anomalies of the network activity time series data in real-time is crucial yet still nontrivial.\n\nApache Flink is a framework and distributed processing engine for stateful computations over unbounded and bounded data streams. The ability of Apache Flink to process continuous data streams in a stateful manner makes it a perfect match for analysing time series data where the repeated observations of data points are continuously produced.\n\nIn this talk, we will walk through the steps to implement a real time anomaly detection system on time series data using Apache Flink. We will implement and compare several algorithms, Exponentially Weighted Moving Average (EWMA) and Probabilistic EWMA (PEWMA), from an academic paper - PROBABILISTIC REASONING FOR STREAMING ANOMALY DETECTION.","location":"Breakout Room 1 (Livestream)","speakers":[{"name":" Ali Zeybek ","title":" Team Lead, Solutions Architect ","company":" Ververica "}],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1136851","date":"September 26, 2023","time":"2:30 PM - 3:15 PM","title":"How We Built Nucleus: Community Brands' Analytics Platform","abstract":"Community Brands is a leading technology company for associations, member-based organizations, nonprofits and schools (K-12). We rely on the Nucleus Analytics platform to provide customers an interactive, unified view of business‚Äô most relevant and time-sensitive data.\n\nIn this talk, we describe how the Nucleus engineering team built a real time, user-facing analytics app with the following requirements:\n\n- Data latency of under two seconds for real-time data\n\n- Sub-second query latency for user-facing analytics\n\n- A fully-managed cloud architecture for operational simplicity\n\n- A fully mutable database capable of handling updates from transactional systems\n\nUsing Confluent‚Äôs cloud-native service for Apache Kafka, Apache Flink, Rockset, and transactional databases in our architecture, we can now quickly and easily serve real-time analytics to our customers via change data capture (CDC) streams. Since switching to Rockset, our operational overhead has decreased by an order of magnitude. In this talk, we will share how we designed and architected our real-time analytics platform, how we use real-time CDC, and the principles used to build new data products with greater speed and agility.","location":"Breakout Room 2","speakers":[{"name":" Curt Buechter ","title":" Product Development Manager ","company":" Community Brands "}],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1165883","date":"September 26, 2023","time":"2:30 PM - 3:30 PM","title":"Resilient Kafka: How DNS Traffic Management and Client Wrappers Ensure Availability","abstract":"Kafka clients sometimes have problems producing messages because of an incident or maintenance. In most cases these issues are short lived, but clients get paged and while long incidents are rare, they are memorable. In this talk, we'll run through our Kafka infrastructure at Shopify and how clients connect to it. Next, we'll describe our solution for performing failovers using DNS. Afterwards, we'll look at some real world scenarios where this system saved us from major outages. In conclusion, Kafka clusters are more resilient to infrastructure hiccups with a DNS traffic manager and opinionated client wrappers.","location":"Breakout Room 4","speakers":[{"name":" Vanessa Vuibert ","title":" Staff Production Engineer ","company":" Shopify "}],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1136843","date":"September 26, 2023","time":"2:30 PM - 3:15 PM","title":"Save Money by Uncovering Kafka‚Äôs Hidden Cloud Costs","abstract":"These days, optimizing costs and increasing efficiencies are on everyone‚Äôs mind, so this might be the first time in many years that you‚Äôve had to rationalize and justify your cloud costs. Your Kafka spend is likely no exception.\n\nBut figuring out how much your Cloud-based Kafka costs can be challenging. Your cloud provider doesn‚Äôt give you a bill that says, ‚Äúhere is how much your Kafka service costs.‚Äù You need to tag all your machines, examine network utilization, and track down all the teams that are using your service.\n\nIn this session, we will dive into Kafka‚Äôs network, storage, compute costs, and more to show you how to calculate and anticipate the bill for your Kafka deployment. And to take it a step further, we‚Äôll explore what Confluent has done to reduce Kafka cloud costs for ourselves and our customers.\n\nBy the end of the session, you‚Äôll have the knowledge you need to be more efficient in your Kafka cloud usage, justify your cloud-based Kafka deployment, and ultimately save you and your company money.","location":"Breakout Room 6","speakers":[{"name":" Addison Huddy ","title":" Senior Director of Product Management ","company":" Confluent "}],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1136839","date":"September 26, 2023","time":"Sep 26, 2:30 PM - Sep 27, 2:50 PM","title":"Stream Processing Solution for the Enterprise","abstract":"Stream processing technologies have been around for more than a decade. Nowadays, Apache Flink has become the de facto standard for stream processing and is being used in many enterprises. In this talk, we summarize our five-year experience in supporting enterprises to implement, deploy, and operate their stream processing solutions based on Apache Flink.\nWe will cover topics such as Flink application lifecycle management, Flink SQL development, multi-tenancy, security, cost optimization, business continuity, customer support, deployment options, and how they are supported in our product Ververica Platform. Finally, we will conclude by discussing the outlook for the future.","location":"Breakout Room 7","speakers":[{"name":" Jun Qin ","title":" Head of Solutions Architecture ","company":" Ververica "}],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1136853","date":"September 26, 2023","time":"2:30 PM - 3:30 PM","title":"The Nuts and Bolts of Kafka Streams---An Architectural Deep Dive","abstract":"Writing a Kafka Streams application is only the first step for your event driven applications. Eventually, you will deploy and tune your applications with different goals in mind (throughput, latency, robustness, high availability, you name it). To run and configure your applications efficiently, it is paramount to understand the internal architecture, and the dependencies and interactions of the different internal components. Otherwise, you end up with a trial-and-error approach that is cumbersome, potentially frustrating, and for sure not fun.\n\nIn this talk, we will explore the internal architecture of Kafka Streams to set you up for successfully running and tuning your applications. -- What does the internal threading model look like? How are partitions assigned and mapped to tasks? Why are there multiple internal consumers? What are the most important/interesting configurations and how do they interact with each other? -- Those are just a few questions we will answer in this session, enabling you to run and tune your applications in record time!","location":"Breakout Room 3","speakers":[{"name":" Matthias Sax ","title":" Software Engineer ","company":" Confluent "}],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1211664","date":"September 26, 2023","time":"2:30 PM - 2:45 PM","title":"Trace All the Things in Under 10 Minutes","abstract":"The distributed tracing ecosystem is fragmented across agents, data formats, and backends. OpenTelemetry, a CNCF project, provides vendor-agnostic SDKs and tools for ingesting, transforming, and sending data to any observability back-end.\nBeginning in the shadows of a ‚Äúdark system‚Äù, we‚Äôll use OpenTelemetry and Jaeger to shine a bit of light. Through a live demo, you‚Äôll see the appeal of OpenTelemetry and how to effectively leverage it.\nYou will leave understanding the key concepts of distributed tracing as well as how you can integrate OpenTelemetry into your systems to reap the benefits of observability.","location":"Meetup Hub (Expo Hall)","speakers":[{"name":" Matt Schroeder ","title":" Director of Technology, Real-Time Data ","company":" Improving "}],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1136863","date":"September 26, 2023","time":"3:00 PM - 3:15 PM","title":"6 Lessons Learned While Migrating From SQS to Kafka","abstract":"Last year, our team designed and implemented a new event-driven pipeline to replace our primary event ingestion flow that was previously based on AWS SQS. While the legacy flow did the trick for the most part, it occasionally behaved in unexpected ways that frustrated the team and worse, could pose a risk to our SLAs. During the process of designing, building, deploying and monitoring the new Kafka-based pipeline, we learned several valuable lessons. Here‚Äôs a glimpse of what we discovered:\n1. Designing the message key is intertwined with both business logic and throughput analysis\n2. The error flows you design may behave in ways you didn‚Äôt expect\n3. It can be actually helpful to keep both legacy and new flows alive\n4. The power of monitoring and alerting from day one, combined with gradual rollout\n5. Throughput will still be limited by the weakest link in the pipeline, regardless of how powerful your infrastructure and frameworks are\n6. Remembering the ‚Äúwhy‚Äù behind the migration is crucial throughout the development process\nAlthough each development team has its own unique business objectives and KPIs, these lessons were generalized so hopefully they can help you too with building Kafka pipelines that are more robust, performant and reliable.","location":"Meetup Hub (Expo Hall)","speakers":[{"name":" Ofer Beit Halachmi ","title":" Director of Engineering, Infrastructure and Architecture ","company":" Simplr "}],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1136847","date":"September 26, 2023","time":"3:00 PM - 3:20 PM","title":"Architecting Scalable IoT Systems with MQTT and Kafka","abstract":"In the evolving digital ecosystem, building scalable IoT architectures is no longer a luxury but a necessity. This session delves into the powerful integration of MQTT's lightweight messaging protocol with Kafka's robust streaming capabilities in Rimac‚Äôs Hypercar Platform.\nJoin to learn how MQTT and Kafka combine to handle millions of data points with ease, allowing Rimac to deliver a scalable and seamless customer experience. Attendees will glean insights into the architectural strategies that manage vast device networks and high-velocity data while preserving reliability, security, consistency, and integrity.","location":"Breakout Room 7","speakers":[{"name":" Christian Meinerding ","title":" CEO ","company":" HiveMQ "}],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1180210","date":"September 26, 2023","time":"3:00 PM - 4:30 PM","title":"Build A Gaming Analytics Pipeline: An AWS GameDay","abstract":"Join us for a Confluent and Amazon Web Services (AWS) GameDay, a gamified workshop where you will build an event-driven, data-streaming pipeline to detect cheating, identify toxic chat, ban players, and update matchmaking ranking in real time. Participants will get hands-on experience with services such as AWS Lambda, Amazon GameLift, Amazon DynamoDB, and Confluent Cloud. Please bring a laptop and prepare yourself to learn, have fun, win swag, and herd Unicorns. AWS and Confluent Cloud accounts will be provided for the session.","location":"Meeting Room 211A","speakers":[],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1211665","date":"September 26, 2023","time":"3:00 PM - 4:00 PM","title":"Build Your Integration with Confluent Cloud and Grow Through Data Streams","abstract":"Interested in joining the Connect with Confluent (CwC) technology partner program?\nA single integration with Confluent Cloud provides the easiest solution for meeting your customers‚Äô real-time data needs and accelerates your growth through the data streaming platform that processes more than an exabyte of data per year. As part of the CwC program, you can give your customers the best experience for working with data streams while maximizing focus on your core business and the rest of your roadmap.\nDuring this session you‚Äôll meet Confluent‚Äôs Kafka experts and build your step-by-step plan for integrating with Confluent Cloud. We‚Äôll also share high-value resources including integration code samples to speed and simplify your project plan.","location":"Spotlight Theater (Expo Hall)","speakers":[],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1165880","date":"September 26, 2023","time":"3:30 PM - 3:40 PM","title":"A New UI for Kafka Connect: Your Favorite IDE!","abstract":"Many of us have had difficulty developing connector configurations before. The process involves many iterations of reading docs, editing a config, and applying that config to see what went wrong. Each of these cycles is just fast enough that we put up with it, but the minutes can add up to hours or days of resubmitting configurations over and over. What if testing your connector configuration happened as you typed, and each cycle was just seconds?\nUsing the industry standard Language Server Protocol (LSP) in a creative way, you can have configuration auto-completion, diagnostic errors, and documentation appear inside your favorite Integrated Development Environment (IDE)! This automatically brings up relevant information, and makes configuration development faster and more agile. We‚Äôll introduce the LSP, how it enables simple development of cross-cutting IDE features, and how we‚Äôve adapted the LSP to handle Connector Configurations. We‚Äôll also demonstrate the Connector Configuration plugin for IntelliJ providing live feedback from a running Connect cluster while writing a connector configuration from scratch. This will also be your chance to influence future features of the IDE plugin, and see how the LSP can improve the development experience for other parts of the Kafka ecosystem.","location":"Breakout Room 5","speakers":[{"name":" Greg Harris ","title":" Senior Software Engineer ","company":" Aiven "}],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1136856","date":"September 26, 2023","time":"3:30 PM - 3:40 PM","title":"Community Gardening: Lessons from Open Source Interactions","abstract":"Open source software means different things to different people. Although collaborating across regional and corporate boundaries can be challenging, developing software in open communities has many benefits. Nothing is truly free, but Apache NiFi is one of numerous software projects with broad community interaction. This presentation offers some insights on open source development from an active contributor and member of the Apache NiFi Project Management Committee.\n\nWhat are the characteristics of a helpful bug report? What makes a good pull request? How does the product handle security vulnerabilities? Project archives provide a rich source of material to answer these questions. After answering many emails, responding to numerous chats, and reviewing hundreds of pull requests, certain general positive qualities are worth highlighting.\n\nWriting quality code is essential, but thoughtful comments, actionable feedback, and clear public communication are also important elements of successful open source projects. This session offers lessons learned from contributing to open source projects, highlighting ways to engage regardless of technical expertise or engineering background.","location":"Breakout Room 1 (Livestream)","speakers":[{"name":" David Handermann ","title":" Principal Engineer ","company":" Cloudera "}],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1136859","date":"September 26, 2023","time":"3:30 PM - 3:40 PM","title":"Flavors of HA","abstract":"Robinhood uses Kafka in every line of its business, from stock and crypto trading to its self-clearing system and online data analytics. Different systems need different reliability guarantees. We will talk about various architectures we use to achieve these desperate goals while maintaining sanity in our client side code.\nThis talk discusses how we removed SPoF through investments in Kafka infrastructure and our client libraries, letting us support a multitude of requirements of various systems inside robinhood. In addition we will discuss:\n- Learnings from building client libraries to support these HA strategies\n- K8s sidecars to help our applications work with sharded architecture\n- Observability and debuggability tools we built to support use cases","location":"Breakout Room 4","speakers":[{"name":" Chandra Kuchi ","title":" Engineering Manager ","company":" Robinhood "}],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1136857","date":"September 26, 2023","time":"3:30 PM - 3:40 PM","title":"kash.py - How to Make Your Data Scientists Love Real-time","abstract":"Implementing real-time data pipelines is still a challenge. Even more so for data scientists who have often been brought up with batch processing and files and typically have only heard of Kafka, but never really used it. Now, do you need to hire a team of Kafka experts, or is there another way?\nWe came up with a novel way to bridge the gap between the batch and file-based world of most data scientists and the world of real-time and streaming using a new Open Source data-processing tool called kash.py (\"Kafka Shell for Python\").\nkash.py allows any Python programmer and data scientist to access the Kafka API in an easier way than ever before. It offers a large number of easy-to-use abstractions on top of the Kafka API, including bash-inspired commands like \"ls\" or \"l\" for listing Kafka topics and \"cat\", \"head\" or \"tail\" for displaying topic content. kash.py bridges the gap between the file and the streaming worlds with the \"cp\" command to upload a file to a topic, or to download a topic to a file, and offers commands inspired by functional programming to do Kafka-Kafka/File-Kafka or Kafka-File stream processing (\"map\", \"flatMap\", \"filter\" etc.) in one-liners - of course with full support for JSON Schema, Avro and Protobuf. Think kcat and add a lot more power and programmability.\nIn this session, we show you how kash.py can be used to bring the two disparate worlds of files and streaming together - and thus not only save a lot of time and money hiring real-time and streaming experts, but also make your data scientists, like ours, start loving real-time.","location":"Breakout Room 2","speakers":[{"name":" Ralph Matthias Debusmann ","title":" Enterprise Kafka Engineer ","company":" Migros "}],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1136858","date":"September 26, 2023","time":"3:30 PM - 3:40 PM","title":"Moving Towards Better Upgrades in Kafka Streams","abstract":"Are upgrades a daunting subject for you and your peers? Has it been a place of confusion or frustration in the past? In this talk, we‚Äôll address the approaches we‚Äôve seen in the industry, with a bit of flair! Take a break from sitting all day and join us in an engaging talk about Kafka Streams presented in the Kinesthetic learning style.\n\nWe will discuss:\n\n- Maintaining and managing your topology lifecycles\n\n- Safeguarding and standardize your workflow processes to ensure better operational practices\n\n- Considerations around upgrading stateful applications\n\nThis talk will encourage movement while we learn to help bolster cognitive retention and offer a break from normal passive listening. So come learn from two Confluent CSTAs in the industry and make kstreams upgrade strategies muscle memory.","location":"Breakout Room 3","speakers":[{"name":" Humberto Rivero ","title":" Customer Success Architect ","company":" Confluent "},{"name":" Whitney Steward ","title":" Customer Success Technical Architect ","company":" Confluent "}],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1136846","date":"September 26, 2023","time":"3:30 PM - 3:45 PM","title":"Schema Driven Development Leveraging Open Source Tools","abstract":"As a company grows, so do the number of teams and with it the complexity of sharing data. Data Contracts via Schemas are the perfect tool for ensuring a smooth process, but how does one deal with this when you are juggling multiple programming languages and database technologies? In this lightning talk we will cover how open source tools such as buf cli, omniproto and bash scripting can be used to create cross language libraries that promote clear lines of ownership and safety in changes by leveraging schema compatibility checks. We will discuss how this can be integrated into workflows both with and without Kafka, as well as whether or not a schema registry is available.","location":"Meetup Hub (Expo Hall)","speakers":[{"name":" Brandon Brown ","title":" Senior Staff Software Engineer ","company":" SecurityScorecard  "}],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1136860","date":"September 26, 2023","time":"3:30 PM - 3:40 PM","title":"Visualizing the Stream","abstract":"Do you need to be able to analyze large amounts of streaming data in order to make gain insights, make data-driven decisions, and support analytics applications? Are you a human who finds it easier to understand pictures than huge collections of numbers?\n\nIf you are a human who likes pictures, then you need to visualize stream data. With multiple topics and thousands or millions of events per second, this can be a challenge.\n\nThis talk will explore how Confluent and Imply can be used to visualize streaming data in real-time. We will discuss visualization tools and options including line charts, bar charts, heat maps, geospatial, and scatter plots, and how they can be used to help humans understand data.","location":"Breakout Room 6","speakers":[{"name":" Rick Jacobs ","title":" Developer Outreach ","company":" Imply "}],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1136862","date":"September 26, 2023","time":"3:45 PM - 4:00 PM","title":"Break","abstract":"N/A","location":"","speakers":[],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1136871","date":"September 26, 2023","time":"4:00 PM - 4:45 PM","title":"‚ö° Flink Power Hour‚ö°","abstract":"Join us for back-to-back lightning talks on Apache Flink topics from three experts.\n- The Flink Table API: Streaming Inspired by SQL, but Beyond and in Java, Timo Walther\n- An Overview of Flink SQL Joins, David Anderson\n- The AdaptiveScheduler, a New Default for Streaming, David Moravek","location":"Meetup Hub (Expo Hall)","speakers":[{"name":" David Anderson ","title":" Software Practice Lead ","company":" Confluent "},{"name":" Timo Walther ","title":" Principal Software Developer I ","company":" Confluent "}],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1136864","date":"September 26, 2023","time":"4:00 PM - 4:20 PM","title":"Boosting Kafka Performance in a Day","abstract":"We‚Äôve all tried adjusting the number partitions, replica fetch size, batch size, socket buffer size, number of I/O threads ‚Ä¶ you could spend an hour enumerating the tuning options for Kafka to squeeze a few percent of extra performance. Finding the right combination of parameters is closer to alchemy than engineering work, time consuming and in bigger deployments sometimes even impossible!\nWhat if there's a better way? What if you could \"just\" make Kafka more efficient and gain better performance without months of engineering effort?\nSince Kafka is written in Java and Scala, in this talk, we'll discuss the effect of Java Virtual Machine (JVM) on a Kafka cluster. We'll demonstrate that running Kafka on Azul Prime JVM can reduce the latency by up to 40% while handling the same load. We‚Äôll go beyond shiny marketing-y charts, dig into flamegraphs, and look at assembly code to fully understand where the speed up is coming from.\nAfter the session, you'll understand the importance of the underlaying JVM and how you can leverage this knowledge to boost the performance of the cluster to achieve better SLAs or reduce the infrastructure costs.","location":"Breakout Room 7","speakers":[{"name":" Ji≈ô√≠ Holu≈°a ","title":" Senior Product Manager ","company":" Azul "}],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1136865","date":"September 26, 2023","time":"4:00 PM - 4:45 PM","title":"Fast Fourier Transform (FFT) of Time Series in Kafka Streams","abstract":"Digital Signal Processing powered by Kafka streams can open IoT to a new era of possibilities by bringing computational power closer to the location where the data originated.\n\nThe purpose of this presentation is to demonstrate the power of Kafka Streams as a backbone for mathematical methods of signal processing. In this session, we‚Äôll explore transforming signals from the time domain to the frequency domain using FFT, maximizing the level of compression of input signals while building a precise frequency alert system.\n\nThis presentation focuses on the following areas:\n\n-Signal imitators of periodic waveforms (sine, triangle, square, sawtooth, etc.) in compliance with OpenMetrics standard.\n\n-Pipeline of logical operators (AND, OR, XOR, etc.) that simulate joined superposition of various signal inputs.\n\n-The processor that performs FFT, converting input signals into individual spectral components and thereby provides frequency information about the signal.\n\n-Prometheus/Grafana visualization of FFT of input signals in real time.\n\nBy the end of the session, you‚Äôll understand the fundamentals of digital signal processing using Kafka and have the tools you need to build and implement FFT in Kafka Streams.","location":"Breakout Room 5","speakers":[{"name":" Igor Khalitov ","title":" Senior Solutions Architect ","company":" Confluent "}],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1136866","date":"September 26, 2023","time":"4:00 PM - 4:45 PM","title":"General Coordinates Network: Harnessing Kafka for Real-Time Open Astronomy at NASA","abstract":"Kafka has come to play an essential role in astronomy research. Particularly where black holes and neutron stars are involved, astronomers are increasingly seeking out the ‚Äútime domain‚Äù and want to study explosive transients and variability. In response, observatories are increasingly adopting streaming technologies to send alerts to astronomers and to get their data to their science users in real time. In this talk, we will discuss architectural choices, challenges, and lessons learned in adapting Kafka for open science and open data. Our novel approach to OpenID Connect / OAuth2 in Kafka is designed to securely scale Kafka from access inside a single organization to access by the general public. We will present a case study of the General Coordinates Network (GCN), a public collaboration platform run by NASA for the astronomy research community to share alerts and rapid communications about high-energy, multi-messenger, and transient phenomena. Over the past 30 years, GCN has helped enable many seminal advances by disseminating observations, quantitative near-term predictions, requests for follow-up observations, and observing plans. GCN distributes alerts between space- and ground-based observatories, physics experiments, and thousands of working astronomers around the world.","location":"Breakout Room 1 (Livestream)","speakers":[{"name":" Judith Racusin ","title":" Astrophysicist ","company":" NASA Goddard Space Flight Center "},{"name":" Leo Singer ","title":" Astrophysicist ","company":" NASA Goddard Space Flight Center "}],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1136867","date":"September 26, 2023","time":"4:00 PM - 4:45 PM","title":"Handle Millions of IoT Devices Connected to Kafka via MQTT","abstract":"From vehicle communication to predictive maintenance, real-time ingestion, analysis and control has become an increasingly important characteristic of the machines we use in everyday life.\n\nYou have a fleet of IoT devices deployed in your customer environments and need to integrate with your event-driven architecture in real-time. You already have an MQTT broker infrastructure in place and need to integrate with your Kafka data plane.\n\nIoT devices use specific protocols such as MQTT and cannot connect directly to Apache Kafka. Various solutions are possible, including data replication between MQTT and Kafka using technologies such as Kafka Connect, as well as proxying MQTT protocol to and from Kafka via a protocol translation gateway.\n\nI will discuss the performance, security and scalability characteristics of these approaches to help you decide what works best for you.\n\nThis session is targeted towards developers interested in learning how to use Kafka as the data plane for their MQTT broker infrastructure, without needing to run separate MQTT brokers.","location":"Breakout Room 2","speakers":[{"name":" John Fallows ","title":" CTO ","company":" Aklivity "}],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1136868","date":"September 26, 2023","time":"4:00 PM - 4:45 PM","title":"Off-Label Data Mesh: A Prescription for Healthier Data","abstract":"Data mesh is a relatively recent architectural innovation, espoused as one of the best ways to fix analytic data. We renegotiate aged social conventions by focusing on treating data as a product, with a clearly defined data product owner, akin to that of any other product. In addition, we focus on building out a self-service platform with integrated governance, letting consumers safely access and use the data they need to solve their business problems.\n\nData mesh is prescribed as a solution for _analytical data_, so that conventionally analytical results (think weekly sales or monthly revenue reports) can be more accurately and predictably computed. But what about non-analytical business operations? Would they not also benefit from data products backed by self-service capabilities and dedicated owners? If you've ever provided a customer with an analytical report that differed from their operational conclusions, then this talk is for you.\n\nAdam discusses the resounding successes he has seen from applying data mesh _off-label_ to both analytical and operational domains. The key? Event streams. Well-defined, incrementally updating data products that can power both real-time and batch-based applications, providing a single source of data for a wide variety of application and analytical use cases. Adam digs into the common areas of success seen across numerous clients and customers and provides you with a set of practical guidelines for implementing your own minimally viable data mesh.\n\nFinally, Adam covers the main social and technical hurdles that you'll encounter as you implement your own data mesh. Learn about important data use cases, data domain modeling techniques, self-service platforms, and building an iteratively successful data mesh.","location":"Breakout Room 6","speakers":[{"name":" Adam Bellemare ","title":" Staff Technologist, Office of the CTO ","company":" Confluent "}],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1136869","date":"September 26, 2023","time":"4:00 PM - 4:45 PM","title":"Side Effects Are Why We Can‚Äôt Have Nice Things","abstract":"What will save us from software complexity? Microservices architectures? Immutable data? Functional languages? Event streaming? These may seem like disparate solutions, but they‚Äôre all connected, They‚Äôre all striving to solve the same fundamental problem: Side effects.\n\nSide effects are one of the most insidious causes of software complexity, and yet we‚Äôre barely aware of them. And the attempts to tackle side effects have produced some of the most interesting developments in software development in the past decade, yet as an industry we‚Äôre barely aware of the fundamental pattern tying everything together. Let‚Äôs fix that.\n\nJoin me to understand what side effects are and why they matter. You‚Äôll learn to spot them in your own code. You‚Äôll see how they sneak into our tests, our APIs and our systems‚Äô designs and make everything harder. Then we‚Äôll look at our industry‚Äôs many solutions to side effects. You‚Äôll see why immutable data structures are appearing in every language; why we invented OO and why it might get replaced with FP. And we‚Äôll see what it all means for the larger designs of microservices, data processing and event systems.\n\nSide effects are a hologram - put the right light on them and you can see a whole picture of modern computing emerging. Take a look and you‚Äôll see the nice things the future could hold.","location":"Breakout Room 3","speakers":[{"name":" Kris Jenkins ","title":" Senior Developer Advocate ","company":" Developer Voices "}],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1136870","date":"September 26, 2023","time":"4:00 PM - 4:45 PM","title":"The Interactive Kafka Light Show","abstract":"The ability to properly design and implement highly resilient event driven systems is critical in our data centric world. But getting one‚Äôs mind around the complex choreography of this data driven architecture can be absurdly difficult. By using a Raspberry Pi Kafka Cluster, light bars, and arcade buttons, we will bring event driven architectures into the physical world to help visually understand all the things. Send messages with our portable Raspberry Pi producers and see them flow through our message brokers and to the consumers. With your own eyes, see how tweaking your messaging system impacts your distributed architecture. Do your messages need delivered in order? Or maybe architecting for throughput is more important. Whether it‚Äôs streaming real-time data or decoupling microservices through event notifications, this presentation will bring to light the important concepts you need to consider. Come join our interactive session as we trip the light fantastic in this colorful eye-opening journey into the event streaming dream.","location":"Breakout Room 4","speakers":[{"name":" Barry Tarlton ","title":" Senior Consultant Software Engineer ","company":" Nationwide "}],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1136872","date":"September 26, 2023","time":"4:30 PM - 4:50 PM","title":"Why BYOC is the Future of Cloud Services","abstract":"The modern data-driven landscape has seen a significant shift to managed cloud services. While cloud services have clear advantages, they also require relinquishing a level of control. Issues around where and how the data is managed can present a strategic dilemma.\nThe \"Bring Your Own Cloud\" (BYOC) deployment model may be the answer. With Zero Trust access and an isolated, protected cluster, BYOC deployments have multiple layers of security. They allow the cluster to run in the user's cloud but can also guarantee SLAs and SLOs are met.\nAs an added benefit, BYOC presents an opportunity to significantly reduce ingress/egress costs, which can add up quickly for traditional managed service deployments.\nIn this talk we‚Äôll cover:\n- An architectural overview of BYOC\n- Behind the scenes of BYOC and deployment strategies\n- A demo of Redpanda BYOC","location":"Breakout Room 7","speakers":[{"name":" Christina Wei-Mei Lin ","title":" Developer Advocate ","company":" Redpanda "}],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1136875","date":"September 26, 2023","time":"5:00 PM - 5:45 PM","title":"A Practical Guide To End-to-End Tracing In Event Driven Architectures","abstract":"Can you determine how a given event came to be? Is it an aggregation, a combination of multiple events with different sources? What are its origins?\n\nAs event driven architectures become more sophisticated, with features such as stateful stream processing, data joining, and multi-cluster flows, it becomes harder to trace the path of an event, its origins and touch points. At the same time, it also becomes more important.\n\nUsing code examples and usage scenarios we will dive into the tracing capabilities of OpenTelemetry for Kafka clients, including those using the Consumer/Producer and Kafka Streams libraries, as well as the Connect and ksqlDB platforms. This will culminate in an end-to-end tracing pipeline demonstration.\n\nThis talk will cover the following topics:\n\n- Distributed tracing concepts, including context propagation and the OpenTelemetry implementation stack\n\n- OpenTelemetry‚Äôs Kafka instrumentation, what is supported out of the box, code examples, edge cases, challenges and solutions\n\n- A demonstration of an end-to-end tracing implementation\n\nIn this session, you will gain an understanding of the importance of end-to-end traceability, and several tools & examples for improving observability in your own distributed event driven applications.","location":"Breakout Room 2","speakers":[{"name":" Roman Kolesnev ","title":" Staff Customer Innovation Engineer ","company":" Confluent "}],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1136874","date":"September 26, 2023","time":"5:00 PM - 5:20 PM","title":"Build Streaming Data Applications in Minutes Using SwimOS and Nstream","abstract":"Optimizing the full value of streaming data often leads to more data systems, additional oversight, and increased latency, rendering \"real-time\" to a mere buzzword. Imagine building a genuine real-time streaming data application in mere minutes. That's what Nstream and SwimOS bring to the table.\nJeremy Custenborder, Nstream's Field CTO, will guide you through the transformative impact of streaming data applications that offer immediate insights, continuously run business logic, and enable decision automation. Learn how Nstream, anchored by open-source SwimOS, accelerates development speed tenfold, thanks to three groundbreaking innovations: dynamic stateful services, seamless streaming APIs, and real-time UIs.\nBut seeing is believing. Jeremy will showcase a live demo, illustrating the tangible advantages: fewer managed data systems, reduced latency, simpler coding, and interactive UIs.\nWhether a Kafka novice or a seasoned user, walk away from this session with actionable takeaways and tools for your business.","location":"Breakout Room 7","speakers":[{"name":" Jeremy Custenborder ","title":" Field CTO ","company":" Nstream "}],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1180216","date":"September 26, 2023","time":"5:00 PM - 5:45 PM","title":"Business Event Driven Architecture & Governance in Action","abstract":"Event-Driven Architecture is the only way to achieve resilient scalable reactive systems. It enables loose coupling, drives autonomy for the dev teams, and is the key to digital business behaviour monitoring.\nBut how do you implement EDA the right way? And make sure that it keeps being implemented the right way?\nAt Current 2022 there was a talk from Confluent explaining the value of a COE (Center of Excellence).\nHowever, in this talk, We‚Äôll be sharing several experiences in setting up a COE for large industrial companies, insurance and logistic environments.\nFrom setting up a strong foundation, defining event designs, best practices, and principles to the guidance of development teams. The COE brings business and IT together to ensure EDA is set-up and used correctly, but also to identify and capitalize on new opportunities that automatically arise from using EDA.\nUsing several real life experiences (AXA Belgium, Engie, Nike, ...)","location":"Breakout Room 6","speakers":[{"name":" Wim Debreuck ","title":" Architect/Founder ","company":" cymo.eu "}],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1136877","date":"September 26, 2023","time":"5:00 PM - 5:45 PM","title":"Debezium Snapshots Revisited!","abstract":"Initial snapshots are a core feature of Debezium: when setting up a new CDC connector, existing tables can be scanned in order to export their full state to consumers, before starting to capture changes from the transaction log. While this works great in general, a few questions came up again and again in the Debezium community over time:\n\n* How to re-snapshot just a single table?\n\n* How to pause and resume long-running snapshots?\n\n* How to run snapshots in parallel to reading changes from the log?\n\nAll this, and more, becomes possible with the notion of incremental snapshots. In this session you'll learn how this innovative scheme of interleaving snapshot queries and log-based change events works under the hood and how it solves common tasks when running CDC pipelines. We'll also discuss advanced topics like parallelising snapshots and customising snapshot contents.","location":"Breakout Room 5","speakers":[{"name":" Gunnar Morling ","title":" Senior Staff Software Engineer ","company":" Decodable "}],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1136878","date":"September 26, 2023","time":"5:00 PM - 5:45 PM","title":"Evolution of Streaming Pipeline at Lyft","abstract":"Lyft is a ride-sharing company which is a two-sided marketplace; balancing supply and demand using various levers (passenger pricing, driver incentive etc.) to maintain an efficient system. Lyft has built a real-time optimization platform that helps to build the product faster. This complex system makes real-time decisions using various data sources; machine learning models; and a streaming infrastructure for low latency, reliability and scalability. This infrastructure consumes a massive number of events from different sources to make real-time product decisions.\n\nRakesh discusses how Lyft organically evolved and scaled the streaming platform that provides a consistent view of the marketplace to aid an individual team independently run their optimization. The platform offers online and offline feature access that helps teams to back test their model in the future.\n\nEach iteration provided better scale, unlocking different capabilities of the marketplace and new avenues for growth. To accomplish these, the team has set the roadmap for next-generation streaming platform and developing smarter tools.\n\nTopics include:\n\n* Describing Lyft‚Äôs streaming platform for dynamic decision-making\n\n* Where Kafka fits in streaming tech stack and how it helped us to scale better\n\n* Productionizing the very first pipeline\n\n* Tools that simplified pipeline creation development environment that is friendly to Data Science people\n\n* Lesson learned","location":"Breakout Room 4","speakers":[{"name":" Rakesh Kumar ","title":" Staff Software Engineer ","company":" Lyft Inc "}],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1136879","date":"September 26, 2023","time":"5:00 PM - 5:45 PM","title":"Flink SQL: The Challenges to Build a Streaming SQL Engine","abstract":"Flink SQL is a powerful tool for stream processing that allows users to write SQL queries over streaming data. However, building a streaming SQL engine is not an easy task. In this session, we will explore the challenges that arise when building a modern streaming SQL engine like Flink SQL.\nWe will discuss the following challenges and how Flink SQL resolve them:\n- Late Data: Handling late arrival data and guaranteeing result correctness.\n- Change Data Ingestion and Processing: How to ingest change data from databases in real-time and apply complex operations on the change events.\n- Event Ordering: Shuffle may disrupt the order of data updates and get the wrong result.\n- Nondeterminism: Nondeterministic functions and external system lookups may produce different results on change data and get the wrong result.\n- State Storage: How to effectively process infinite datasets with limited storage without losing the correctness of results.\n\nWe will also show real-world examples of using Flink SQL to solve common stream processing problems. By the end of this session, you will better understand the challenges involved in building a streaming SQL engine and how to overcome them.","location":"Breakout Room 3","speakers":[{"name":" Jingsong Li ","title":" Staff Engineer ","company":" Alibaba "}],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1211666","date":"September 26, 2023","time":"5:00 PM - 6:00 PM","title":"Intersectional Happy Hour","abstract":"N/A","location":"Spotlight Theater (Expo Hall)","speakers":[],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1136881","date":"September 26, 2023","time":"5:00 PM - 5:15 PM","title":"Promote, Don't Repeat: How to Manage Consistent Configurations Across Multiple Kafka Environments","abstract":"Conventional Kafka developers are liable to perform monotonous functions like creating topics, managing topic configurations, granting access for producers/consumers, topic sizing, capacity planning and other day-2 operations. At Fidelity Investments we support hundreds of applications publishing and subscribing to tens of thousands of event streams across business units. Changes to applications are constant. Expecting developers to manually repeat commands across environments slows development and increases the chance for misconfigurations, ultimately degrading the quality of work.\nIn this talk, we will discuss how Fidelity Investments modeled a unique API to seamlessly lift and shift application topics, ACLs, quotas, and every other entity from lower environment to higher environment clusters, thus resulting in the propagation of exact configurations across environments with zero human errors. We will also highlight the salient features of this in-house REST API in providing options to scale differently in higher environments. Flexibility of modifying users across environments while preserving complete Kafka cluster configuration is an added bonus. This entire process empowers Kafka applications teams to promote version-controlled releases across multiple environments with just a single click and a safety net to rollback to older configurations.","location":"Meetup Hub (Expo Hall)","speakers":[{"name":" Craig Long ","title":" Software Engineer ","company":" Fidelity Investments "},{"name":" Brian Xiao ","title":" Software Engineer ","company":" Fidelity Investments "}],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1136880","date":"September 26, 2023","time":"5:00 PM - 5:45 PM","title":"Standing on the Shoulders of Open-Source Giants: The Serverless Realtime Lakehouse","abstract":"Unlike just a few years ago, today the lakehouse architecture is an established data platform embraced by all major cloud data companies such as AWS, Azure, Google, Oracle, Microsoft, Snowflake and Databricks.\n\nThis session kicks off with a technical, no-nonsense introduction to the lakehouse concept, dives deep into the lakehouse architecture and recaps how a data lakehouse is built from the ground up with streaming as a first-class citizen.\n\nThen we focus on serverless for streaming use cases. Serverless concepts are well-known from developers triggering hundreds of thousands of AWS Lambda functions at a negligible cost. However, the same concept becomes more interesting when looking at data platforms.\n\nWe have all heard about the principle \"It runs best on Powerpoint\", so I decided to skip slides here and bring a serverless demo instead:\n\nA hands-on, fun, and interactive serverless streaming use case example where we ingest live events from hundreds of mobile devices (don't miss out - bring your phone and be part of it!!). Based on this use case I will critically explore how much of a modern lakehouse is serverless and how we implemented that at Databricks (spoiler alert: serverless is everywhere from data pipelines, workflows, optimized Spark APIs, to ML).\n\nTL;DR benefits for the Data Practitioners:\n-Recap the OSS foundation of the Lakehouse architecture and understand its appeal\n- Understand the benefits of leveraging a lakehouse for streaming and what's there beyond Spark Structured Streaming.\n- Meat of the talk: The Serverless Lakehouse. I give you the tech bits beyond the hype. How does a serverless lakehouse differ from other serverless offers?\n- Live, hands-on, interactive demo to explore serverless data engineering data end-to-end. For each step we have a critical look and I explain what it means, e.g for you saving costs and removing operational overhead.","location":"Breakout Room 1 (Livestream)","speakers":[{"name":" Frank Munz ","title":" Principal TM Engineer ","company":" Databricks "}],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1136882","date":"September 26, 2023","time":"5:30 PM - 5:50 PM","title":"Introducing Oxia: A Scalable Zookeeper Alternative","abstract":"Event streaming platforms like Kafka have traditionally leaned on ZooKeeper as the cornerstone for coordination and metadata management. This presentation introduces Oxia, a compelling alternative solution.\nHailing from the labs of StreamNative, Oxia brings forth a genuinely horizontally scalable metadata framework. It empowers distributed messaging systems to seamlessly handle hundreds of millions of topics, all while removing the intricacies and operational burdens associated with ZooKeeper.\nThe transformative potential of Oxia extends to developers' messaging strategies and application architectures. It holds the promise of simplifying both, marking a significant evolution in the event streaming landscape.","location":"Breakout Room 7","speakers":[{"name":" David Kjerrumgaard ","title":" Developer Advocate ","company":" StreamNative "}],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1211667","date":"September 26, 2023","time":"6:00 PM - 10:00 PM","title":"Current 2023 Party","abstract":"Join us Tuesday evening to unwind from day one of learning at the Current 2023 Party! Head over to San Pedro Square Market to enjoy libations, a variety of gourmet food vendors, live music, and networking with your fellow attendees from the data streaming community.","location":"San Pedro Square Market","speakers":[],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1211668","date":"September 27, 2023","time":"7:30 AM - 8:30 AM","title":"Breakfast","abstract":"N/A","location":"Hall 3","speakers":[],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1222576","date":"September 27, 2023","time":"7:30 AM - 3:00 PM","title":"Registration","abstract":"N/A","location":"Hall 1 Concourse","speakers":[],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1217210","date":"September 27, 2023","time":"8:00 AM - 4:00 PM","title":"Expo","abstract":"N/A","location":"Expo Hall","speakers":[],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1136883","date":"September 27, 2023","time":"9:00 AM - 10:15 AM","title":"Kafka, Flink, and Beyond","abstract":"In this keynote, leaders from the Kafka and Flink communities will highlight recent contributions, upcoming project improvements, and the innovative applications that users are building to shape the future of data streaming.","location":"San Jose Civic","speakers":[{"name":" Satish Duggana ","title":" Sr Staff Engineer ","company":" Uber "},{"name":" Ismael Juma ","title":" Senior Principal Engineer ","company":" Confluent "},{"name":" Tobias Nothaft ","title":" Sub Product Owner Streaming Platform ","company":" BMW Group "},{"name":" Martijn Visser ","title":" Senior Product Manager ","company":" Confluent "}],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1136885","date":"September 27, 2023","time":"10:30 AM - 11:15 AM","title":"Datalake Rock Paper Scissors: Iceberg + Flink or Iceberg + Spark?","abstract":"Do you have existing data pipelines for real-time data and want to add storage into the mix? Are you planning to use Apache Iceberg tables for storage, but are unsure whether to choose Apache Flink or Apache Spark to ingest the data from your Apache Kafka topics? While you have choices, how do you assess which technology is the right one for your use case?\n\nAt Bloomberg, Kafka and Iceberg are core elements in our real-time data pipelines and storage sinks. In this session, we'll share our experiences and lessons learned working with both technologies to ingest data from Kafka into our Iceberg datalake at near-real-time speeds. As we evaluate the pros and cons of Flink and Spark, we will compare and contrast the two approaches, specifically with regard to their functionality, performance, fault-tolerance, scaling, and resource utilization. We‚Äôll also discuss how the bursty nature of Spark reads, the different parallelism approaches offered by Flink and Spark, and the small-file-problem may impact the overall performance of your data pipelines.\n\nWhen we‚Äôre done, you‚Äôll have a better understanding of how these technologies work and can make a more informed choice for your next datalake integration.","location":"Breakout Room 1 (Livestream)","speakers":[{"name":" Sitarama Chekuri ","title":" Senior Software Engineer ","company":" Bloomberg "},{"name":" Ben de Vera ","title":" Software Engineer ","company":" Bloomberg "}],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1136886","date":"September 27, 2023","time":"10:30 AM - 11:15 AM","title":"Deeply Declarative Data Pipelines","abstract":"With Flink and Kubernetes, it's possible to deploy stream processing jobs with just SQL and YAML. This low-code approach can certainly save a lot of development time. However, there is more to data pipelines than just streaming SQL. We must wire up many different systems, thread through schemas, and, worst-of-all, write a lot of configuration.\n\nIn this talk, we'll explore just how \"declarative\" we can make streaming data pipelines on Kubernetes. I'll show how we can go deeper by adding more and more operators to the stack. How deep can we go?","location":"Breakout Room 5","speakers":[{"name":" Ryanne Dolan ","title":" Senior Staff Software Engineer ","company":" LinkedIn  "}],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1180213","date":"September 27, 2023","time":"10:30 AM - 11:30 AM","title":"Fundamentals of Apache Kafka¬Æ","abstract":"Come learn at a free training providing an overview of what Apache Kafka is, what it's used for, and the core concepts that enable it to power a highly scalable, available and resilient real-time data in motion platform.\nThe training begins with an introduction to the shift toward real-time data streaming, and continues all the way through to best practices for developing applications with Kafka and how to integrate Kafka into your environment. No need to pre-register.","location":"Meeting Room 212","speakers":[{"name":" Antonio Loma Daza ","title":" Sr. Technical Trainer ","company":" Confluent "}],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1136884","date":"September 27, 2023","time":"10:30 AM - 10:50 AM","title":"How Curing ‚ÄúKafkatosis‚Äù Can Improve Your Stream Management and Governance ","abstract":"Many enterprises find that as their Kafka estate grows to include many brokers and clusters in diverse cloud and on-prem environments, they experience ‚ÄúKafkatosis‚Äù‚Äî a systemic condition that causes their Kafka estate to become increasingly difficult to control and scale over time.\nIn this talk, Schabowsky will introduce the causes and symptoms of Kafkatosis, such as a lack of stream reuse, inefficient application onboarding, and operational disruptions. He will help you understand the nature and impact of these problems through real-world examples, and teach you how to recognize warning signs within your own organization.\nThis session will arm you with the practical strategies and tools you need to optimize your stream management practices, safeguard your data pipelines, and harness the full potential of real-time event-driven data without falling victim to Kafkatosis.","location":"Breakout Room 7","speakers":[{"name":" Jonathan Schabowsky ","title":" Field CTO ","company":" Solace "}],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1136888","date":"September 27, 2023","time":"10:30 AM - 11:15 AM","title":"Mitigating Kafka Broker ‚ÄòGray‚Äô Failures For Key Based Partitioners With Partition Multihoming","abstract":"Kafka broker gray failures are a common source of incidents at New Relic. We define these gray failures as events where a broker slows or stops request processing while continuing to lead a partition. A single broker gray failure often has cascading impact for key-based partitioning producers, creating back pressure and upstream lag for messages to all brokers. While there have been adaptive partition switching improvements (e.g. KIP-794 Strictly Uniform Sticky Partitioner) that allow producers to route around problematic brokers, these do not work for key-based partitioning strategies.\n\nIn this talk I describe partition multihoming (PMH), a form of virtual partitioning where two or more physical Kafka partitions are guaranteed to be consumed by the same consumer instance. When a broker is unhealthy, a multihoming partitioner can route messages through partitions led by a healthy broker destined to the same consumer as before the failure, preserving application functionality.\n\nWe will go over:\n\n* The real world impact of gray failures that led to the creation of PMH\n\n* The implementation details of PMH on key-based topics\n\n* The PMH workflow when a broker becomes unhealthy\n\n* The future roadmap for adding virtual partitioning in Apache Kafka for PMH\n\nAfter this talk you will understand why PMH is key to reliability and should become a first-class feature of Kafka.","location":"Breakout Room 4","speakers":[{"name":" Christopher Wildman ","title":" Principal Software Engineer ","company":" New Relic "}],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1180208","date":"September 27, 2023","time":"10:30 AM - 11:15 AM","title":"Our Multi-Year Journey to a 10x Faster Confluent Cloud","abstract":"Confluent Cloud is a cloud-native service based on Apache Kafka. We run tens of thousands of clusters across all major cloud service providers (AWS, GCP and Azure). In this talk, we will go over our journey to make Confluent Cloud 10x faster than Apache Kafka.\nWe will talk about how we designed our various workloads, the complexities involved in our cloud-native service, the challenges we faced, and the various pitfalls we ran into. We will also cover the interesting learnings, which in hindsight, are first principles from this multi-year journey.\nBy attending this talk, attendees will be able to take our learnings from making Confluent Cloud latencies 10x better and possibly apply similar principles to their cloud native data streaming systems.","location":"Breakout Room 6","speakers":[{"name":" Marc Selwan ","title":" Product Manager ","company":" Confluent "},{"name":" Shriram Sridharan ","title":" Senior Manager, Engineering ","company":" Confluent "}],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1136889","date":"September 27, 2023","time":"10:30 AM - 11:15 AM","title":"Securing Your Streaming Data with Role Based Access Control: What, Why and How","abstract":"As streaming platforms such as Apache Kafka become the central nervous system of enterprises, it is crucial to ensure security of streaming data as such data plays an increasingly important role in mission critical applications in organizations. Role-Based Access Control (RBAC) is one of the most common ways to provide security for data in motion. Access control privileges that are defined in a RBAC service determine which role can access and perform operations on specific resources. In this talk, we first present the state of the art in Role-Based Access Control for streaming data in Apache Kafka. We then present a novel approach where we bring the same RBAC concepts from relational systems to the data in motion space and compare it with the current solutions.\n\nAttendees will learn about the state of the art in security and Role-Based Access Control in data streaming technologies and understand challenges in these approaches. They will also learn a novel approach that they can use in their organizations to secure access to the streaming data, whether it is stored in one Kafka cluster or across many Kafka clusters.","location":"Breakout Room 3","speakers":[{"name":" Hojjat Jafarpour ","title":" CEO ","company":" DeltaStream "},{"name":" Krishna Raman ","title":" Software Engineer ","company":" DeltaStream Inc. "}],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1136890","date":"September 27, 2023","time":"10:30 AM - 11:15 AM","title":"Time-State Analytics","abstract":"Across many domains, we see a growing need for complex analytics to track precise metrics at Internet scale to detect issues, identify mitigations, and analyze patterns. Think about delays in airlines (Logistics), food delivery tracking (Apps), detect fraudulent transactions (Fintech), flagging computers for intrusion (Cybersecurity), device health (IoT), and many more.\n\nFor instance, at Conviva, our customers want to analyze the buffering that users on some types of devices suffer, when using a specific CDN.\n\nWe refer to such problems as Multidimensional Time-State Analytics. Time-State here refers to the stateful context-sensitive analysis over event streams needed to capture metrics of interest, in contrast to simple aggregations. Multidimensional refers to the need to run ad hoc queries to drill down into subpopulations of interest. Furthermore, we need both real-time streaming and offline retrospective analysis capabilities.\n\nIn this talk, we will share our experiences to explain why state-of-art systems offer poor abstractions to tackle such workloads and why they suffer from poor cost-performance tradeoffs and significant complexity.\n\nWe will also describe Conviva‚Äôs architectural and algorithmic efforts to tackle these challenges. We present early evidence on how raising the level of abstraction can reduce developer effort, bugs, and cloud costs by (up to) an order of magnitude, and offer a unified framework to support both streaming and retrospective analysis. We will also discuss how our ideas can be plugged into existing pipelines and how our new ``visual'' abstraction can democratize analytics across many domains and to non-programmers.","location":"Breakout Room 2","speakers":[{"name":" Henry Milner ","title":" Principal Engineer ","company":" Conviva "},{"name":" Vyas Sekar ","title":" Chief Scientist ","company":" Conviva "}],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1180211","date":"September 27, 2023","time":"11:00 AM - 12:30 PM","title":"Build A Gaming Analytics Pipeline: An AWS GameDay","abstract":"Join us for a Confluent and Amazon Web Services (AWS) GameDay, a gamified workshop where you will build an event-driven, data-streaming pipeline to detect cheating, identify toxic chat, ban players, and update matchmaking ranking in real time. Participants will get hands-on experience with services such as AWS Lambda, Amazon GameLift, Amazon DynamoDB, and Confluent Cloud. Please bring a laptop and prepare yourself to learn, have fun, win swag, and herd Unicorns. AWS and Confluent Cloud accounts will be provided for the session.","location":"Meeting Room 211A","speakers":[],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1136891","date":"September 27, 2023","time":"11:00 AM - 11:20 AM","title":"Operationalizing Pluralsight's Data with Materialize","abstract":"Streaming systems are a powerful tool that unlock real-time analytics and applications within organizations. But, they come at a cost: it often takes a team of experts a considerable amount of time to set these systems up, operate them in production, and make meaningful changes to the underlying business logic.\nWhen Pluralsight noticed the high cost and slow time to value of their real-time data products, they knew they needed a new tool for the job. Enter Materialize, the first operational data warehouse. Materialize makes many of the benefits of popular streaming systems available through a simpler, higher-level interface: SQL. Using Materialize, complex and expensive data pipelines can be swapped out for incrementally-maintained materialized views.\nIn this talk, we'll present how Pluralsight is replacing streaming systems with an operational data warehouse for their real-time use cases. Today, Materialize powers Pluralsights Plan Analytics and core data models for their content offerings. Looking forward, the Pluralsight team plans to use Materialize for all of their real-time, user-facing applications.","location":"Breakout Room 7","speakers":[{"name":" Arjun Narayan ","title":" CEO ","company":" Materialize "}],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1211669","date":"September 27, 2023","time":"11:30 AM - 12:30 PM","title":"Build Your Integration with Confluent Cloud and Grow Through Data Streams","abstract":"Interested in joining the Connect with Confluent (CwC) technology partner program?\nA single integration with Confluent Cloud provides the easiest solution for meeting your customers‚Äô real-time data needs and accelerates your growth through the data streaming platform that processes more than an exabyte of data per year. As part of the CwC program, you can give your customers the best experience for working with data streams while maximizing focus on your core business and the rest of your roadmap.\nDuring this session you‚Äôll meet Confluent‚Äôs Kafka experts and build your step-by-step plan for integrating with Confluent Cloud. We‚Äôll also share high-value resources including integration code samples to speed and simplify your project plan.","location":"Spotlight Theater (Expo Hall)","speakers":[],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1136893","date":"September 27, 2023","time":"11:30 AM - 12:15 PM","title":"Building a Winning Data Engineering Culture","abstract":"Data engineering is the backbone of data-driven decision-making. However, building a successful data engineering team is no easy task. Without a data engineering culture, data teams are set up for failure: inefficient data systems, unfruitful data projects, trust lost with partners, and unhappy data engineers who will eventually leave for a better place.\n\nIn this session, we will talk about the roles of data engineers, why data engineering is critical to the success of data organizations, and how to build a winning data engineering culture that empowers both data engineers and partners.","location":"Breakout Room 2","speakers":[{"name":" Xinran Waibel ","title":" Senior Data Engineer ","company":" Netflix "}],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1136943","date":"September 27, 2023","time":"11:30 AM - 12:15 PM","title":"Getting Comfortable with Self-Advocacy","abstract":"Many people are shy of accepting praise or highlighting their professional achievements to the detriment of their own career success. In this talk, I'll highlight specific ways one can reframe negative perceptions of advocating for themselves - to become comfortable sharing their wins effectively.\n----\nThis talk will help attendees learn the importance of self-advocacy, and learn tangible ways they can get better at it!\nSelf-advocacy skills aren‚Äôt typically taught in educational institutions but are critical to career success. Advocating for yourself can result in higher pay, better jobs, promotions, and a host of other resources. Yet it is inherently hard, and not something most people feel comfortable doing.\nThis talk covers the ways to reframe the mental narrative that inhibits us from self-advocacy. I also cover specific ways of practicing and getting wins from self-advocacy. The best investment you can make towards your career is to get comfortable, confident, and genuine in advocating for yourself!\nTo recap the main takeaways from my talk:\n1) Learn more about what self-advocacy is, and why it might be hard for some people\n2) Reframe the internal narrative that inhibits us from advocating for ourselves\n3) Reframe the external narrative to practice our authentic voice in self-advocacy\n4) Hold yourself accountable with an easy-to-follow personal or group challenge!","location":"Meetup Hub (Expo Hall)","speakers":[{"name":" Shailvi Wakhlu ","title":" Head of Data ","company":" Ex-Strava, Ex-Komodo Health "}],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1136909","date":"September 27, 2023","time":"11:30 AM - 11:50 AM","title":"Put Events to Work and Respond in Real Time","abstract":"Events provide valuable knowledge about what‚Äôs happening across the business. By tapping into real-time streams of information, organizations can connect the dots between disparate events, detecting new trends, customer issues or competitive threats as they arise. To unlock this value, events have to be distributed from siloed data sources, event endpoints must be managed for secure access and reuse, and teams must be empowered to easily process events and define critical scenarios. Join me to learn how IBM Event Automation, a composable solution, puts your events to work by enabling both business and IT users to detect scenarios, act in real time, and automate decisions.","location":"Breakout Room 7","speakers":[{"name":" Alan Chatt ","title":" Product Manager for Event Driven Integration ","company":" IBM "}],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1136896","date":"September 27, 2023","time":"11:30 AM - 12:15 PM","title":"Query Your Streaming Data on Kafka using SQL: Why, How, and What","abstract":"Streaming data is rapidly becoming a key component in modern applications, and Apache Kafka has emerged as a popular and powerful platform for managing and processing these data streams. However, as the volume and complexity of streaming data continue to grow, it becomes increasingly critical to have efficient and effective ways of querying and analyzing this data.\n\nThis is where query engines like Apache Flink, Trino, Timeplus, Materialize, and ksqlDB come in. These powerful tools offer flexible and scalable ways of processing and analyzing streaming data in real-time, enabling users to extract valuable insights from their data streams.\n\nIn this talk, we will introduce the audience to the world of querying streaming data on Apache Kafka with SQL, compare and contrast the features and capabilities of each of these tools, and provide an in-depth analysis of their respective Pros and Cons. We will also discuss the best practices and scenarios where each tool is most effective.\n\nIn conclusion, query engines like Apache Flink, Trino, ksqlDB, Timeplus, Materialize and are useful tools in processing and analyzing streaming data on Kafka. With their ability to extract valuable insights from real-time data streams, these tools are a valuable asset for modern data-driven applications.","location":"Breakout Room 5","speakers":[{"name":" Gang Tao ","title":" CTO ","company":" Timeplus "},{"name":" Jove Zhong ","title":" Head of Product ","company":" Timeplus "}],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1136897","date":"September 27, 2023","time":"11:30 AM - 12:15 PM","title":"Robinhood‚Äôs Kafkaproxy: Decoupling Kafka Consumer Logic from Application Business Logic","abstract":"Apache Kafka is Robinhood‚Äôs most mission-critical infrastructure. It is used in every line of Robinhood‚Äôs business, from stock and crypto trading to self-clearing, market data and data science. Most of our microservices are written in either Golang or Python, and we have multiple librdkafka-based client library wrappers (kafkahood) developed in both languages. As Robinhood grows, it has become challenging for infrastructure engineers to manage the different requirements that the multitude of application teams have for their producers and consumers. To efficiently manage all Kafka consumers at Robinhood, we have developed a Consumer Proxy that decouples the following:\n\n‚óè Kafka consumer logic from message processing in application business logic\n\n‚óè Resource utilization in Kafka consumer from that in application container\n\n‚óè Kafka consumer failure from application failure\n\nThe Consumer Proxy is built in a Kubernetes sidecar container, it offers stateless processing, and it manages commits and timeouts. Most importantly, applications need the ability to scale up during market hours and scale down after extended trading hours, and the Consumer Proxy will allow them to easily rebalance consumer groups during these scaling events. Eventually, specific language library wrappers can remain a \"thin\" layer whose complexities are only to facilitate the business logic of the corresponding application team with what they should do with the payload.\n\nUnifying Kafka consumers and producers for an application team in one sidecar proxy container allows the infrastructure team to effectively manage clients at Robinhood. If your organization is looking to centralize Kafka consumption logic to a singular client library (instead of multiple different client libraries), please attend this talk to see how Robinhood does it so that the infrastructure team can focus development on a singular library rather than N different client libraries.","location":"Breakout Room 4","speakers":[{"name":" Tony Chen ","title":" Software Engineer ","company":" Robinhood "},{"name":" Mun Yong Jang ","title":" Software Engineer ","company":" Robinhood "}],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1211670","date":"September 27, 2023","time":"11:30 AM - 1:30 PM","title":"Streaming Innovation: How to Architect the Financial Services of the Future (Luncheon)","abstract":"Data streaming is fueling the next wave of emerging tech in the finance and banking industry. Join us for a lunch breakout session to learn how you can be the catalyst for change in your enterprise. You will hear from Confluent experts and financial services leaders on:\n‚Ä¢Gen AI for risk evaluation in financial services\n‚Ä¢Stream governance\n‚Ä¢Data streaming in practice with Fidelity Investments\n‚Ä¢Unlocking the potential of your high-fidelity data","location":"Grand Ballroom 220A","speakers":[{"name":" Andrew Sellers ","title":" Head of Technology Strategy ","company":" Confluent "},{"name":" Anna McDonald ","title":" Principal Customer Success Technical Architect ","company":" Confluent "}],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1136906","date":"September 27, 2023","time":"11:30 AM - 12:15 PM","title":"Transactions in Action: the Story of Exactly Once in Apache Kafka","abstract":"Transactions were added to Apache Kafka with KIP-98. While much of the protocol remains intact, transactions in Kafka have evolved over time to handle edge cases and errors found over the years. KIP-890 hopes to cover most of the remaining gaps in the protocol. This talk will give a refresher on transactions and idempotency and chronicle the various KIPs that improved the protocol over the years. We will also discuss the problem of hanging transactions and how KIP-890 hopes to solve it as well as strengthen the transactional protocol overall.","location":"Breakout Room 3","speakers":[{"name":" Justine Olshan ","title":" Software Engineer ","company":" Confluent "}],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1136898","date":"September 27, 2023","time":"11:30 AM - 12:15 PM","title":"Unleashing your Kafka Streams Application Metrics!","abstract":"Kafka Streams has a rich set of metrics for monitoring application health. Through these metrics, you can uncover performance issues, resource allocation concerns, and improve the performance of your application through deployment and configuration changes.\n\nProviding dashboards around all of these metrics can be rather challenging. In addition, the vast amount of metrics is extensive. Which metrics are important depends on the type of application you‚Äôre building. Let's uncover what you should be monitoring, why you should be monitoring it, and leave you with properly monitored Kafka Streams applications.\n\nNot only will you gain an understanding of task-id, sub-topology, and partition-id, but you will also see how to visualize that topology in a dashboard. Explore the new metrics added to Kafka Streams, since 3.0 was released, and go in-depth with the awesome end-to-end latency metrics. Finally, learn how to use these metrics to determine the number of instances an application needs when being deployed.\n\nUnleash your Kafka Stream Application metrics making it easier to run your applications effectively.","location":"Breakout Room 1 (Livestream)","speakers":[{"name":" Neil Buesing ","title":" Co-Founder, CTO ","company":" Kinetic Edge "}],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1136899","date":"September 27, 2023","time":"11:30 AM - 12:15 PM","title":"Why Serverless Flink Matters - Blazing Fast Stream Processing Made Scalable","abstract":"The shift from batch processing to real-time processing of data is accelerating. Building real-time data applications is a necessity for many businesses as customers expect data to be always up-to-date and their apps to react to changes as they happen. However building and productizing real-time applications is often a complex and lengthy process due to limited serverless options to build such apps.\n\nThe introduction of AWS lambdas was a watershed moment in the world of cloud computing. It allowed developers to fire up ‚Äúfully-managed‚Äù computer programs while paying for only when the program ran. Serverless compute comes with three big advantages - improved scalability, reduced cost, and increased flexibility. We‚Äôre bringing this same powerful paradigm to real time data processing with Flink in Confluent Cloud. Using this model, users can focus on writing business logic instead of managing nodes and other infrastructure.\n\nAttendees will learn the benefits of serverless and see how it fits into the context of stream processing. We‚Äôll then kick off a demo where we‚Äôll focus on a real world production use case that uses Flink jobs to power an application with extremely low latency.","location":"Breakout Room 6","speakers":[{"name":" Jean-Sebastien Brunner ","title":" Director, Product Management ","company":" Confluent "},{"name":" Mayank Juneja ","title":" Product Manager ","company":" Confluent "}],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1136901","date":"September 27, 2023","time":"12:00 PM - 12:20 PM","title":"From the Battlefield: Squeezing the Most From Your Kafka Infrastructure","abstract":"In the early stages, building your Kafka infrastructure is easy and cheap: its scope is small, and things are moving fast. Engineers spend more time building pipelines, teams spin up extravagant resources and you find yourself and others diverting routinely diverting their attention from your core business. In this talk, we‚Äôll explore the problems you‚Äôll experience from your Kafka infrastructure expanding and many clever solutions to mitigate them.","location":"Breakout Room 7","speakers":[{"name":" Stuart Mould ","title":" Product Owner ","company":" Conduktor "}],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1136900","date":"September 27, 2023","time":"12:30 PM - 1:30 PM","title":"Lunch","abstract":"Join us in Hall 3 to grab lunch, meet new friends, and relax before the afternoon sessions.\nWe are also offering casual Birds of a Feather tables to connect and learn with fellow attendees on a range of topics including:\nApache Nifi Use Cases\nAssessing Kafka Performance/Latency\nBuilding Data Pipelines with Kafka Connect\nContributing to Apache Flink\nDebugging Apache Flink\nDeploying Kafka Streams Applications\nKora: The Cloud-Native Engine for Apache Kafka\nMoving from Batch to Streaming\nMulti-Region HA/DR with Kafka","location":"Hall 3","speakers":[],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1136932","date":"September 27, 2023","time":"1:30 PM - 2:15 PM","title":"üé∂üéµBo-stream-ian Rhapsody: A Musical Demo of Kafka Connect and Kafka Streams üéµüé∂","abstract":"You‚Äôve heard of Apache Kafka. You know that real-time event streaming can be a powerful tool to power your project, product, or even company. But beyond storing and relaying messages, what can Kafka do?\n\nIn this talk, get an overview of two key components of the Kafka ecosystem beyond just brokers and clients: Kafka Connect, a distributed ingest/export framework, and Kafka Streams, a distributed stream processing library. Learn about the APIs available for developing and deploying a custom source and sink connector, and for bringing up a Streams application to manipulate the data in between them.\n\nThrough a musical demonstration involving Kafka Connect and Kafka Streams, audio will be recorded, distorted, analyzed, and played back‚Äìlive and in real time.\n\nAudience members should expect to come away with a good understanding of how to develop Kafka Connect connectors and Kafka Streams applications, as well as some basics of digital signal processing.","location":"Breakout Room 1 (Livestream)","speakers":[{"name":" Chris Egerton ","title":" Staff Apache Kafka Open Source Developer ","company":" Aiven "}],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1136911","date":"September 27, 2023","time":"1:30 PM - 2:15 PM","title":"Beyond Monoliths: Thrivent‚Äôs Lessons in Building a Modern Integration Architecture","abstract":"Like most financial service firms, Thrivent is in the midst of transformation. They are breaking apart monoliths to create a more distributed, flexible, data architecture. The challenges faced span equally across technology as well as human behavior. In this talk, we will showcase strategies, patterns, and techniques that were developed during an effort to abstract (and strangle) 20+ internal systems into a unified data integration platform.\n\nFrom a technical perspective, we will highlight:\n\n‚û° Why we chose JSON Schema over Avro for Data Contracts.\n\n‚û° How to automate the addition of OpenTelemetry into applications to increase quality and as a result, trust.\n\n‚û° Creating automated governance with Terraform and Sentinel that does not limit developer independence.\n\nAnd from the human side of things, you will hear about:\n\n‚û° Why \"build it and they will come\" is a fallacy and adoption boils down to culture and a product-focused mindset.\n\n‚û° Strategies for building trust and creating strong team brands.\n\n‚û° Using Conway's Law as a tool in your team-building process.\n\nYou will learn from Thrivent's successes and failures, and leave equipped with practical solutions for building (or improving) your own data modernization journey.","location":"Breakout Room 4","speakers":[{"name":" Andrew Kolb ","title":" Senior Architect ","company":" Thrivent Financial "},{"name":" Matt Schroeder ","title":" Director of Technology, Real-Time Data ","company":" Improving "}],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1211671","date":"September 27, 2023","time":"1:30 PM - 3:00 PM","title":"Cracking the Code: Tackling Silicon Valley's Homelessness Together w/ Destination Home","abstract":"Homelessness takes many shapes and forms, and is the byproduct of several intertwined problems in our society. By taking the time to truly understand this complex issue, we all can play a role in ending homelessness in our community. ¬†Step into the heart of compassion and join us for a transformative evening as we partner with Destination Home for an insightful homeless presentation. As a testament to our commitment, we're hosting a volunteer kit-making event during this session, crafting essential move-in kits to support those transitioning into housing. To amplify our impact, we invite you to be a part of our sock drive donation by bringing new packages of adult socks, drop them off during this presentation. Your contribution will weave warmth and hope into the lives of the homeless in Silicon Valley, showcasing the remarkable change we can enact when we stand together.","location":"Spotlight Theater (Expo Hall)","speakers":[],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1183834","date":"September 27, 2023","time":"1:30 PM - 2:15 PM","title":"Exactly-Once Semantics Revisited: Distributed Transactions across Flink and Kafka","abstract":"Apache Flink‚Äôs Exactly-Once Semantics (EOS) integration for writing to Apache Kafka has several pitfalls, due mostly to the fact that the Kafka transaction protocol was not originally designed with distributed transactions in mind. The integration uses Java reflection hacks as a workaround, and the solution can still result in data loss under certain scenarios. Can we do better?\nIn this session, you‚Äôll see how the Flink and Kafka communities are uniting to tackle these long-standing technical debts. We‚Äôll introduce the basics of how Flink achieves EOS with external systems and explore the common hurdles that are encountered when implementing distributed transactions. Then we‚Äôll dive into the details of the proposed changes to both the Kafka transaction protocol and Flink transaction coordination that seek to provide a more robust integration.\nBy the end of the talk, you‚Äôll know the unique challenges of EOS with Flink and Kafka and the improvements you can expect across both projects.","location":"Breakout Room 3","speakers":[{"name":" Alexander Sorokoumov ","title":" Staff Software Engineer ","company":" Confluent "},{"name":" Tzu-Li (Gordon) Tai ","title":" Staff Software Engineer ","company":" Confluent "}],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1136902","date":"September 27, 2023","time":"1:30 PM - 2:15 PM","title":"From Raw Data to an Interactive Data App in an Hour: Powered by Snowpark Python","abstract":"As data practitioners, we often rely on the data engineering teams upstream to deliver the right data needed to train ML models at scale. Deploying these ML models as a data application to downstream business users is constrained by one‚Äôs web development experience. Using Snowpark, you can build end to end data pipelines, and data applications from scratch using Python.\nIn this talk, you will learn to build a Streamlit data application to help visualize the ROI of different advertising spends of an example organization.\n- Setup Environment: Use stages and tables to ingest and organize raw data from S3 into Snowflake.\n- Data Engineering: Leverage Snowpark for Python DataFrames to perform data transformations such as group by, aggregate, pivot, and join to prep the data for downstream applications.\n- Data Pipelines: Use Snowflake Tasks to turn your data pipeline code into operational pipelines with integrated monitoring.\n- Machine Learning: Prepare data and run ML Training in Snowflake using Snowpark ML and deploy the model as a Snowpark User-Defined-Function (UDF).\n- Streamlit Application: Build an interactive application using Python (no web development experience required) to help visualize the ROI of different advertising spend budgets.","location":"Breakout Room 6","speakers":[{"name":" Vino Duraisamy ","title":" Developer Advocate ","company":" Snowflake  "}],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1136837","date":"September 27, 2023","time":"1:30 PM - 2:15 PM","title":"Restate: Stream Processing, but for Microservices","abstract":"Developing applications in a microservice architecture (MSA) is hard. For all the benefits that MSAs bring over monolithic application architectures, they expose developers to all sorts of tough distributed systems problems, making it non-trivial to build applications that are consistent, scalable, and resilient.\n\nBuilding applications in an event-driven way is a great start to get MSA complexity under control, by leveraging the guarantees of logs/message queues (like Kafka) to offload some of the problems. Stream processing systems (KStreams, Flink, etc.) can be powerful tools as well. Their tight integration of state, messaging, and application logic yields strong guarantees, like exactly-once state & event-delivery semantics. However, having been built originally for data processing, those systems cannot easily express business logic with complex control-flow, orchestration tasks, or workflows. Existing systems also have very different operational characteristics compared to stateless service deployments.\n\nIn this session, we share ideas from a novel system we are developing, called 'Restate'. Our work is inspired by event-sourcing and stream processing systems, but rethought from the ground up for microservices. Restate connects services with each other (and between services and Kafka), and handles crucial aspects like state, communication, failover, and consistency. It does that with a programming model that is crafted not for data processing or analytics, but for services and applications with complex interactions and control flow. The result is an elegant experience of writing applications that are simple, consistent, yet highly resilient and scalable.\n\nWe believe that Restate complements the current landscape of event-processing systems, by offering a solution for cases where events facilitate control flow and coordination, rather than real-time data processing.","location":"Breakout Room 5","speakers":[{"name":" Stephan Ewen ","title":" Founder & CEO ","company":" Restate "},{"name":" Giselle van Dongen ","title":" Developer Advocate ","company":" Restate "}],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1180215","date":"September 27, 2023","time":"1:30 PM - 3:15 PM","title":"Schema Linking on Confluent Cloud (Lecture & Lab)","abstract":"In a real-time event-based system, it is vital to use the Schema registry for the best performance. Schema Linking keeps schemas in sync across multiple Schema Registry clusters. Schema Linking can be used in conjunction with Cluster Linking to keep both schemas and topic data in sync across the clusters.\nThis session will be broken into two parts: Lecture & Lab with a 15 minute break.\nWithin the lecture, we will discuss how the Confluent Schema Registry performs data validation, compatibility check and allows data in motion to evolve. You will also learn how to configure and run the Schema Linking to replicate your schemas between Schema Registry clusters.\nFollowing the lecture will be a demo and hands-on* lab, where we will use Schema Registry in Confluent Cloud to create schemas, set-up Schema Linking using an exporter to sync schemas between Schema Registry clusters, and try modifying its configuration to witness how it will affect the data at destination.\n* Bring your own laptop to work on hands-on lab or just enjoy the demo of the hands-on lab.","location":"Meeting Room 212","speakers":[{"name":" Antonio Loma Daza ","title":" Sr. Technical Trainer ","company":" Confluent "}],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1136903","date":"September 27, 2023","time":"1:30 PM - 2:15 PM","title":"Sustainability & Streaming Data: Merging Real-Time Insights, Green Futures & Profitability","abstract":"In the face of global climate change and limited natural resources, sustainability has become an urgent priority for individuals and organizations. Streaming data applications can play a critical role in addressing this challenge by enabling real-time visibility and contextual meaning of environmental data to make more sustainable and cost-saving decisions quickly. Nstream Founder and CTO, Chis Sachs understands that the intersection of sustainability and technology, including the use of streaming data applications, has the potential to benefit the bottom line and the planet. Chris will explore how organizations can leverage real-time insights to reduce waste, conserve resources, lower carbon footprints, and reduce operational expenses. He will also discuss the ethical considerations around collecting and using environmental data and how to ensure that streaming data applications are deployed sustainably and responsibly without impacting profitability.","location":"Breakout Room 2","speakers":[{"name":" Chris Sachs ","title":" Founder & CTO ","company":" Nstream "}],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1136914","date":"September 27, 2023","time":"1:30 PM - 2:15 PM","title":"The Trifecta of Real-Time Applications: Apache Kafka, Flink, and Druid","abstract":"The data coming into Kafka is fresh and hot. And you can deliver a new level of operational visibility and intelligence fueling applications with it. But streaming data is no longer real-time when the sink is batch. So the challenge is processing it and analyzing it at scale and extracting those insights - before they go stale.\nSo what‚Äôs the right architecture? Should you ingest streams into a data warehouse or data lake? Maybe use a stream processor or a database? Engineering teams love using Apache Flink, but they also love using Apache Druid, a popular real-time analytics database used by 1000s of companies like Confluent and Netflix. Do you need Flink and Druid? When does it make sense vs when does it not?\nJoin this session to learn about Apache Druid and why companies use it in combination with Kafka and Flink for real-time applications. Learn how Apache Druid complements Flink and Kafka - and what makes it purpose-built for analyzing streams and events. This talk shows real-world examples from companies that use Apache Druid with Kafka and Flink in production today and the best-practices that every dev can take advantage of.","location":"Breakout Room 7","speakers":[{"name":" Gian Merlino ","title":" Co-founder and CTO ","company":" Imply "},{"name":" Kai Waehner ","title":" Field CTO ","company":" Confluent "}],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1136910","date":"September 27, 2023","time":"2:30 PM - 3:15 PM","title":"A Glide, Skip or a Jump: Efficiently Stream Data into Your Medallion Architecture with Apache Hudi","abstract":"The medallion architecture graduates raw data sitting in operational systems into a set of refined tables in a series of stages, ultimately processing data to serve analytics from gold tables. While there is a deep desire to build this architecture incrementally from streaming data sources like Kafka, it is very challenging with current technologies available on lakehouses; a lot of technologies can‚Äôt efficiently update records or efficiently process incremental data without recomputing all the data to serve low-latency tables. Apache Hudi is a transactional data lake platform with full mutability support, including streaming upserts, and provides a powerful incremental processing framework. Apache Hudi powers the largest transactional data lakes in the industry, differentiating on fast upserts and change streams to only process and serve the change records.\n\nTo further improve the upsert performance, Hudi now supports a new record-level index that deterministically maps the record key to the file location orders of magnitude faster. As a result, Hudi speeds up computationally expensive MERGE operations even more by avoiding full table scans. On the query side, Hudi now supports database-style change data capture with before, and after images to chain flow of inserts, updates and deletes change records from bronze to silver to gold tables.\n\nIn this talk, attendees will walk away with:\n\n- The current challenges of building a medallion architecture at low-latency\n\n- How the record index and incremental updates work with Apache Hudi\n\n- How the new Hudi CDC feature unlocks incremental processing on the lake\n\n- How you can efficiently build a medallion architecture by avoiding expensive operations","location":"Breakout Room 2","speakers":[{"name":" Nadine Farah ","title":" Head of Dev Rel ","company":" Onehouse "},{"name":" Ethan Guo ","title":" Database Engineer ","company":" Onehouse "}],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1136904","date":"September 27, 2023","time":"2:30 PM - 3:15 PM","title":"Dynamic Change Data Capture with Flink CDC and Consistent Hashing","abstract":"Change Data Capture (CDC) is a popular technique for extracting data from databases in realtime. However, many CDC deployments are static: e.g. a single connector is configured to ingest data for one or several tables.\n\nAt Goldsky, we needed a way to configure CDC for a large Postgres database dynamically: the list of tables to ingest is driven by customer-facing features and is constantly changing.\n\nWe started using Flink CDC connectors built on top of the Debezium project, but we immediately faced many challenges caused mainly by the lack of incremental snapshotting.\n\nBut even after implementing incremental snapshotting ourselves, we still faced an issue around using replication slots in Postgres: we couldn't use a single connector to ingest all tables (it's just too much data), and we couldn't create a new connector for every new set of tables (we'd quickly run out of replication slots). So we needed to find a way to maintain a fixed number of replication slots for a dynamic list of tables.\n\nIn the end, we chose a consistent hashing algorithm to distribute the list of tables across multiple Flink jobs. The jobs also required some customizations to support the incremental snapshotting semantics from Flink CDC.\n\nWe learned a lot about Debezium, Flink CDC and Postgres replication, and we're excited to share our learnings with the community!","location":"Breakout Room 3","speakers":[{"name":" Xiao Meng ","title":" Software Engineer ","company":" Goldsky "},{"name":" Yaroslav Tkachenko ","title":" Principal Software Engineer ","company":" Goldsky "}],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1136912","date":"September 27, 2023","time":"2:30 PM - 3:15 PM","title":"Go Big or Go Home: Approaching Kafka Replication at Scale","abstract":"Processing a lot of data with Kafka means knowing how and when to scale horizontally and vertically. When you‚Äôve exhausted the boundaries of scaling inside a single cluster, replication becomes critical but sometimes standard replication is not enough.\n\nNew Relic once earned the dubious title of ‚ÄúWorld‚Äôs Largest Kafka Cluster‚Äù, and in our journey to break this cluster into dozens of smaller clusters, we needed to route events between clusters and topics based on headers.\n\nAt the time, this meant we had to do it ourselves. Starting out, our goal was fan out (one-to-many) replication. Since then our needs have expanded to include many-to-one and many-to-many replication.\n\nIn this talk we'll discuss what bottlenecks we have hit as we scaled out, and what measures we took to remove them, such as:\n\n- Replicating data based on Kafka Headers\n\n- Connecting to many source and destination Kafka clusters\n\n- Managing the replication of Kafka topics of varying traffic\n\n- The use of an intermediary Kafka cluster\n\nAt the end of this talk you will understand how we have scaled replication and routing to support New Relic's ever growing data ingestion, and all the mitigations it took to get us there.","location":"Breakout Room 4","speakers":[{"name":" Julia Holgado ","title":" Software Engineer ","company":" New Relic "}],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1136913","date":"September 27, 2023","time":"2:30 PM - 3:15 PM","title":"Kafkastrophies: What CashApp has Learned From Solving Kafka Related Incidents in Production","abstract":"CashApp is a popular platform that handles the transfer of money, stock, and Bitcoin for millions of users every year. Our eventing system, built on top of Kafka, plays a crucial role in this process, moving hundreds of thousands of events per second across thousands of topics with a total consumer lag approaching 4 billion events. However, as with any system operating at scale, issues can arise, and in this talk, we'll be discussing some of the most interesting incidents we've experienced and how we resolved them.\n\nWe'll delve into some of the high-severity incidents that CashApp has encountered, where Kafka played a significant role, for better or for worse. We'll provide examples of problems ranging from configuration-induced data loss to thundering herds and poison events. For each incident, we'll discuss how the team was alerted, how they identified and resolved the root causes, and what new tools and processes were implemented to prevent similar incidents from occurring again.\n\nBy attending this talk, attendees will gain valuable insights into the challenges of running eventing systems at scale. They will learn about potential problems that can occur in their own eventing systems and production-tested debugging techniques. Finally, we'll provide advice on how to prevent similar incidents from happening in the future. Join us to hear real-world cautionary tales and learn from our experiences.","location":"Breakout Room 6","speakers":[{"name":" Hamdan Javeed ","title":" Software Engineer ","company":" Block, Inc. "},{"name":" David Purcell ","title":" Senior Software Engineer ","company":" Block Inc "}],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1136892","date":"September 27, 2023","time":"2:30 PM - 3:15 PM","title":"Moving Data at Enterprise Scale","abstract":"Modernizing Government Healthcare IT is a journey through decades of system implementation based on the best in class technology available at the time. What‚Äôs left behind is business logic buried in the programming language de jour or maybe even a long forgotten low code application environment and data, tons of data stored in the dreaded ‚Äúsilo‚Äù. Movement of data within the system was at best backed by a data persistence technology (VSAM, RDBMS, etc.) and at worst flat files or an Office application. Rare is the opportunity to stand up a greenfield project and just forget about the old and focus on building the new because there is more often than not value in the data contained in the silos. Fast forward to the present with an eye on the future and we as an IT industry are in the golden age of technology and software architecture that supports the movement of data. Not just within the scope of a single system but at enterprise scale. This session will focus on the movement of data at enterprise scale by establishing a foundation that normalizes how systems define, discover, manage and exchange information. In scope are Data Mesh, Data Fabric, DataOps, Event Streaming, Data Exchange, etc.","location":"Meetup Hub (Expo Hall)","speakers":[{"name":" Joe Chavez ","title":" Enterprise Architect ","company":" Cognitive Software Services LLC "}],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1136895","date":"September 27, 2023","time":"2:30 PM - 2:50 PM","title":"Optimize Costs and Scale Your Streaming Applications with Virtually Unlimited Storage from AWS Services","abstract":"Apache Kafka is well-positioned to support different use cases including unified batch and stream processing, ML training with historic data or recomputing results with streaming data when application logic changes. These use cases often require storing data for several weeks, months, or even years. Data is retained in the Kafka cluster as long as required by configuring the retention policy. However, retaining data in a Kafka cluster can become expensive because storage and compute are tightly coupled in a cluster.\nIn this session you will learn how AWS streaming data services can power your streaming applications that can scale to virtually unlimited storage with Tiered Storage. Discover how AWS services collaborate to address diverse event driven architectures, Change Data Capture (CDC) applications and real-time analytics use cases. Unleash the potential of AWS‚Äô cost efficient storage tiering for long term data retention, agile decision-making and proactive data strategies.","location":"Breakout Room 7","speakers":[{"name":" Todd McGrath ","title":" Principal Streaming Solution Architect ","company":" AWS "},{"name":" Vidhi Taneja ","title":" Principal Product Mgmt - Tech ","company":" Amazon "}],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1136915","date":"September 27, 2023","time":"2:30 PM - 3:15 PM","title":"Streaming is a Detail","abstract":"We can all agree that streaming is super cool. And for a while now, the adoption conversation has been largely led with an all-in mentality. But that‚Äôs silly. The only concerns end users have are:\n\n-The freshness of their data\n\n-Latency they require to meet their SLAs from source to consumption\n\n-All while maintaining data quality and governance.\n\nLuckily, the industry has realized this and we have seen a shift of streaming capabilities surfacing as an in-database technology, via objects as trivial to analytics engineers as views - materialized that is. With this convergence of streaming capabilities and batch level accessibility, this is when ELT tools like dbt can join in and expand out the adoption story.\n\ndbt is the T in ELT, Extract Load and Transform. In dbt, analytics engineers design models - SQL (and occasional python) statements that encapsulate business logic. At runtime, dbt will wrap that logic in a DDL statement and send it over to the data platform to execute.\n\nIn this session, we‚Äôll discuss how we see streaming at dbt Labs. We will dive into how we are extending dbt to support low-latency scenarios and the recent additions we have made to make batch and streaming allies in a DAG rather than archenemies.","location":"Breakout Room 5","speakers":[{"name":" Amy Chen ","title":" Partner Engineering Manager ","company":" dbt labs "},{"name":" Florian Eiden ","title":" Product Manager ","company":" dbt Labs "}],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1136916","date":"September 27, 2023","time":"2:30 PM - 3:15 PM","title":"What's in store? Part Deux; Creating Custom Queries with Kafka Streams IQv2","abstract":"Kafka Streams is a powerful stream-processing library that offers stateful operations. ¬†Along with the stateful operations is a unique feature of Kafka Streams, Interactive Queries, or IQ. ¬†IQ allows you to leverage the application's state from the outside by directly querying the state stores.\nIQ has always been a part of Kafka Streams, but the original version lacked some key features. ¬†Namely, the ability to query the state of a custom state store. ¬†With the addition of IQ v2, that has now changed. ¬†Additionally, IQ v2 brings other benefits like pushing queries down to the store level making querying more efficient.\nIn this talk, I will cover:\n* Building an Interactive Query service including routing queries between app instances\n* Creating custom queries using Interactive Query v2\n* Testing your IQ Application\nAttending this talk will teach you how to use IQ v2 to create queries more aligned with your unique business needs.","location":"Breakout Room 1 (Livestream)","speakers":[{"name":" Bill Bejeck ","title":" DevX Engineer ","company":" Confluent "}],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1211673","date":"September 27, 2023","time":"3:00 PM - 3:20 PM","title":"Accelerating Path to Production for Generative AI-powered Applications","abstract":"In this session, we will discuss some recent developments in Generative AI and how those can be leveraged to build intelligent applications. Learn how to bring the power of large language models (LLMs) to your private, real-time operational data across multiple data types. We will talk about improving the accuracy of LLMs in your applications by leveraging Retrieval Augmented Generation, which provides proprietary knowledge to the LLM.\nFrom real-time responses to sophisticated interactions, learn how you can easily build a range of AI-driven experiences that leverage your operational data with minimal complexity.\nMongoDB Atlas provides native vector search capabilities and a flexible document model all within an enterprise-ready developer data platform empowering teams to iterate quickly on applications enriched with generative AI. Coupling Atlas with Confluent makes it easier to leverage streaming data when informing LLMs with proprietary data.","location":"Breakout Room 7","speakers":[{"name":" Prakul Agarwal ","title":" Senior Product Manager, Machine Learning ","company":" MongoDB "},{"name":" David Macias ","title":" Lead, Product Marketing ","company":" MongoDB "}],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1180212","date":"September 27, 2023","time":"3:00 PM - 4:30 PM","title":"Build A Gaming Analytics Pipeline: An AWS GameDay","abstract":"Join us for a Confluent and Amazon Web Services (AWS) GameDay, a gamified workshop where you will build an event-driven, data-streaming pipeline to detect cheating, identify toxic chat, ban players, and update matchmaking ranking in real time. Participants will get hands-on experience with services such as AWS Lambda, Amazon GameLift, Amazon DynamoDB, and Confluent Cloud. Please bring a laptop and prepare yourself to learn, have fun, win swag, and herd Unicorns. AWS and Confluent Cloud accounts will be provided for the session.","location":"Meeting Room 211A","speakers":[],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1211672","date":"September 27, 2023","time":"3:00 PM - 4:00 PM","title":"The Art of Storytelling: From Startup to Transformation (Fireside Chat)","abstract":"Stories are one of the most powerful tools you can use to engage and connect with your audience. The power of a single story goes far beyond simply relaying facts and data and can be a highly effective tool tocreate customer loyalty. Stories emotionalize information. They give color and depth to otherwise bland material and they allow people to connect with the message in a deeper, more meaningful way. Those potential customers/investors can then connect with your product, service and your entire business in a way that will make you talk about differently. In this session we will look at four key stories that every business should be telling as well as what the elements that make your story stand out. Stories are extremely important and necessary in the world of business, but all stories are not treated equally. How do you identify the right story for your business? Connect with your audience emotionally? Therefore making your business and product come alive for your customer.","location":"Spotlight Theater (Expo Hall)","speakers":[{"name":" TyKeshia Spells ","title":" Director, Content Strategy ","company":" Stand Together "}],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1136918","date":"September 27, 2023","time":"3:30 PM - 3:40 PM","title":"Accurately Backtesting Real-time Streaming Features","abstract":"Tools like Flink and Kafka Streams allow us to do machine learning with real-time features. For example, if we have user logs coming off a Kafka topic, we can perform windowed joins and aggregations in real-time to predict if a user is going to click on an advertisement, or is committing fraud. We can come up with a multitude of these features and store them in Redis, which can feed a ML prediction service.\n\nHow do we decide which real-time feature to use? We could do A/B tests on live data. This is expensive and will take a long time to generate statistically significant results.\n\nPeople typically dump historical Kafka partitions in cheap cloud storage. Can we use this historical data to \"backtest\" our features? This could allow data scientists to use batch analytics systems like SparkSQL and BigQuery to rapidly iterate on real-time features.\n\nThe key question to answer: given a Flink job definition of a real-time feature, the historical Kafka data and an exact point in time in history, can we confidently say what the feature value would have been in Redis?\n\nIn this talk, I will demonstrate that it is impossible to accurately answer this question. Approaches such as Flink's batch mode are not able to accurately handle late events and indicate when in processing time a window closed. In fact, they practically guarantee feature leakage.\n\nThe key reason behind the challenge is that historical Kafka records only contains Kafka producer timestamps (event time), and does not contain the processing time. This makes it extremely challenging to say with certainty what records would have made it into a particular window, and when exactly in processing time an event time window fires based on watermarks.\n\nI will then talk about assumptions we can make to get \"good enough\" backfill results that try to avoid feature leakage. The best approach likely depends on your particular Kafka/Flink setup.","location":"Breakout Room 1 (Livestream)","speakers":[{"name":" Tony Wang ","title":" PhD Student ","company":" Stanford "}],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1136919","date":"September 27, 2023","time":"3:30 PM - 3:40 PM","title":"CI/CD patterns for dbt Projects","abstract":"Remember how you used to test SQL changes in production? More than changing the way we build data pipelines, dbt also provides some lesser-known features that help you manage their deployment.\n\nIf you‚Äôve ever wondered how to integrate dbt into your CI/CD processes (think automatic project linting, spinning up testing environments, parsing the manifest file), this session is for you!\n\nWe‚Äôll work our way through a template CI/CD workflow using GitHub Actions, and learn some dbt tricks (like using state and defer that allow you to slim down the whole process).","location":"Breakout Room 5","speakers":[{"name":" Marta Paes ","title":" Head of Developer Experience ","company":" Materialize "}],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1136920","date":"September 27, 2023","time":"3:30 PM - 3:40 PM","title":"Fluvii: A Lightweight Kafka Streams Client for Python","abstract":"Fluvii is a new open-sourced Python-based Kafka Streams client library written by Red Hat and built on top of confluent-kafka-python. It's focused on making business-level processing with complex data models as intuitive and lightweight as possible.\n\nDespite the growing popularity of Kafka and Python being one of the largest programming languages, its Kafka client options are disproportionately limited, especially streams-like libraries. Though Faust exists, it can feel fairly heavy for various use cases, especially in a cloud-native environment.\n\nFluvii was written to not only fill this Python void, but also appeal to any users who need to write and maintain complex Kafka ETL workloads by providing topic-backed tabling and fully automated transaction handling (aka exactly-once guarantees).\n\nFluvii's streamlined interface that will feel intuitive for veterans and inviting to newcomers; its simplicity makes for an excellent entry point into Kafka, yet the extensibility makes for quick iteration and robust customization when you're ready for it!\n\nIn this talk, we will briefly explore Fluvii via its feature set and some simple examples so that you can confidently get started with it!","location":"Breakout Room 3","speakers":[{"name":" Tim Sawicki ","title":" Python SDK Engineer ","company":" Quix "}],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1136925","date":"September 27, 2023","time":"3:30 PM - 3:45 PM","title":"From Zero to Hero: Sharing Huge Amounts of Streaming Data with Open Source Delta Sharing","abstract":"This lightning talk is an introduction to Delta Sharing; A Linux Foundation open source solution for sharing massive amounts of data in a cheap, secure, scalable and *streaming* way.\nHomegrown data-sharing solutions based on SFTP or APIs aren‚Äôt scalable and saddle you with operational overhead. Off-the-shelf data-sharing solutions only work on specific sharing networks, promoting vendor lock-in and can be costly. Others don't support streaming data.\nDelta Sharing reliably accesses data at the bandwidth of modern cloud object stores, such as S3, ADLS, or GCS.\nAny client supporting pandas, Apache Spark‚Ñ¢, or Python, as well as commercial clients such as Power BI can connect to the sharing server. Clients always read the latest version of the data which can also be partitioned to limit the amount of data transferred.\nLearn what you need to know about data sharing in 2023 in this lightning talk.","location":"Meetup Hub (Expo Hall)","speakers":[{"name":" Frank Munz ","title":" Principal TM Engineer ","company":" Databricks "}],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1136921","date":"September 27, 2023","time":"3:30 PM - 3:40 PM","title":"Kafka Latency Analyzer: Get Insights into Per-record, End-to-end Latency","abstract":"When testing client performance, an important benchmark is per-record latency. This includes ingestion latency (ingestion timestamp - event timestamp), consumer latency (consumer wallclock timestamp - ingestion time);\nWhilst there are tools like kafka.tools.TestEndToEndLatency for running such tests with producer/consumer primitives, testing a higher order producer like Kafka Connect source connector is a challenge. Many users rely on manual, ad-hoc observations for benchmarking how long it took for a source connector to write a record into Kafka.\nIn this session, we shall cover kafka-latency-analyzer, a script that can use configurable timestamps to produce quantile reports of latency (focused on average and tail) at a topic level and optionally relay such results downstream to another kafka topic for analysis ans also includes features such as sampling; This approach is useful for providing latency SLAs, as well as for performing exploratory benchmarks & tuning of clients/connectors;\nWhat we shall cover:\n1. Understanding ingestion latency: CreateTime vs LogAppendTime (and why additional timestamps need to be injected for true event time in sources)\n2. Understanding other connector bound latencies involved (such as SMT chains with connectors)\n3. Understanding consumer / processing latency, as seen by a consumer: ie, WallClockTime (although we shall also consider how to introduce timestamp injection for sink-side write)\n4. Analysis of results: Optimization based on latency distribution (mean and tail quantiles: p90, p95, p99)\n5. Managing results of test runs with your observability tooling: OpenMetrics, Prometheus, Datadog integration & more","location":"Breakout Room 4","speakers":[{"name":" Pavan Keshavamurthy ","title":" CTO ","company":" Platformatory "}],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1136922","date":"September 27, 2023","time":"3:30 PM - 3:40 PM","title":"Seek and Destroy Kafka Under Replication","abstract":"It's important that even under load, Apache Kafka ensures user topics are fully replicated in synch.\nReplication is essential to endure resilience to data loss, so both users and operators care about it.\n\nIf a topic partition falls out of the ISR (In-Synch-replicas) set, a user experiences unavailability (when producing with the default acknowledgment setting).\n\nUsers may use non-default acks mode to work around it, but the effect on a Kafka cluster is to make the under-replication worse.\n\nEven simple Under replication with no Under Min Isr is to be avoided as a cluster update may cause the dreaded Under Min ISR.\n\nThere are a number of settings that can be used, from quotas to number of replication threads to more low-level settings.\n\nThis session wants to show how we successfully measured and evolved our Kafkas configuration, with the goal of giving the best possible user experience (and resilience to their data).\n\nHofstadter's Law applied!\n\n\"It always takes longer than you expect, even when you take into account Hofstadter's Law.\"","location":"Breakout Room 2","speakers":[{"name":" Edoardo Comar ","title":" Software Developer ","company":" IBM "}],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1136923","date":"September 27, 2023","time":"3:30 PM - 3:40 PM","title":"Six Different Things You Can Do In Kafka With Geo-Replication","abstract":"Move it, share it, bridge it, stage it, backup it, optimize it, bop it. Did you know you can do these things with geo-replication in Kafka? (well, except bop it)\n\nKafka can stream data in real time between different clusters, regions, cloud environments (‚Äúgeo-replication‚Äù) using tools like Apache Kafka¬Æ MirrorMaker 2 and Confluent Cluster Linking. Come hear six totally different things that companies around the world have used these tools for:\n\n- Optimizing the latency and cost of their data streaming applications\n\n- Sharing real-time data feeds with other departments, and even other companies\n\n- Promoting workloads from staging environments to production\n\n- Bridging the gap between the disconnected edge and the cloud\n\n- Backing up their data for Disaster Recovery\n\n- Moving from managing their own Kafka cluster to using a SaaS cluster","location":"Breakout Room 6","speakers":[{"name":" Luke Knepper ","title":" Product Manager for Global Kafka ","company":" Confluent "}],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1211679","date":"September 27, 2023","time":"3:30 PM - 3:50 PM","title":"Striim: Unifying Change Data Capture, AI/ML, and Exactly-once Processing in a Managed Scalable, Streaming Platform","abstract":"Learn how Striim architected and manages a unified data streaming platform optimized for fast deployment of event stream delivery and processing pipelines that deliver real-time analytics and AI for business use cases. Companies of all sizes are able to adopt data streaming in a matter of weeks with Striim‚Äôs built-in log-based change data capture, semantics for recovery with exactly once processing, and horizontal and vertical scalability with a click of a button.","location":"Breakout Room 5","speakers":[{"name":" Varun Verma ","title":" Vice President of Development, Striim Cloud ","company":" Striim "}],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1136924","date":"September 27, 2023","time":"3:45 PM - 4:00 PM","title":"Break","abstract":"N/A","location":"","speakers":[],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1136926","date":"September 27, 2023","time":"4:00 PM - 4:20 PM","title":"ClickPipes -  Seamlessly connect Kafka to ClickHouse Cloud","abstract":"ClickPipes is a managed integration platform for ClickHouse Cloud that makes ingesting data from a diverse set of sources as simple as clicking a few buttons. Designed for the most demanding workloads, ClickPipes's robust and scalable architecture ensures consistent performance and reliability.","location":"Breakout Room 7","speakers":[{"name":" Ryadh Dahimene ","title":" Integrations ","company":" ClickHouse "}],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1136927","date":"September 27, 2023","time":"4:00 PM - 4:45 PM","title":"Configuring Kafka Connect To Be Successful At Scale","abstract":"With Kafka Connect providing the ability to connect numerous external systems without having to write custom code it can be an integral part of the pipeline for putting data in motion. However, there are some settings and practices that can lead to a better experience if you are an admin of a Kafka Connect cluster as more teams add additional connectors for other use cases or more load is added to the Kafka Connect cluster overall.\n\nThis presentation will look at the best practices to configure Kafka Connect to output important data when evaluating if more resources are needed for a Kafka Connect worker or if a new node should be added to the cluster overall.\n\nAreas of interest to be discussed:\n\n* What memory areas make up a Kafka Connect process (JVM process)?\n\n* What is verbose gc and why should it be enabled?\n\n* Is Kafka Connect configured to give me the specific information for my connectors and their respective tasks?\n\n* Which connect.protocol should I be using in environment?","location":"Breakout Room 1 (Livestream)","speakers":[{"name":" Travis Sweet ","title":" Senior Technical Support Engineer ","company":" Confluent "}],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1136939","date":"September 27, 2023","time":"4:00 PM - 4:45 PM","title":"Forecasting Kafka Lag Issues with Machine Learning","abstract":"A key operational challenge for running Kafka in production is managing Kafka Partition Lag. Kafka partitions exhibit a variety of normal trends, influenced by how consumers consume data in partitions. Kafka Lag also exhibits abnormal patterns caused by issues in the Kafka clusters or in its consumers. Administrators need to monitor Kafka Lag, distinguish between normal and abnormal trends and act when application outcomes are impacted. Lag impacts latency and accuracy of data and insights produced from a Big Data pipeline. How can we continuously monitor Kafka Lag automatically, identify normal and abnormal trends and forecast Lag issues ahead of time?\n\nIn this session, we will discuss our work in this regard using machine learning. We will discuss popular lag patterns and how our ensemble forecasting system learns from the past and predicts future trends. We will also showcase some case studies and benefits of having such a system as part of a Kafka observability platform.","location":"Breakout Room 4","speakers":[{"name":" Kumaran Ponnambalam ","title":" Principal Engineer - AI ","company":" Cisco Systems Inc "}],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1136940","date":"September 27, 2023","time":"4:00 PM - 4:45 PM","title":"Getting Under the Hood of Kafka Streams: Optimizing Storage Engines to Tune Up Performance","abstract":"With a few tweaks under the hood of your Kafka Streams implementation, you greatly improve performance. Sound too good to be true? Well, the secret lies in understanding storage engines.\n\nYou may already know that if you're using Kafka streams, you already have a storage engine in place, but do you know what options are available to tune it for optimal performance and scalability?\n\nThis presentation will discuss the importance of optimizing and choosing storage engines for Kafka streams applications.\n\nOutline:\n\n- What a storage engine is and how it relates to Kafka stateful streams\n\n- The importance of understanding storage engines for optimal performance and scalability\n\n- Evaluation of Storage Engines - Overview of popular storage engines, including Leveldb, Rocksdb, and Speedb open-source\n\n- Review of the 5 most relevant configurable items and how they affect performance\n\n- Practical ways to optimize and fine tune your storage engine\n\n- Showcase - 2 minutes drop-in replacement demonstration","location":"Breakout Room 3","speakers":[{"name":" Bosmat Tuvel ","title":" Director of Product Management ","company":" Speedb "}],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1136928","date":"September 27, 2023","time":"4:00 PM - 4:45 PM","title":"Indeed Flex: The Story of a Revolutionary Recruitment Platform","abstract":"This is a tale of two streams when the pandemic hit and how we changed with the times and built a revolutionary recruitment platform for ‚Äúgoing into work‚Äù. We engaged employers, recruiters and job seekers from industrial, healthcare, retail, hospital and facilities management sectors by building a unique platform where the job seeker has full control to pick their schedule, pay rate and what meets their preferences. Our goals are to give job seekers and employers a platform that thrives on simplicity, transparency and low costs. The Flexer stands today with full control of their time at the edge of opportunities to thrive on.\n\nThis presentation will go into the details of how we are tearing down a monolithic platform piece by piece and building a robust architecture,\n\n- Routing events between two platforms\n\n- Many sources and,\n\n- Consumed by downstream several applications\n\nWe will discuss the caveats and bugs we learned when we worked with schema registry and evolution of schemas. We will highlight improvements we gained from automation and observability with Datadog integration for Confluent Cloud.\n\nIf you‚Äôre in discussions surrounding event driven systems at your organization then this talk is for you. Join Ronak and me for this talk and let‚Äôs have a discussion.","location":"Breakout Room 5","speakers":[{"name":" Ronak Patel ","title":" Software Engineering Manager ","company":" Indeed "},{"name":" Gayathri Veale ","title":" Principal Cloud Engineer ","company":" Indeed "}],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1136929","date":"September 27, 2023","time":"4:00 PM - 4:45 PM","title":"Maximizing Real-Time Data Processing with Apache Kafka and InfluxDB: A Comprehensive Guide","abstract":"Combining Apache Kafka and InfluxDB can provide a powerful data pipeline for processing and analyzing real-time data. Kafka can be used to ingest data from various sources and stream it to InfluxDB for storage and processing. InfluxDB can then be used to analyze and visualize the data, providing insights and actionable information in real-time. This architecture can be especially useful for IoT applications, where large volumes of sensor data are generated in real-time and need to be processed and analyzed quickly. InfluxDB now offers storage in a parquet file format built on top of the Apache Arrow project that allows for querying in SQL and integration to a larger variety of visualization and analysis tools that Kafka users can now take advantage of. This talk will go into connecting the two platforms and the why, how, and what you can accomplish by doing so.","location":"Breakout Room 2","speakers":[{"name":" Zoe Steinkamp ","title":" Developer Advocate ","company":" InfluxData "}],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1136931","date":"September 27, 2023","time":"4:00 PM - 4:45 PM","title":"Scalable E-Commerce Data Pipelines with Kafka: Real-Time Analytics, Batch, ML, Data Lake, and Beyond","abstract":"In today's fast-paced global E-Commerce industry, the amount of data generated by online shoppers is massive. To deliver real-time analytics, effective advertising campaigns and machine learning based personalized recommendations are crucial. However, building a reliable and scalable data pipeline to support this is a challenging task.\n\nIn this talk, we'll share how we tackled the challenge of building a fully managed robust data pipeline using a combination of streaming analytics, batch processing, data lake, and machine learning. Our platform, built on Google Cloud Platform and powered by Confluent Kafka, enables us to process a massive volume of events per day.\n\nWe'll dive into the technical details of our architecture, tech stack, and data flow, including how we use\n\n‚Ä¢ Kafka Streams Java applications which are deployed in kubernetes to consume, deduplicate, transform, filter, and write data into HBase NoSQL database for real-time analytics,\n\n‚Ä¢ Push to Meta for advertising campaigns,\n\n‚Ä¢ Google AI for personalized recommendations,\n\n‚Ä¢ Confluent sink connector to push events to Google Cloud Storage and BigQuery, and ksqlDB for bot filtering.\n\n‚Ä¢ We'll also cover our observability, monitoring, and deployment practices.\n\nBut we don't want to just talk about our pipeline, we want to help you build one too. You'll leave our talk with practical insights and lessons learned from our experience, including tips on building a reliable, fault-tolerant, and scalable data pipeline, choosing the right tech stack, and ensuring end-to-end observability. Join us, and learn how to take your data pipeline to the next level.","location":"Breakout Room 6","speakers":[{"name":" Mahendra Kumar ","title":" VP, Data and Software Engineering ","company":" BigCommerce "},{"name":" Aristatle Subramaniam ","title":" Lead Data Engineer ","company":" BigCommerce "}],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1136934","date":"September 27, 2023","time":"4:30 PM - 4:50 PM","title":"From Edge to Cloud, Creating Data Pipelines Using Open-source with Strimzi on Kubernetes","abstract":"In a world where data continues to become increasingly important, that same data can be used as a building block for event-driven architectures leveraging change data capture. ¬†Open-source projects such as Debezium, Apicurio Registry, and Strimzi, which include components required for Kafka on Kubernetes, are key enablers for designing such architectures.\nIn this session, we‚Äôll walk through a real-world example of capturing changes made in a relational database with a connector configured to use Apicurio Registry, publishing those changes to Kafka serialized in Avro‚Äôs compact binary form, and utilizing Kafka Streams, Quarkus, and Camel-K to build a pipeline to manage an Elastic search index with extremely low latency efficiently.","location":"Breakout Room 7","speakers":[{"name":" Carles Arnal ","title":" Senior Software Engineer ","company":" Red Hat "},{"name":" Chris Cranford ","title":" Principal Software Engineer ","company":" Red Hat "}],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1136937","date":"September 27, 2023","time":"5:00 PM - 5:45 PM","title":"Apache Kafka's Next-Gen Rebalance Protocol: Towards More Stable and Scalable Consumer Groups","abstract":"At Current 2022, we introduced KIP-848: The Next Generation of the Consumer Rebalance Protocol. Almost a year after, its development is well under way! In this talk, we will dive into all aspects of the new protocol, look into the architecture of Apache Kafka's brand new group coordinator, discuss the upgrade path for both the consumers and the brokers, and finally update the community about where we stand in the development.","location":"Breakout Room 4","speakers":[{"name":" David Jacot ","title":" Staff Software Engineer II ","company":" Confluent "}],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1136936","date":"September 27, 2023","time":"5:00 PM - 5:20 PM","title":"AWS S3 Connector to Backup/Restore","abstract":"What is one of the most popular patterns of using Kafka & AWS? Sending topic data to S3.\nIt‚Äôs the ultimate data sink for long term storage and analytics. Naturally then, it also makes a good fit for storing Kafka topic data.\nSince saving data in S3 is so cheap & popular, enabling it for Kafka data opens-up many use-cases:\n- To drive analytics\n- For Disaster Recovery (DR) of Kafka\n- To migrate Kafka on-prem to cloud\n- Develop safely in separate dev Kafka cluster\n- To save costs\nCan you do it now? Yes, but it‚Äôs painful. You can use a connector and battle with a poor developer experience. And it‚Äôs possibly very expensive if you use a non open-source connector.\nCan we do better? Enter AWS with Lenses.\nIn this talk we will show you:\n- The latest feature-packed Lenses open-source S3 Source & Sink connectors\n- Patterns & best practices for how to backup/restore to S3\n- How to have a seamless, one-click experience to backup and restore in Lenses 5.3","location":"Breakout Room 7","speakers":[{"name":" Adamos Loizou ","title":" Product Manager ","company":" Lenses.io "},{"name":" Joe Khazen ","title":null,"company":" AWS "}],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1136938","date":"September 27, 2023","time":"5:00 PM - 5:45 PM","title":"Build Real-time Machine Learning Apps on Generative AI with Kafka Streams","abstract":"The rise of large language models like GPT-4 and generative AI has changed the traditional approach of data and ML teams to engineer their own features and train models in batch on large sets of historic data. Since training of proprietary LLMs is not anymore an affordable option to most organizations, the inference of foundational models via APIs seems to be the natural way of consumption, providing new challenges for the architecture of real-time apps and workflows.\n\nIn this talk we show how data and machine learning teams can rapidly prototype and deploy real-time ML apps, ingesting real-time data with the help of Apache Kafka¬Æ and Airy, an open-source app framework. We will discuss different options to finetune LLMs and ‚Äûchaining‚Äú them with other ML models at inference in a microservices architecture utilizing Kafka Streams and Kubernetes. We will also discuss how streaming can enable dynamic features for ML models and prompt engineering to integrate with generative AI.\n\nAt the end of the talk we will give an outlook on the opportunity to dynamically retrain machine learning models in real-time with streaming and batch sources, utilizing Ray and Kubernetes to spin up GPU node pools for model training on demand. In this context, we will also discuss how event streaming can be used for reinforcement learning with human feedback (RLHF) to improve the accuracy of predictions and to make the ML model more robust over time.","location":"Breakout Room 6","speakers":[{"name":" Steffen Hoellinger ","title":" Co-Founder & CEO ","company":" Airy, Inc. "}],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1136930","date":"September 27, 2023","time":"5:00 PM - 5:45 PM","title":"OpenMessaging Benchmark: Measuring the Performance of Streaming Systems","abstract":"While the database industry has worked on standardizing benchmarks for decades, with examples like the TPC consortium for relational databases and YCSB for the NoSQL world, there was really no such equivalent in the messaging and streaming system space.\n\nThis talk will present the OpenMessaging Benchmark, why it was created, and how one can use it to model messaging workloads and verify the behavior of different systems.\n\nThere are several aspects that characterize the performance and behavior of messaging systems: from the access to disk IO resources, to how efficiently the system can scale on multiple dimensions.\n\nFor users, it is often very difficult to gain a good understanding of all implications and impact that changes in the performance of the messaging system can have on their application.\n\nWe will also go through the set of properties that should be considered when comparing performance and unique challenges that are specific to messaging systems, along with an analysis of some of these systems like Apache Pulsar, Apache Kafka, RabbitMQ, and others.","location":"Breakout Room 2","speakers":[{"name":" Matteo Merli ","title":" CTO ","company":" StreamNative "}],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1136941","date":"September 27, 2023","time":"5:00 PM - 5:45 PM","title":"Rule Based Asset Management Workflow Automation at Netflix","abstract":"At Netflix, we deal with millions of digital assets every day. Hours of video clips, along with audio, text and image assets are ingested for various purposes. Several workflows are then executed on them; such as inspection, transcoding, editing, logging, etc. These assets can also be used in machine learning workflows, either to train these models, or to get content insights. Not all workflows are applicable to all assets, and some workflows depend on other workflows to run. Additionally, new workflows are introduced regularly, and they need to be executed on existing assets, as well.\n\nWe implemented a workflow rule engine that allows users to define rules and conditions to specify the applicable workflows for assets, based on their types, metadata and states. In order to make this system scalable and fault tolerant, we utilize Kafka to send out events on asset state changes (on create, update, workflow completion, etc.) with minimal information in the payload (asset id and version). The rule engine then enriches this payload by fetching additional metadata, evaluates it against the workflow rules, triggers applicable workflows based on the outcome, and monitors their results by listening to the workflow events.\n\nBy using a highly available Kafka setup, we can easily scale, handle ETL cases such as migrations, replay messages if needed without impacting asset ingestions.","location":"Breakout Room 1 (Livestream)","speakers":[{"name":" Burak Bacioglu ","title":" Staff Software Engineer ","company":" Netflix "},{"name":" Meenakshi Jindal ","title":" Staff Software Engineer ","company":" Netflix "}],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1222161","date":"September 27, 2023","time":"5:00 PM - 5:45 PM","title":"Using Kafka at Scale - A Case Study of Micro Services Data Pipelines at Evernorth Health Services","abstract":"In this talk, we look at textbook examples of using kafka at scale. Specifically focused on Evernorth Health Service's journey of implementing microservices data pipelines, we provide an overview of the patterns we used while implementing CDC data pipelines for these micro services using confluent kafka, the challenges we faced, lessons learned and the unique solutions that we developed over the years to overcome those challenges. We also look at how we are moving these micro services eco systems to public cloud (AWS) and the strategies that we have/are implementing to ensure a smooth consumer cutover. We peek into how kafka consumers can cutover to a replicated topic using offsets based on create time timestamps and how/why this is critical for a downtime-less and data-loss-less consumer cutover for streaming consumers. To conclude, we take a look at how we are re-imagining these pipelines on AWS and how SaaS offerings like Confluent Cloud and Confluent connectors could play a major role.","location":"Breakout Room 3","speakers":[{"name":" Nilay Sundarkar ","title":" Principal Engineer ","company":" Evernorth Health Services "}],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1136942","date":"September 27, 2023","time":"5:00 PM - 5:45 PM","title":"When Only the Last Writer Wins We All Lose: Active-Active Geo-Replication in Venice","abstract":"Venice is LinkedIn‚Äôs low latency derived database that powers most of the company‚Äôs AI use cases. Last year we open sourced Venice, which means that you (Yeah! You!!) can deploy and run Venice in your stack. Venice is designed from the ground up with asynchrony in mind, making it a great fit for ingesting the output of your stream processing jobs. It also supports Lambda and Kappa architectures out of the box.\n\nThis talk presents Venice and deep dives into how we designed it to enable high data ingestion volumes via Kafka, merging it all coherently from many data sources and many geographically distributed regions. Venice‚Äôs active-active replication allows write operations to be produced in any region and be consumed in any order by all other regions, while still achieving eventual consistency. We‚Äôll cover how Venice‚Äôs conflict resolution strategy can be a powerful abstraction when dealing with many writers.","location":"Breakout Room 5","speakers":[{"name":" Zac Policzer ","title":" Senior Staff Engineer ","company":" LinkedIn "}],"last_updated":"2023-09-18T18:49:17.798614Z"}
{"url":"https://events.bizzabo.com/468544/agenda/session/1222573","date":"September 27, 2023","time":"6:00 PM - 6:30 PM","title":"Current 2023 Closing Session","abstract":"We‚Äôll celebrate the speakers (and attendees) who helped make this conference possible, highlight some of the best sessions from the past two days, and hand out the prestigious Data Streaming awards.‚Ä¶but that‚Äôs not all. Come see what else we have up our sleeves and help us to close out Current 2023 in style!","location":"Breakout Room 1 (Livestream)","speakers":[],"last_updated":"2023-09-18T18:49:17.798614Z"}
