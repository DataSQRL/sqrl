package com.datasqrl.packager.postprocess;

import com.fasterxml.jackson.databind.ObjectMapper;
import com.fasterxml.jackson.dataformat.yaml.YAMLFactory;
import java.io.IOException;
import java.nio.file.Files;
import java.nio.file.Path;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.LinkedHashMap;
import java.util.List;
import java.util.Map;
import java.util.Optional;
import java.util.stream.Collectors;
import lombok.SneakyThrows;
import lombok.extern.slf4j.Slf4j;

@Slf4j
public class DockerPostprocessor implements Postprocessor {

  public static String getYml(Optional<Path> mountDir) {
    String volumneMnt = "    volumes:\n"
        + mountDir
        .map(dir -> dir.toAbsolutePath().normalize())
        .map(dir -> "      - " + dir.toAbsolutePath() + ":/build\n")
        .orElse("")
        + "      - ./init-flink.sh:/exec/init-flink.sh\n";

    return "# This is a docker-compose template for starting a DataSQRL compiled data pipeline\n"
        + "# This template uses the Apache Flink as the stream engine, Postgres as the database engine, and Vertx as the server engine.\n"
        + "# It assumes that:\n"
        + "# 1. You ran the `compile` command to compile your SQRL script (and API specification)\n"
        + "# 2. Are in the `build/deploy` directory which contains the deployment artifacts generated by the compiler\n"
        + "# 3. Have built the deployment artifacts for each pipeline engine\n"
        + "# Refer to the deployment documentation for more information:\n"
        + "# https://www.datasqrl.com/docs/reference/operations/deploy/overview/\n"
        + "version: \"3.8\"\n"
        + "services:\n"
        + "  database:\n"
        + "    image: ankane/pgvector:v0.5.0\n"
        + "    restart: always\n"
        + "    environment:\n"
        + "      - POSTGRES_USER=postgres\n"
        + "      - POSTGRES_PASSWORD=postgres\n"
        + "      - POSTGRES_DB=datasqrl\n"
        + "    ports:\n"
        + "      - '5432:5432'\n"
        + "    volumes:\n"
        + "      - ./database-schema.sql:/docker-entrypoint-initdb.d/init-schema.sql\n"
        + "\n"
        + "  flink-jobmanager:\n"
        + "    image: flink:1.16.1-scala_2.12-java11\n"
        + "    ports:\n"
        + "      - \"8081:8081\"\n"
        + "    command: /bin/bash /exec/init-flink.sh jobmanager\n"
        + "    environment:\n"
        + "      - |\n"
        + "        FLINK_PROPERTIES=\n"
        + "        jobmanager.rpc.address: flink-jobmanager\n"
        + volumneMnt
        + "\n"
        + "  flink-taskmanager:\n"
        + "    image: flink:1.16.1-scala_2.12-java11\n"
        + "    depends_on:\n"
        + "      - flink-jobmanager\n"
        + "    command: /bin/bash /exec/init-flink.sh taskmanager\n"
        + "    environment:\n"
        + "      - |\n"
        + "        FLINK_PROPERTIES=\n"
        + "        jobmanager.rpc.address: flink-jobmanager\n"
        + "        taskmanager.numberOfTaskSlots: 1\n"
        + volumneMnt
        + "\n"
        + "  kafka:\n"
        + "    image: docker.io/bitnami/kafka:3.4.0-debian-11-r38\n"
        + "    ports:\n"
        + "      - \"9092:9092\"\n"
        + "      - \"9094:9094\"\n"
        + "    environment:\n"
        + "      - KAFKA_CFG_AUTO_CREATE_TOPICS_ENABLE=true\n"
        + "      - ALLOW_PLAINTEXT_LISTENER=yes\n"
        + "      - KAFKA_CFG_LISTENERS=PLAINTEXT://:9092,CONTROLLER://:9093,EXTERNAL://:9094\n"
        + "      - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092,EXTERNAL://localhost:9094\n"
        + "      - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=CONTROLLER:PLAINTEXT,EXTERNAL:PLAINTEXT,PLAINTEXT:PLAINTEXT\n"
        + "  kafka-setup:\n"
        + "    image: docker.io/bitnami/kafka:3.4.0-debian-11-r38\n"
        + "    volumes:\n"
        + "      - './create-topics.sh:/create-topics.sh'\n"
        + "    command: ['/bin/bash', '/create-topics.sh']\n"
        + "    depends_on:\n"
        + "      - kafka\n"
        + "\n"
        + "  server:\n"
        + "    image: eclipse-temurin:11\n"
        + "    command: java -jar vertx-server.jar\n"
        + "    depends_on:\n"
        + "      - database\n"
        + "      - kafka-setup\n"
        + "    ports:\n"
        + "      - \"8888:8888\"\n"
        + "    volumes:\n"
        + "      - ./server-model.json:/server-model.json\n"
        + "      - ./server-config.json:/server-config.json\n"
        + "      - ./vertx-server.jar:/vertx-server.jar\n"
        + "\n"
        + "  flink-job-submitter:\n"
        + "    image: badouralix/curl-jq:alpine\n"
        + "    depends_on:\n"
        + "      - flink-jobmanager\n"
        + "      - database\n"
        + "      - kafka-setup\n"
        + "    volumes:\n"
        + "      - ./flink-job.jar:/flink-job.jar\n"
        + "      - ./submit-flink-job.sh:/submit-flink-job.sh\n"
        + "    entrypoint: /submit-flink-job.sh\n"
        + "\n";
  }

  //Writes a dockercompose.yml
  @Override
  public void process(ProcessorContext context) {
    //Check if there is a docker-compose.yml in a profile
    Optional<Path> dockerCompose = getDockerCompose(context.getBuildDir(), context.getProfiles());

    String yml = dockerCompose
        .map(p -> replaceMountDirs(p, context.getMountDir()))
        .orElseGet(() -> addDockerCompose(context.getMountDir(), context.getTargetDir()));

    Path toFile = context.getTargetDir().resolve("docker-compose.yml");
    try {
      Files.createDirectories(context.getTargetDir());
      Files.writeString(toFile, yml);
    } catch (Exception e) {
      log.error("Could not copy docker-compose file.");
      throw new RuntimeException(e);
    }
  }

  @SneakyThrows
  private String replaceMountDirs(Path compose, Optional<Path> mountDir) {
    try {
      String yamlContent = Files.readString(compose);
      ObjectMapper mapper = new ObjectMapper(new YAMLFactory());

      Map<String, Object> yamlMap = mapper.readValue(yamlContent, LinkedHashMap.class);
      updateServices(yamlMap, "flink-jobmanager", mountDir);
      updateServices(yamlMap, "flink-taskmanager", mountDir);

      return mapper.writeValueAsString(yamlMap);
    } catch (IOException e) {
      log.error("Error processing docker-compose file: " + e.getMessage(), e);
      throw new RuntimeException(e);
    }
  }

  private void updateServices(Map<String, Object> yamlMap, String serviceName,
      Optional<Path> mountDir) {
    Map<String, Object> services = (Map<String, Object>) yamlMap.get("services");
    if (services != null && services.containsKey(serviceName)) {
      Map<String, Object> service = (Map<String, Object>) services.get(serviceName);
      List<String> existingVolumes = (List<String>) service.get("volumes");
      if (existingVolumes == null) {
        ArrayList volumes = new ArrayList<>();
        mountDir.ifPresent(dir -> volumes.add(dir.toAbsolutePath().normalize() + ":/build"));
        volumes.add("./init-flink.sh:/exec/init-flink.sh");
        service.put("volumes", volumes);
      } else {
        mountDir.ifPresent(dir -> existingVolumes.add(dir.toAbsolutePath().normalize() + ":/build"));
      }
    }
  }

  private Optional<Path> getDockerCompose(Path buildDir, String[] profiles) {
    List<Path> composeFiles = Arrays.stream(profiles)
        .map(p -> buildDir.resolve(p).resolve("docker-compose.yml"))
        .filter(f -> Files.isRegularFile(f))
        .collect(Collectors.toList());

    if (composeFiles.size() > 1) {
      throw new RuntimeException(
          "Multiple docker-compose.yml in profiles is not currently supported.");
    }
    if (composeFiles.isEmpty()) {
      return Optional.empty();
    }
    return Optional.of(composeFiles.get(0));
  }

  protected String addDockerCompose(Optional<Path> mountDir, Path targetDir) {
    return getYml(mountDir);
  }
}
