-- The point of this test case is to instantiate the various connectors and formats DataSQRL supports
-- not to actually connect to those systems at runtime

-- ========== KAFKA (with Avro) ==========
CREATE TABLE kafka_avro_table (
                                  user_id INT,
                                  name STRING
) WITH (
    'connector' = 'kafka',
    'topic' = 'users-avro',
    'properties.bootstrap.servers' = 'localhost:9092',
    'properties.group.id' = 'flink-kafka-avro-group',
    'format' = 'avro'
      );

-- ========== KAFKA-safe (with json) ==========
CREATE TABLE kafka_safe_json_table (
                                  user_id INT,
                                  name STRING
) WITH (
    'connector' = 'kafka-safe',
    'topic' = 'users-json-avro',
    'properties.bootstrap.servers' = 'localhost:9092',
    'properties.group.id' = 'test',
    'format' = 'flexible-json'
      );

-- ========== KAFKA-safe (with csv) ==========
CREATE TABLE kafka_safe_csv_table (
                                  user_id INT,
                                  name STRING
) WITH (
    'connector' = 'kafka-safe',
    'topic' = 'users-csv-avro',
    'properties.bootstrap.servers' = 'localhost:9092',
    'properties.group.id' = 'test',
    'format' = 'flexible-csv'
      );

-- ========== UPSERT KAFKA (with Confluent Avro) ==========
CREATE TABLE upsert_kafka_confluent_avro (
                                             user_id INT,
                                             name STRING,
                                             PRIMARY KEY (user_id) NOT ENFORCED
) WITH (
      'connector' = 'upsert-kafka',
      'topic' = 'users-upsert',
      'properties.group.id' = 'flink-kafka-avro-group',
      'properties.bootstrap.servers' = 'localhost:9092',
      'key.format' = 'avro-confluent',
      'key.avro-confluent.schema-registry.url' = 'http://localhost:8081',
      'value.format' = 'avro-confluent',
      'value.avro-confluent.schema-registry.url' = 'http://localhost:8081'
      );

-- ========== KAFKA (with Debezium JSON) ==========
CREATE TABLE kafka_debezium_table (
                                      user_id INT,
                                      name STRING
) WITH (
      'connector' = 'kafka',
      'topic' = 'users-debezium',
      'properties.group.id' = 'flink-kafka-avro-group',
      'properties.bootstrap.servers' = 'localhost:9092',
      'format' = 'debezium-json'
      );

-- ========== ICEBERG (with Parquet) ==========
CREATE TABLE iceberg_parquet_table (
                                       user_id INT,
                                       name STRING
) PARTITIONED BY (user_id)
WITH (
  'connector' = 'iceberg',
  'catalog-name' = 'local_fs',
  'catalog-type' = 'hadoop',
  'warehouse' = 'file:///tmp/iceberg/warehouse',
  'format-version' = '2',
  'write.format.default' = 'parquet'
);

-- ========== JDBC for temporal join ==========
CREATE TABLE _jdbc_table (
                            user_id INT,
                            name STRING
) WITH (
      'connector' = 'jdbc',
      'url' = 'jdbc:postgresql://localhost:5432/mydb',
      'table-name' = 'users',
      'username' = 'myuser',
      'password' = 'mypassword'
      );

-- ========== KINESIS (with JSON) ==========
-- CREATE TABLE kinesis_json_table (
--                                     user_id INT,
--                                     name STRING
-- ) WITH (
--       'connector' = 'kinesis',
--       'stream' = 'users-stream',
--       'aws.region' = 'us-west-2',
--       'scan.stream.initpos' = 'LATEST',
--       'format' = 'json'
--       );

-- ========== DYNAMODB (via Amazon DynamoDB connector) ==========
-- CREATE TABLE dynamodb_table (
--                                 user_id INT,
--                                 name STRING
-- ) WITH (
--       'connector' = 'dynamodb',
--       'table-name' = 'users-table',
--       'aws.region' = 'us-west-2'
--       );