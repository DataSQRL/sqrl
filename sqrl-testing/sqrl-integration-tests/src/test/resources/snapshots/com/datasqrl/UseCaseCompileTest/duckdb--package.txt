>>>pipeline_explain.txt
=== MyTable1
ID:     default_catalog.default_database.MyTable1
Type:   stream
Stage:  iceberg
Inputs: default_catalog.default_database._MyApplications1
Annotations:
 - stream-root: _Applications
Primary Key: -
Timestamp  : -
Schema:
 - id: BIGINT NOT NULL
 - hello: CHAR(12) CHARACTER SET "UTF-16LE" NOT NULL
Plan:
LogicalProject(id=[$0], hello=['hello world1'])
  LogicalTableScan(table=[[default_catalog, default_database, _MyApplications1]])
SQL: CREATE VIEW MyTable1 AS  SELECT id, 'hello world1' as hello FROM _MyApplications1;

=== MyTable2
ID:     default_catalog.default_database.MyTable2
Type:   stream
Stage:  iceberg
Inputs: default_catalog.default_database._MyApplications2
Annotations:
 - stream-root: _Applications
Primary Key: -
Timestamp  : -
Schema:
 - id: BIGINT NOT NULL
 - hello: CHAR(12) CHARACTER SET "UTF-16LE" NOT NULL
Plan:
LogicalProject(id=[$0], hello=['hello world2'])
  LogicalTableScan(table=[[default_catalog, default_database, _MyApplications2]])
SQL: CREATE VIEW MyTable2 AS  SELECT id, 'hello world2' as hello FROM _MyApplications2;

=== MyTable3
ID:     default_catalog.default_database.MyTable3
Type:   stream
Stage:  postgres
Inputs: default_catalog.default_database._MyApplications2
Annotations:
 - stream-root: _Applications
Primary Key: -
Timestamp  : -
Schema:
 - id: BIGINT NOT NULL
 - hello: CHAR(12) CHARACTER SET "UTF-16LE" NOT NULL
Plan:
LogicalProject(id=[$0], hello=['hello world3'])
  LogicalTableScan(table=[[default_catalog, default_database, _MyApplications2]])
SQL: CREATE VIEW MyTable3 AS  SELECT id, 'hello world3' as hello FROM _MyApplications2;

=== _Applications
ID:     default_catalog.default_database._Applications
Type:   stream
Stage:  flink
Inputs: default_catalog.default_database._Applications__base
Annotations:
 - stream-root: _Applications
Primary Key: id, updated_at
Timestamp  : updated_at
Schema:
 - id: BIGINT NOT NULL
 - customer_id: BIGINT NOT NULL
 - loan_type_id: BIGINT NOT NULL
 - amount: DOUBLE NOT NULL
 - duration: BIGINT NOT NULL
 - application_date: TIMESTAMP_WITH_LOCAL_TIME_ZONE(3) NOT NULL
 - updated_at: TIMESTAMP_LTZ(3) *ROWTIME* NOT NULL
Plan:
LogicalWatermarkAssigner(rowtime=[updated_at], watermark=[-($6, 1:INTERVAL SECOND)])
  LogicalTableScan(table=[[default_catalog, default_database, _Applications]])
SQL: CREATE VIEW `_Applications__view`
AS
SELECT `_Applications`.`id`, `_Applications`.`customer_id`, `_Applications`.`loan_type_id`, `_Applications`.`amount`, `_Applications`.`duration`, `_Applications`.`application_date`, `_Applications`.`updated_at`
FROM `default_catalog`.`default_database`.`_Applications` AS `_Applications`
=== _MyApplications1
ID:     default_catalog.default_database._MyApplications1
Type:   stream
Stage:  flink
Inputs: default_catalog.default_database._Applications
Annotations:
 - stream-root: _Applications
Primary Key: id, updated_at
Timestamp  : updated_at
Schema:
 - id: BIGINT NOT NULL
 - customer_id: BIGINT NOT NULL
 - loan_type_id: BIGINT NOT NULL
 - amount: DOUBLE NOT NULL
 - duration: BIGINT NOT NULL
 - application_date: TIMESTAMP_WITH_LOCAL_TIME_ZONE(3) NOT NULL
 - updated_at: TIMESTAMP_LTZ(3) *ROWTIME* NOT NULL
Plan:
LogicalProject(id=[$0], customer_id=[$1], loan_type_id=[$2], amount=[$3], duration=[$4], application_date=[$5], updated_at=[$6])
  LogicalTableScan(table=[[default_catalog, default_database, _Applications]])
SQL: CREATE VIEW _MyApplications1 AS  SELECT * FROM _Applications;

=== _MyApplications2
ID:     default_catalog.default_database._MyApplications2
Type:   stream
Stage:  flink
Inputs: default_catalog.default_database._Applications
Annotations:
 - stream-root: _Applications
Primary Key: id, updated_at
Timestamp  : updated_at
Schema:
 - id: BIGINT NOT NULL
 - customer_id: BIGINT NOT NULL
 - loan_type_id: BIGINT NOT NULL
 - amount: DOUBLE NOT NULL
 - duration: BIGINT NOT NULL
 - application_date: TIMESTAMP_WITH_LOCAL_TIME_ZONE(3) NOT NULL
 - updated_at: TIMESTAMP_LTZ(3) *ROWTIME* NOT NULL
Plan:
LogicalProject(id=[$0], customer_id=[$1], loan_type_id=[$2], amount=[$3], duration=[$4], application_date=[$5], updated_at=[$6])
  LogicalFilter(condition=[>=($0, 0)])
    LogicalTableScan(table=[[default_catalog, default_database, _Applications]])
SQL: CREATE VIEW _MyApplications2 AS  SELECT * FROM _Applications WHERE id >= 0;

>>>flink-sql-no-functions.sql
CREATE TEMPORARY TABLE `_Applications__schema` (
  `id` BIGINT NOT NULL,
  `customer_id` BIGINT NOT NULL,
  `loan_type_id` BIGINT NOT NULL,
  `amount` DOUBLE NOT NULL,
  `duration` BIGINT NOT NULL,
  `application_date` TIMESTAMP(3) WITH LOCAL TIME ZONE NOT NULL,
  `updated_at` TIMESTAMP(3) WITH LOCAL TIME ZONE NOT NULL
) WITH (
  'connector' = 'datagen'
);
CREATE TABLE `_Applications` (
  PRIMARY KEY (`id`, `updated_at`) NOT ENFORCED,
  WATERMARK FOR `updated_at` AS `updated_at` - INTERVAL '0.001' SECOND
) WITH (
  'format' = 'flexible-json',
  'path' = '${DATA_PATH}/applications.jsonl',
  'source.monitor-interval' = '1',
  'connector' = 'filesystem'
)
LIKE `_Applications__schema`;
CREATE VIEW `_MyApplications1`
AS
SELECT *
FROM `_Applications`;
CREATE VIEW `_MyApplications2`
AS
SELECT *
FROM `_Applications`
WHERE `id` >= 0;
CREATE VIEW `MyTable1`
AS
SELECT `id`, 'hello world1' AS `hello`
FROM `_MyApplications1`;
CREATE VIEW `MyTable2`
AS
SELECT `id`, 'hello world2' AS `hello`
FROM `_MyApplications2`;
CREATE VIEW `MyTable3`
AS
SELECT `id`, 'hello world3' AS `hello`
FROM `_MyApplications2`;
CREATE TABLE `_MyApplications1_1` (
  `id` BIGINT NOT NULL,
  `customer_id` BIGINT NOT NULL,
  `loan_type_id` BIGINT NOT NULL,
  `amount` DOUBLE NOT NULL,
  `duration` BIGINT NOT NULL,
  `application_date` TIMESTAMP(3) WITH LOCAL TIME ZONE NOT NULL,
  `updated_at` TIMESTAMP(3) WITH LOCAL TIME ZONE NOT NULL,
  PRIMARY KEY (`id`, `updated_at`) NOT ENFORCED
)
PARTITIONED BY (`id`)
WITH (
  'catalog-name' = 'mydatabase',
  'catalog-table' = '_MyApplications1',
  'catalog-type' = 'hadoop',
  'connector' = 'iceberg',
  'warehouse' = '/tmp/duckdb',
  'write.parquet.page-size-bytes' = '1000'
);
CREATE TABLE `_MyApplications2_2` (
  `id` BIGINT NOT NULL,
  `customer_id` BIGINT NOT NULL,
  `loan_type_id` BIGINT NOT NULL,
  `amount` DOUBLE NOT NULL,
  `duration` BIGINT NOT NULL,
  `application_date` TIMESTAMP(3) WITH LOCAL TIME ZONE NOT NULL,
  `updated_at` TIMESTAMP(3) WITH LOCAL TIME ZONE NOT NULL,
  PRIMARY KEY (`id`, `updated_at`) NOT ENFORCED
) WITH (
  'catalog-name' = 'mydatabase',
  'catalog-table' = '_MyApplications2',
  'catalog-type' = 'hadoop',
  'connector' = 'iceberg',
  'warehouse' = '/tmp/duckdb',
  'write.parquet.page-size-bytes' = '1000'
);
CREATE TABLE `_MyApplications2_3` (
  `id` BIGINT NOT NULL,
  `customer_id` BIGINT NOT NULL,
  `loan_type_id` BIGINT NOT NULL,
  `amount` DOUBLE NOT NULL,
  `duration` BIGINT NOT NULL,
  `application_date` TIMESTAMP(3) WITH LOCAL TIME ZONE NOT NULL,
  `updated_at` TIMESTAMP(3) WITH LOCAL TIME ZONE NOT NULL,
  PRIMARY KEY (`id`, `updated_at`) NOT ENFORCED
) WITH (
  'connector' = 'jdbc-sqrl',
  'driver' = 'org.postgresql.Driver',
  'password' = '${JDBC_PASSWORD}',
  'table-name' = '_MyApplications2',
  'url' = 'jdbc:postgresql://${JDBC_AUTHORITY}',
  'username' = '${JDBC_USERNAME}'
);
EXECUTE STATEMENT SET BEGIN
INSERT INTO `default_catalog`.`default_database`.`_MyApplications1_1`
(SELECT *
 FROM `default_catalog`.`default_database`.`_MyApplications1`)
;
INSERT INTO `default_catalog`.`default_database`.`_MyApplications2_2`
 (SELECT *
  FROM `default_catalog`.`default_database`.`_MyApplications2`)
 ;
 INSERT INTO `default_catalog`.`default_database`.`_MyApplications2_3`
  (SELECT *
   FROM `default_catalog`.`default_database`.`_MyApplications2`)
  ;
  END
>>>iceberg-duckdb-schema.sql
CREATE TABLE IF NOT EXISTS "_MyApplications1" ("id" BIGINT NOT NULL, "customer_id" BIGINT NOT NULL, "loan_type_id" BIGINT NOT NULL, "amount" DOUBLE PRECISION NOT NULL, "duration" BIGINT NOT NULL, "application_date" TIMESTAMP WITH TIME ZONE NOT NULL, "updated_at" TIMESTAMP WITH TIME ZONE NOT NULL , PRIMARY KEY ("id","updated_at"));
CREATE TABLE IF NOT EXISTS "_MyApplications2" ("id" BIGINT NOT NULL, "customer_id" BIGINT NOT NULL, "loan_type_id" BIGINT NOT NULL, "amount" DOUBLE PRECISION NOT NULL, "duration" BIGINT NOT NULL, "application_date" TIMESTAMP WITH TIME ZONE NOT NULL, "updated_at" TIMESTAMP WITH TIME ZONE NOT NULL , PRIMARY KEY ("id","updated_at"))
>>>iceberg-duckdb-views.sql

>>>iceberg-schema.sql
CREATE TABLE IF NOT EXISTS "_MyApplications1" ("id" BIGINT NOT NULL, "customer_id" BIGINT NOT NULL, "loan_type_id" BIGINT NOT NULL, "amount" DOUBLE PRECISION NOT NULL, "duration" BIGINT NOT NULL, "application_date" TIMESTAMP WITH TIME ZONE NOT NULL, "updated_at" TIMESTAMP WITH TIME ZONE NOT NULL , PRIMARY KEY ("id","updated_at"));
CREATE TABLE IF NOT EXISTS "_MyApplications2" ("id" BIGINT NOT NULL, "customer_id" BIGINT NOT NULL, "loan_type_id" BIGINT NOT NULL, "amount" DOUBLE PRECISION NOT NULL, "duration" BIGINT NOT NULL, "application_date" TIMESTAMP WITH TIME ZONE NOT NULL, "updated_at" TIMESTAMP WITH TIME ZONE NOT NULL , PRIMARY KEY ("id","updated_at"))
>>>iceberg-views.sql

>>>kafka.json
{
  "topics" : [ ]
}
>>>postgres-schema.sql
CREATE TABLE IF NOT EXISTS "_MyApplications2" ("id" BIGINT NOT NULL, "customer_id" BIGINT NOT NULL, "loan_type_id" BIGINT NOT NULL, "amount" DOUBLE PRECISION NOT NULL, "duration" BIGINT NOT NULL, "application_date" TIMESTAMP WITH TIME ZONE NOT NULL, "updated_at" TIMESTAMP WITH TIME ZONE NOT NULL , PRIMARY KEY ("id","updated_at"));

CREATE INDEX IF NOT EXISTS "_MyApplications2_hash_c0" ON "_MyApplications2" USING hash ("id")
>>>postgres-views.sql
CREATE OR REPLACE VIEW "MyTable3"("id", "hello") AS SELECT "id", 'hello world3' AS "hello"
FROM "_MyApplications2"
>>>vertx.json
{
  "model" : {
    "queries" : [
      {
        "type" : "args",
        "parentType" : "Query",
        "fieldName" : "MyTable1",
        "exec" : {
          "arguments" : [
            {
              "type" : "variable",
              "path" : "offset"
            },
            {
              "type" : "variable",
              "path" : "limit"
            }
          ],
          "query" : {
            "type" : "SqlQuery",
            "sql" : "SELECT \"id\", 'hello world1' AS \"hello\"\nFROM \"iceberg_scan\"('/tmp/duckdb/default_database/_MyApplications1', ALLOW_MOVED_PATHS = TRUE)",
            "parameters" : [ ],
            "pagination" : "LIMIT_AND_OFFSET",
            "database" : "DUCKDB"
          }
        }
      },
      {
        "type" : "args",
        "parentType" : "Query",
        "fieldName" : "MyTable2",
        "exec" : {
          "arguments" : [
            {
              "type" : "variable",
              "path" : "offset"
            },
            {
              "type" : "variable",
              "path" : "limit"
            }
          ],
          "query" : {
            "type" : "SqlQuery",
            "sql" : "SELECT \"id\", 'hello world2' AS \"hello\"\nFROM \"iceberg_scan\"('/tmp/duckdb/default_database/_MyApplications2', ALLOW_MOVED_PATHS = TRUE)",
            "parameters" : [ ],
            "pagination" : "LIMIT_AND_OFFSET",
            "database" : "DUCKDB"
          }
        }
      },
      {
        "type" : "args",
        "parentType" : "Query",
        "fieldName" : "MyTable3",
        "exec" : {
          "arguments" : [
            {
              "type" : "variable",
              "path" : "offset"
            },
            {
              "type" : "variable",
              "path" : "limit"
            }
          ],
          "query" : {
            "type" : "SqlQuery",
            "sql" : "SELECT \"id\", 'hello world3' AS \"hello\"\nFROM \"_MyApplications2\"",
            "parameters" : [ ],
            "pagination" : "LIMIT_AND_OFFSET",
            "database" : "POSTGRES"
          }
        }
      }
    ],
    "mutations" : [ ],
    "subscriptions" : [ ],
    "schema" : {
      "type" : "string",
      "schema" : "\"An arbitrary precision signed integer\"\nscalar GraphQLBigInteger\n\ntype MyTable1 {\n  id: GraphQLBigInteger!\n  hello: String!\n}\n\ntype MyTable2 {\n  id: GraphQLBigInteger!\n  hello: String!\n}\n\ntype MyTable3 {\n  id: GraphQLBigInteger!\n  hello: String!\n}\n\ntype Query {\n  MyTable1(limit: Int = 10, offset: Int = 0): [MyTable1!]\n  MyTable2(limit: Int = 10, offset: Int = 0): [MyTable2!]\n  MyTable3(limit: Int = 10, offset: Int = 0): [MyTable3!]\n}\n"
    }
  }
}
