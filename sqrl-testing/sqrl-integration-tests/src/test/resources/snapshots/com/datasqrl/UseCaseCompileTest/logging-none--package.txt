>>>pipeline_explain.txt
=== Data
ID:     default_catalog.default_database.Data
Type:   stream
Stage:  flink
Inputs: default_catalog.default_database.Data__def
Annotations:
 - stream-root: Data__def
Primary Key: ID
Timestamp  : TIMESTAMP
Schema:
 - ID: BIGINT NOT NULL
 - EPOCH_TIMESTAMP: BIGINT NOT NULL
 - SOME_VALUE: VARCHAR(2147483647) CHARACTER SET "UTF-16LE" NOT NULL
 - TIMESTAMP: TIMESTAMP_LTZ(3) *ROWTIME*
Plan:
LogicalProject(ID=[$0], EPOCH_TIMESTAMP=[$1], SOME_VALUE=[$2], TIMESTAMP=[$3])
  LogicalWatermarkAssigner(rowtime=[TIMESTAMP], watermark=[-($3, 1:INTERVAL SECOND)])
    LogicalProject(ID=[$0], EPOCH_TIMESTAMP=[$1], SOME_VALUE=[$2], TIMESTAMP=[TO_TIMESTAMP_LTZ($1, 3)])
      LogicalTableScan(table=[[default_catalog, default_database, Data__def]])
SQL: CREATE VIEW `Data`
AS
SELECT *
FROM `default_catalog`.`default_database`.`Data__def`
=== DummyLogData
ID:     print.DummyLogData
Type:   export
Stage:  flink
Inputs: default_catalog.default_database.Data

>>>flink-sql-no-functions.sql
CREATE TABLE `Data__def` (
  `ID` BIGINT NOT NULL,
  `EPOCH_TIMESTAMP` BIGINT NOT NULL,
  `SOME_VALUE` STRING NOT NULL,
  `TIMESTAMP` AS TO_TIMESTAMP_LTZ(`EPOCH_TIMESTAMP`, 3),
  PRIMARY KEY (`ID`) NOT ENFORCED,
  WATERMARK FOR `TIMESTAMP` AS `TIMESTAMP` - INTERVAL '0.001' SECOND
) WITH (
  'connector' = 'datagen',
  'number-of-rows' = '10',
  'fields.ID.kind' = 'sequence',
  'fields.ID.start' = '0',
  'fields.ID.end' = '9',
  'fields.EPOCH_TIMESTAMP.kind' = 'sequence',
  'fields.EPOCH_TIMESTAMP.start' = '1719318565000',
  'fields.EPOCH_TIMESTAMP.end' = '1719319565000',
  'fields.SOME_VALUE.kind' = 'random'
);
CREATE VIEW `Data`
AS
SELECT *
FROM `default_catalog`.`default_database`.`Data__def`;
CREATE TABLE `DummyLogData_1` (
  `ID` BIGINT NOT NULL,
  `EPOCH_TIMESTAMP` BIGINT NOT NULL,
  `SOME_VALUE` VARCHAR(2147483647) CHARACTER SET `UTF-16LE` NOT NULL,
  `TIMESTAMP` TIMESTAMP(3) WITH LOCAL TIME ZONE,
  PRIMARY KEY (`ID`) NOT ENFORCED
) WITH (
  'connector' = 'print',
  'print-identifier' = 'DummyLogData'
);
EXECUTE STATEMENT SET BEGIN
INSERT INTO `default_catalog`.`default_database`.`DummyLogData_1`
(SELECT *
 FROM `default_catalog`.`default_database`.`Data`)
;
END
>>>kafka.json
{
  "topics" : [ ]
}
