>>>pipeline_explain.txt
=== iceberg_parquet_table
ID:     default_catalog.default_database.iceberg_parquet_table
Type:   stream
Stage:  flink
Inputs: default_catalog.default_database.iceberg_parquet_table__base
Primary Key: -
Timestamp  : -
Schema:
 - user_id: INTEGER
 - name: VARCHAR(2147483647) CHARACTER SET "UTF-16LE"
Plan:
LogicalTableScan(table=[[default_catalog, default_database, iceberg_parquet_table]])
SQL: CREATE VIEW `iceberg_parquet_table__view`
AS
SELECT `iceberg_parquet_table`.`user_id`, `iceberg_parquet_table`.`name`
FROM `default_catalog`.`default_database`.`iceberg_parquet_table` AS `iceberg_parquet_table`
=== kafka_avro_table
ID:     default_catalog.default_database.kafka_avro_table
Type:   stream
Stage:  flink
Inputs: default_catalog.default_database.kafka_avro_table__base
Primary Key: -
Timestamp  : -
Schema:
 - user_id: INTEGER
 - name: VARCHAR(2147483647) CHARACTER SET "UTF-16LE"
Plan:
LogicalTableScan(table=[[default_catalog, default_database, kafka_avro_table]])
SQL: CREATE VIEW `kafka_avro_table__view`
AS
SELECT `kafka_avro_table`.`user_id`, `kafka_avro_table`.`name`
FROM `default_catalog`.`default_database`.`kafka_avro_table` AS `kafka_avro_table`
=== kafka_debezium_table
ID:     default_catalog.default_database.kafka_debezium_table
Type:   stream
Stage:  flink
Inputs: default_catalog.default_database.kafka_debezium_table__base
Primary Key: -
Timestamp  : -
Schema:
 - user_id: INTEGER
 - name: VARCHAR(2147483647) CHARACTER SET "UTF-16LE"
Plan:
LogicalTableScan(table=[[default_catalog, default_database, kafka_debezium_table]])
SQL: CREATE VIEW `kafka_debezium_table__view`
AS
SELECT `kafka_debezium_table`.`user_id`, `kafka_debezium_table`.`name`
FROM `default_catalog`.`default_database`.`kafka_debezium_table` AS `kafka_debezium_table`
=== kafka_safe_csv_table
ID:     default_catalog.default_database.kafka_safe_csv_table
Type:   stream
Stage:  flink
Inputs: default_catalog.default_database.kafka_safe_csv_table__base
Primary Key: -
Timestamp  : -
Schema:
 - user_id: INTEGER
 - name: VARCHAR(2147483647) CHARACTER SET "UTF-16LE"
Plan:
LogicalTableScan(table=[[default_catalog, default_database, kafka_safe_csv_table]])
SQL: CREATE VIEW `kafka_safe_csv_table__view`
AS
SELECT `kafka_safe_csv_table`.`user_id`, `kafka_safe_csv_table`.`name`
FROM `default_catalog`.`default_database`.`kafka_safe_csv_table` AS `kafka_safe_csv_table`
=== kafka_safe_json_table
ID:     default_catalog.default_database.kafka_safe_json_table
Type:   stream
Stage:  flink
Inputs: default_catalog.default_database.kafka_safe_json_table__base
Primary Key: -
Timestamp  : -
Schema:
 - user_id: INTEGER
 - name: VARCHAR(2147483647) CHARACTER SET "UTF-16LE"
Plan:
LogicalTableScan(table=[[default_catalog, default_database, kafka_safe_json_table]])
SQL: CREATE VIEW `kafka_safe_json_table__view`
AS
SELECT `kafka_safe_json_table`.`user_id`, `kafka_safe_json_table`.`name`
FROM `default_catalog`.`default_database`.`kafka_safe_json_table` AS `kafka_safe_json_table`
=== upsert_kafka_confluent_avro
ID:     default_catalog.default_database.upsert_kafka_confluent_avro
Type:   state
Stage:  flink
Inputs: default_catalog.default_database.upsert_kafka_confluent_avro__base
Primary Key: user_id
Timestamp  : -
Schema:
 - user_id: INTEGER NOT NULL
 - name: VARCHAR(2147483647) CHARACTER SET "UTF-16LE"
Plan:
LogicalTableScan(table=[[default_catalog, default_database, upsert_kafka_confluent_avro]])
SQL: CREATE VIEW `upsert_kafka_confluent_avro__view`
AS
SELECT `upsert_kafka_confluent_avro`.`user_id`, `upsert_kafka_confluent_avro`.`name`
FROM `default_catalog`.`default_database`.`upsert_kafka_confluent_avro` AS `upsert_kafka_confluent_avro`
>>>flink-sql-no-functions.sql
CREATE TABLE `kafka_avro_table` (
  `user_id` INTEGER,
  `name` STRING
) WITH (
  'connector' = 'kafka',
  'topic' = 'users-avro',
  'properties.bootstrap.servers' = 'localhost:9092',
  'properties.group.id' = 'flink-kafka-avro-group',
  'format' = 'avro'
);
CREATE TABLE `kafka_safe_json_table` (
  `user_id` INTEGER,
  `name` STRING
) WITH (
  'connector' = 'kafka-safe',
  'topic' = 'users-json-avro',
  'properties.bootstrap.servers' = 'localhost:9092',
  'properties.group.id' = 'test',
  'format' = 'flexible-json'
);
CREATE TABLE `kafka_safe_csv_table` (
  `user_id` INTEGER,
  `name` STRING
) WITH (
  'connector' = 'kafka-safe',
  'topic' = 'users-csv-avro',
  'properties.bootstrap.servers' = 'localhost:9092',
  'properties.group.id' = 'test',
  'format' = 'flexible-csv'
);
CREATE TABLE `upsert_kafka_confluent_avro` (
  `user_id` INTEGER,
  `name` STRING,
  PRIMARY KEY (`user_id`) NOT ENFORCED
) WITH (
  'connector' = 'upsert-kafka',
  'topic' = 'users-upsert',
  'properties.group.id' = 'flink-kafka-avro-group',
  'properties.bootstrap.servers' = 'localhost:9092',
  'key.format' = 'avro-confluent',
  'key.avro-confluent.schema-registry.url' = 'http://localhost:8081',
  'value.format' = 'avro-confluent',
  'value.avro-confluent.schema-registry.url' = 'http://localhost:8081'
);
CREATE TABLE `kafka_debezium_table` (
  `user_id` INTEGER,
  `name` STRING
) WITH (
  'connector' = 'kafka',
  'topic' = 'users-debezium',
  'properties.group.id' = 'flink-kafka-avro-group',
  'properties.bootstrap.servers' = 'localhost:9092',
  'format' = 'debezium-json'
);
CREATE TABLE `iceberg_parquet_table` (
  `user_id` INTEGER,
  `name` STRING
)
PARTITIONED BY (`user_id`)
WITH (
  'connector' = 'iceberg',
  'catalog-name' = 'local_fs',
  'catalog-type' = 'hadoop',
  'warehouse' = 'file:///tmp/iceberg/warehouse',
  'format-version' = '2',
  'write.format.default' = 'parquet'
);
CREATE TABLE `_jdbc_table` (
  `user_id` INTEGER,
  `name` STRING
) WITH (
  'connector' = 'jdbc',
  'url' = 'jdbc:postgresql://localhost:5432/mydb',
  'table-name' = 'users',
  'username' = 'myuser',
  'password' = 'mypassword'
);
CREATE TABLE `iceberg_parquet_table_1` (
  `user_id` INTEGER,
  `name` VARCHAR(2147483647) CHARACTER SET `UTF-16LE`,
  `__pk_hash` CHAR(32) CHARACTER SET `UTF-16LE`,
  PRIMARY KEY (`__pk_hash`) NOT ENFORCED
) WITH (
  'connector' = 'jdbc-sqrl',
  'password' = '${JDBC_PASSWORD}',
  'driver' = 'org.postgresql.Driver',
  'username' = '${JDBC_USERNAME}',
  'url' = 'jdbc:postgresql://${JDBC_AUTHORITY}',
  'table-name' = 'iceberg_parquet_table'
);
CREATE TABLE `kafka_avro_table_2` (
  `user_id` INTEGER,
  `name` VARCHAR(2147483647) CHARACTER SET `UTF-16LE`,
  `__pk_hash` CHAR(32) CHARACTER SET `UTF-16LE`,
  PRIMARY KEY (`__pk_hash`) NOT ENFORCED
) WITH (
  'connector' = 'jdbc-sqrl',
  'password' = '${JDBC_PASSWORD}',
  'driver' = 'org.postgresql.Driver',
  'username' = '${JDBC_USERNAME}',
  'url' = 'jdbc:postgresql://${JDBC_AUTHORITY}',
  'table-name' = 'kafka_avro_table'
);
CREATE TABLE `kafka_debezium_table_3` (
  `user_id` INTEGER,
  `name` VARCHAR(2147483647) CHARACTER SET `UTF-16LE`,
  `__pk_hash` CHAR(32) CHARACTER SET `UTF-16LE`,
  PRIMARY KEY (`__pk_hash`) NOT ENFORCED
) WITH (
  'connector' = 'jdbc-sqrl',
  'password' = '${JDBC_PASSWORD}',
  'driver' = 'org.postgresql.Driver',
  'username' = '${JDBC_USERNAME}',
  'url' = 'jdbc:postgresql://${JDBC_AUTHORITY}',
  'table-name' = 'kafka_debezium_table'
);
CREATE TABLE `kafka_safe_csv_table_4` (
  `user_id` INTEGER,
  `name` VARCHAR(2147483647) CHARACTER SET `UTF-16LE`,
  `__pk_hash` CHAR(32) CHARACTER SET `UTF-16LE`,
  PRIMARY KEY (`__pk_hash`) NOT ENFORCED
) WITH (
  'connector' = 'jdbc-sqrl',
  'password' = '${JDBC_PASSWORD}',
  'driver' = 'org.postgresql.Driver',
  'username' = '${JDBC_USERNAME}',
  'url' = 'jdbc:postgresql://${JDBC_AUTHORITY}',
  'table-name' = 'kafka_safe_csv_table'
);
CREATE TABLE `kafka_safe_json_table_5` (
  `user_id` INTEGER,
  `name` VARCHAR(2147483647) CHARACTER SET `UTF-16LE`,
  `__pk_hash` CHAR(32) CHARACTER SET `UTF-16LE`,
  PRIMARY KEY (`__pk_hash`) NOT ENFORCED
) WITH (
  'connector' = 'jdbc-sqrl',
  'password' = '${JDBC_PASSWORD}',
  'driver' = 'org.postgresql.Driver',
  'username' = '${JDBC_USERNAME}',
  'url' = 'jdbc:postgresql://${JDBC_AUTHORITY}',
  'table-name' = 'kafka_safe_json_table'
);
CREATE TABLE `upsert_kafka_confluent_avro_6` (
  `user_id` INTEGER NOT NULL,
  `name` VARCHAR(2147483647) CHARACTER SET `UTF-16LE`,
  PRIMARY KEY (`user_id`) NOT ENFORCED
) WITH (
  'connector' = 'jdbc-sqrl',
  'password' = '${JDBC_PASSWORD}',
  'driver' = 'org.postgresql.Driver',
  'username' = '${JDBC_USERNAME}',
  'url' = 'jdbc:postgresql://${JDBC_AUTHORITY}',
  'table-name' = 'upsert_kafka_confluent_avro'
);
EXECUTE STATEMENT SET BEGIN
INSERT INTO `default_catalog`.`default_database`.`iceberg_parquet_table_1`
(SELECT `user_id`, `name`, `hash_columns`(`user_id`, `name`) AS `__pk_hash`
 FROM `default_catalog`.`default_database`.`iceberg_parquet_table`)
;
INSERT INTO `default_catalog`.`default_database`.`kafka_avro_table_2`
 (SELECT `user_id`, `name`, `hash_columns`(`user_id`, `name`) AS `__pk_hash`
  FROM `default_catalog`.`default_database`.`kafka_avro_table`)
 ;
 INSERT INTO `default_catalog`.`default_database`.`kafka_debezium_table_3`
  (SELECT `user_id`, `name`, `hash_columns`(`user_id`, `name`) AS `__pk_hash`
   FROM `default_catalog`.`default_database`.`kafka_debezium_table`)
  ;
  INSERT INTO `default_catalog`.`default_database`.`kafka_safe_csv_table_4`
   (SELECT `user_id`, `name`, `hash_columns`(`user_id`, `name`) AS `__pk_hash`
    FROM `default_catalog`.`default_database`.`kafka_safe_csv_table`)
   ;
   INSERT INTO `default_catalog`.`default_database`.`kafka_safe_json_table_5`
    (SELECT `user_id`, `name`, `hash_columns`(`user_id`, `name`) AS `__pk_hash`
     FROM `default_catalog`.`default_database`.`kafka_safe_json_table`)
    ;
    INSERT INTO `default_catalog`.`default_database`.`upsert_kafka_confluent_avro_6`
     (SELECT *
      FROM `default_catalog`.`default_database`.`upsert_kafka_confluent_avro`)
     ;
     END
>>>kafka.json
{
  "topics" : [ ]
}
>>>postgres-schema.sql
CREATE TABLE IF NOT EXISTS "iceberg_parquet_table" ("user_id" INTEGER , "name" TEXT , "__pk_hash" TEXT  , PRIMARY KEY ("__pk_hash"));
CREATE TABLE IF NOT EXISTS "kafka_avro_table" ("user_id" INTEGER , "name" TEXT , "__pk_hash" TEXT  , PRIMARY KEY ("__pk_hash"));
CREATE TABLE IF NOT EXISTS "kafka_debezium_table" ("user_id" INTEGER , "name" TEXT , "__pk_hash" TEXT  , PRIMARY KEY ("__pk_hash"));
CREATE TABLE IF NOT EXISTS "kafka_safe_csv_table" ("user_id" INTEGER , "name" TEXT , "__pk_hash" TEXT  , PRIMARY KEY ("__pk_hash"));
CREATE TABLE IF NOT EXISTS "kafka_safe_json_table" ("user_id" INTEGER , "name" TEXT , "__pk_hash" TEXT  , PRIMARY KEY ("__pk_hash"));
CREATE TABLE IF NOT EXISTS "upsert_kafka_confluent_avro" ("user_id" INTEGER NOT NULL, "name" TEXT  , PRIMARY KEY ("user_id"))
>>>postgres-views.sql

>>>vertx.json
{
  "model" : {
    "queries" : [
      {
        "type" : "args",
        "parentType" : "Query",
        "fieldName" : "iceberg_parquet_table",
        "exec" : {
          "arguments" : [
            {
              "type" : "variable",
              "path" : "offset"
            },
            {
              "type" : "variable",
              "path" : "limit"
            }
          ],
          "query" : {
            "type" : "SqlQuery",
            "sql" : "SELECT \"user_id\", \"name\"\nFROM \"iceberg_parquet_table\"",
            "parameters" : [ ],
            "pagination" : "LIMIT_AND_OFFSET",
            "database" : "POSTGRES"
          }
        }
      },
      {
        "type" : "args",
        "parentType" : "Query",
        "fieldName" : "kafka_avro_table",
        "exec" : {
          "arguments" : [
            {
              "type" : "variable",
              "path" : "offset"
            },
            {
              "type" : "variable",
              "path" : "limit"
            }
          ],
          "query" : {
            "type" : "SqlQuery",
            "sql" : "SELECT \"user_id\", \"name\"\nFROM \"kafka_avro_table\"",
            "parameters" : [ ],
            "pagination" : "LIMIT_AND_OFFSET",
            "database" : "POSTGRES"
          }
        }
      },
      {
        "type" : "args",
        "parentType" : "Query",
        "fieldName" : "kafka_debezium_table",
        "exec" : {
          "arguments" : [
            {
              "type" : "variable",
              "path" : "offset"
            },
            {
              "type" : "variable",
              "path" : "limit"
            }
          ],
          "query" : {
            "type" : "SqlQuery",
            "sql" : "SELECT \"user_id\", \"name\"\nFROM \"kafka_debezium_table\"",
            "parameters" : [ ],
            "pagination" : "LIMIT_AND_OFFSET",
            "database" : "POSTGRES"
          }
        }
      },
      {
        "type" : "args",
        "parentType" : "Query",
        "fieldName" : "kafka_safe_csv_table",
        "exec" : {
          "arguments" : [
            {
              "type" : "variable",
              "path" : "offset"
            },
            {
              "type" : "variable",
              "path" : "limit"
            }
          ],
          "query" : {
            "type" : "SqlQuery",
            "sql" : "SELECT \"user_id\", \"name\"\nFROM \"kafka_safe_csv_table\"",
            "parameters" : [ ],
            "pagination" : "LIMIT_AND_OFFSET",
            "database" : "POSTGRES"
          }
        }
      },
      {
        "type" : "args",
        "parentType" : "Query",
        "fieldName" : "kafka_safe_json_table",
        "exec" : {
          "arguments" : [
            {
              "type" : "variable",
              "path" : "offset"
            },
            {
              "type" : "variable",
              "path" : "limit"
            }
          ],
          "query" : {
            "type" : "SqlQuery",
            "sql" : "SELECT \"user_id\", \"name\"\nFROM \"kafka_safe_json_table\"",
            "parameters" : [ ],
            "pagination" : "LIMIT_AND_OFFSET",
            "database" : "POSTGRES"
          }
        }
      },
      {
        "type" : "args",
        "parentType" : "Query",
        "fieldName" : "upsert_kafka_confluent_avro",
        "exec" : {
          "arguments" : [
            {
              "type" : "variable",
              "path" : "offset"
            },
            {
              "type" : "variable",
              "path" : "limit"
            }
          ],
          "query" : {
            "type" : "SqlQuery",
            "sql" : "SELECT *\nFROM \"upsert_kafka_confluent_avro\"",
            "parameters" : [ ],
            "pagination" : "LIMIT_AND_OFFSET",
            "database" : "POSTGRES"
          }
        }
      }
    ],
    "mutations" : [ ],
    "subscriptions" : [ ],
    "schema" : {
      "type" : "string",
      "schema" : "\"An arbitrary precision signed integer\"\nscalar GraphQLBigInteger\n\ntype Query {\n  iceberg_parquet_table(limit: Int = 10, offset: Int = 0): [iceberg_parquet_table!]\n  kafka_avro_table(limit: Int = 10, offset: Int = 0): [kafka_avro_table!]\n  kafka_debezium_table(limit: Int = 10, offset: Int = 0): [kafka_debezium_table!]\n  kafka_safe_csv_table(limit: Int = 10, offset: Int = 0): [kafka_safe_csv_table!]\n  kafka_safe_json_table(limit: Int = 10, offset: Int = 0): [kafka_safe_json_table!]\n  upsert_kafka_confluent_avro(limit: Int = 10, offset: Int = 0): [upsert_kafka_confluent_avro!]\n}\n\ntype iceberg_parquet_table {\n  user_id: Int\n  name: String\n}\n\ntype kafka_avro_table {\n  user_id: Int\n  name: String\n}\n\ntype kafka_debezium_table {\n  user_id: Int\n  name: String\n}\n\ntype kafka_safe_csv_table {\n  user_id: Int\n  name: String\n}\n\ntype kafka_safe_json_table {\n  user_id: Int\n  name: String\n}\n\ntype upsert_kafka_confluent_avro {\n  user_id: Int!\n  name: String\n}\n"
    }
  }
}
