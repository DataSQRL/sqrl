>>>pipeline_explain.txt
=== Data
ID:     data_2
Type:   stream
Stage:  streams
Primary Key: id
Timestamp  : timestamp
Schema:
 - id: BIGINT NOT NULL
 - epoch_timestamp: BIGINT NOT NULL
 - some_value: VARCHAR(2147483647) CHARACTER SET "UTF-16LE" NOT NULL
 - timestamp: TIMESTAMP_WITH_LOCAL_TIME_ZONE(3) NOT NULL
Plan:
LogicalTableScan(table=[[data_1]], hints=[[[WatermarkHint inheritPath:[] options:[3]]]]) hints[WatermarkHint options:[3]]

=== kafka.LogData
ID:     data_2_1
Type:   export
Stage:  streams
Inputs: data_2

>>>flink.json
{
  "flinkSql" : [
    "CREATE TEMPORARY FUNCTION IF NOT EXISTS `epochmillitotimestamp` AS 'com.datasqrl.time.EpochMilliToTimestamp' LANGUAGE JAVA;",
    "CREATE TEMPORARY TABLE `data_1` (\n  `id` BIGINT NOT NULL,\n  `epoch_timestamp` BIGINT NOT NULL,\n  `some_value` VARCHAR(2147483647) CHARACTER SET `UTF-16LE` NOT NULL,\n  `timestamp` AS EPOCHMILLITOTIMESTAMP(`epoch_timestamp`),\n  PRIMARY KEY (`id`) NOT ENFORCED,\n  WATERMARK FOR `timestamp` AS (`timestamp` - INTERVAL '0.001' SECOND)\n) WITH (\n  'fields.id.end' = '9',\n  'fields.epoch_timestamp.kind' = 'sequence',\n  'number-of-rows' = '10',\n  'connector' = 'datagen',\n  'fields.epoch_timestamp.end' = '1719319565',\n  'fields.some_value.kind' = 'random',\n  'fields.id.kind' = 'sequence',\n  'fields.id.start' = '0',\n  'fields.epoch_timestamp.start' = '1719318565'\n);",
    "CREATE TEMPORARY TABLE `data_2` (\n  `id` BIGINT NOT NULL,\n  `epoch_timestamp` BIGINT NOT NULL,\n  `some_value` VARCHAR(2147483647) CHARACTER SET `UTF-16LE` NOT NULL,\n  `timestamp` TIMESTAMP(3) WITH LOCAL TIME ZONE NOT NULL,\n  PRIMARY KEY (`id`) NOT ENFORCED\n) WITH (\n  'password' = '${JDBC_PASSWORD}',\n  'connector' = 'jdbc-sqrl',\n  'driver' = 'org.postgresql.Driver',\n  'table-name' = 'data_2',\n  'url' = '${JDBC_URL}',\n  'username' = '${JDBC_USERNAME}'\n);",
    "CREATE TEMPORARY TABLE `data_2_1` (\n  `id` BIGINT NOT NULL,\n  `epoch_timestamp` BIGINT NOT NULL,\n  `some_value` VARCHAR(2147483647) CHARACTER SET `UTF-16LE` NOT NULL,\n  `timestamp` TIMESTAMP(3) WITH LOCAL TIME ZONE NOT NULL\n) WITH (\n  'properties.bootstrap.servers' = '${PROPERTIES_BOOTSTRAP_SERVERS}',\n  'properties.auto.offset.reset' = 'earliest',\n  'connector' = 'kafka',\n  'format' = 'flexible-json',\n  'properties.group.id' = '${PROPERTIES_GROUP_ID}',\n  'topic' = 'data_2',\n  'scan.startup.mode' = 'group-offsets'\n);",
    "CREATE VIEW `table$1`\nAS\nSELECT *\nFROM `data_1`;",
    "CREATE VIEW `table$2`\nAS\nSELECT *\nFROM `data_1`;",
    "EXECUTE STATEMENT SET BEGIN\nINSERT INTO `data_2`\n(SELECT *\nFROM `table$1`)\n;\nINSERT INTO `data_2_1`\n(SELECT *\nFROM `table$2`)\n;\nEND;"
  ],
  "connectors" : [
    "datagen",
    "jdbc-sqrl",
    "kafka"
  ],
  "formats" : [
    "flexible-json"
  ]
}
>>>kafka.json
{
  "topics" : [
    {
      "name" : "data_2",
      "numPartitions" : 1,
      "replicationFactor" : 1,
      "replicasAssignments" : { },
      "config" : { }
    }
  ]
}
>>>postgres.json
{
  "ddl" : [
    {
      "name" : "data_2",
      "columns" : [
        "\"id\" BIGINT NOT NULL",
        "\"epoch_timestamp\" BIGINT NOT NULL",
        "\"some_value\" TEXT NOT NULL",
        "\"timestamp\" TIMESTAMP WITH TIME ZONE NOT NULL"
      ],
      "primaryKeys" : [
        "\"id\""
      ],
      "sql" : "CREATE TABLE IF NOT EXISTS data_2 (\"id\" BIGINT NOT NULL,\"epoch_timestamp\" BIGINT NOT NULL,\"some_value\" TEXT NOT NULL,\"timestamp\" TIMESTAMP WITH TIME ZONE NOT NULL , PRIMARY KEY (\"id\"));"
    }
  ]
}
>>>vertx.json
{
  "model" : {
    "coords" : [
      {
        "type" : "args",
        "parentType" : "Query",
        "fieldName" : "Data",
        "matchs" : [
          {
            "arguments" : [
              {
                "type" : "variable",
                "type" : "variable",
                "path" : "id"
              },
              {
                "type" : "variable",
                "type" : "variable",
                "path" : "limit"
              },
              {
                "type" : "variable",
                "type" : "variable",
                "path" : "offset"
              }
            ],
            "query" : {
              "type" : "PagedJdbcQuery",
              "type" : "PagedJdbcQuery",
              "sql" : "SELECT \"id\", \"epoch_timestamp\", \"some_value\", \"timestamp\", 1 AS \"__pk\"\nFROM \"data_2\"\nWHERE \"id\" = $1\nORDER BY \"timestamp\" DESC NULLS LAST, 1",
              "parameters" : [
                {
                  "type" : "arg",
                  "type" : "arg",
                  "path" : "id"
                }
              ]
            }
          },
          {
            "arguments" : [
              {
                "type" : "variable",
                "type" : "variable",
                "path" : "limit"
              },
              {
                "type" : "variable",
                "type" : "variable",
                "path" : "offset"
              }
            ],
            "query" : {
              "type" : "PagedJdbcQuery",
              "type" : "PagedJdbcQuery",
              "sql" : "SELECT *\nFROM \"data_2\"\nORDER BY \"timestamp\" DESC NULLS LAST, \"id\"",
              "parameters" : [ ]
            }
          }
        ]
      }
    ],
    "mutations" : [ ],
    "subscriptions" : [ ],
    "schema" : {
      "type" : "string",
      "type" : "string",
      "schema" : "type Data {\n  id: Float!\n  epoch_timestamp: Float!\n  some_value: String!\n  timestamp: DateTime!\n}\n\n\"An RFC-3339 compliant DateTime Scalar\"\nscalar DateTime\n\ntype Query {\n  Data(id: Float, limit: Int = 10, offset: Int = 0): [Data!]\n}\n"
    }
  }
}
