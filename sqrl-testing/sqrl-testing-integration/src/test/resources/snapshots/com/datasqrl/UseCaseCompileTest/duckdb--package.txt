>>>pipeline_explain.txt
=== MyTable1
ID:     default_catalog.default_database.MyTable1
Type:   stream
Stage:  iceberg
Inputs: default_catalog.default_database._MyApplications1
Annotations:
 - stream-root: _Applications
Primary Key: -
Timestamp  : -
Schema:
 - id: BIGINT NOT NULL
 - hello: CHAR(12) CHARACTER SET "UTF-16LE" NOT NULL
Plan:
LogicalProject(id=[$0], hello=['hello world1'])
  LogicalTableScan(table=[[default_catalog, default_database, _MyApplications1]])
SQL: CREATE VIEW MyTable1 AS  SELECT id, 'hello world1' as hello FROM _MyApplications1;

=== MyTable2
ID:     default_catalog.default_database.MyTable2
Type:   stream
Stage:  iceberg
Inputs: default_catalog.default_database._MyApplications2
Annotations:
 - stream-root: _Applications
Primary Key: -
Timestamp  : -
Schema:
 - id: BIGINT NOT NULL
 - hello: CHAR(12) CHARACTER SET "UTF-16LE" NOT NULL
Plan:
LogicalProject(id=[$0], hello=['hello world2'])
  LogicalTableScan(table=[[default_catalog, default_database, _MyApplications2]])
SQL: CREATE VIEW MyTable2 AS  SELECT id, 'hello world2' as hello FROM _MyApplications2;

=== MyTable3
ID:     default_catalog.default_database.MyTable3
Type:   stream
Stage:  postgres
Inputs: default_catalog.default_database._MyApplications2
Annotations:
 - stream-root: _Applications
Primary Key: -
Timestamp  : -
Schema:
 - id: BIGINT NOT NULL
 - hello: CHAR(12) CHARACTER SET "UTF-16LE" NOT NULL
Plan:
LogicalProject(id=[$0], hello=['hello world3'])
  LogicalTableScan(table=[[default_catalog, default_database, _MyApplications2]])
SQL: CREATE VIEW MyTable3 AS  SELECT id, 'hello world3' as hello FROM _MyApplications2;

=== _Applications
ID:     default_catalog.default_database._Applications
Type:   stream
Stage:  flink
Inputs: default_catalog.default_database._Applications__base
Annotations:
 - stream-root: _Applications
Primary Key: id, updated_at
Timestamp  : updated_at
Schema:
 - id: BIGINT NOT NULL
 - customer_id: BIGINT NOT NULL
 - loan_type_id: BIGINT NOT NULL
 - amount: DOUBLE NOT NULL
 - duration: BIGINT NOT NULL
 - application_date: TIMESTAMP_WITH_LOCAL_TIME_ZONE(3) NOT NULL
 - updated_at: TIMESTAMP_LTZ(3) *ROWTIME* NOT NULL
Plan:
LogicalWatermarkAssigner(rowtime=[updated_at], watermark=[-($6, 1:INTERVAL SECOND)])
  LogicalTableScan(table=[[default_catalog, default_database, _Applications]])
SQL: CREATE VIEW `_Applications__view`
AS
SELECT `_Applications`.`id`, `_Applications`.`customer_id`, `_Applications`.`loan_type_id`, `_Applications`.`amount`, `_Applications`.`duration`, `_Applications`.`application_date`, `_Applications`.`updated_at`
FROM `default_catalog`.`default_database`.`_Applications` AS `_Applications`
=== _MyApplications1
ID:     default_catalog.default_database._MyApplications1
Type:   stream
Stage:  flink
Inputs: default_catalog.default_database._Applications
Annotations:
 - stream-root: _Applications
Primary Key: id, updated_at
Timestamp  : updated_at
Schema:
 - id: BIGINT NOT NULL
 - customer_id: BIGINT NOT NULL
 - loan_type_id: BIGINT NOT NULL
 - amount: DOUBLE NOT NULL
 - duration: BIGINT NOT NULL
 - application_date: TIMESTAMP_WITH_LOCAL_TIME_ZONE(3) NOT NULL
 - updated_at: TIMESTAMP_LTZ(3) *ROWTIME* NOT NULL
Plan:
LogicalProject(id=[$0], customer_id=[$1], loan_type_id=[$2], amount=[$3], duration=[$4], application_date=[$5], updated_at=[$6])
  LogicalTableScan(table=[[default_catalog, default_database, _Applications]])
SQL: CREATE VIEW _MyApplications1 AS  SELECT * FROM _Applications;

=== _MyApplications2
ID:     default_catalog.default_database._MyApplications2
Type:   stream
Stage:  flink
Inputs: default_catalog.default_database._Applications
Annotations:
 - stream-root: _Applications
Primary Key: id, updated_at
Timestamp  : updated_at
Schema:
 - id: BIGINT NOT NULL
 - customer_id: BIGINT NOT NULL
 - loan_type_id: BIGINT NOT NULL
 - amount: DOUBLE NOT NULL
 - duration: BIGINT NOT NULL
 - application_date: TIMESTAMP_WITH_LOCAL_TIME_ZONE(3) NOT NULL
 - updated_at: TIMESTAMP_LTZ(3) *ROWTIME* NOT NULL
Plan:
LogicalProject(id=[$0], customer_id=[$1], loan_type_id=[$2], amount=[$3], duration=[$4], application_date=[$5], updated_at=[$6])
  LogicalFilter(condition=[>=($0, 0)])
    LogicalTableScan(table=[[default_catalog, default_database, _Applications]])
SQL: CREATE VIEW _MyApplications2 AS  SELECT * FROM _Applications WHERE id >= 0;

>>>flink-sql-no-functions.sql
CREATE TEMPORARY TABLE `_Applications__schema` (
  `id` BIGINT NOT NULL,
  `customer_id` BIGINT NOT NULL,
  `loan_type_id` BIGINT NOT NULL,
  `amount` DOUBLE NOT NULL,
  `duration` BIGINT NOT NULL,
  `application_date` TIMESTAMP(3) WITH LOCAL TIME ZONE NOT NULL,
  `updated_at` TIMESTAMP(3) WITH LOCAL TIME ZONE NOT NULL
) WITH (
  'connector' = 'datagen'
);
CREATE TABLE `_Applications` (
  PRIMARY KEY (`id`, `updated_at`) NOT ENFORCED,
  WATERMARK FOR `updated_at` AS `updated_at` - INTERVAL '0.001' SECOND
) WITH (
  'format' = 'flexible-json',
  'path' = '${DATA_PATH}/applications.jsonl',
  'source.monitor-interval' = '1',
  'connector' = 'filesystem'
)
LIKE `_Applications__schema`;
CREATE VIEW `_MyApplications1`
AS
SELECT *
FROM `_Applications`;
CREATE VIEW `_MyApplications2`
AS
SELECT *
FROM `_Applications`
WHERE `id` >= 0;
CREATE VIEW `MyTable1`
AS
SELECT `id`, 'hello world1' AS `hello`
FROM `_MyApplications1`;
CREATE VIEW `MyTable2`
AS
SELECT `id`, 'hello world2' AS `hello`
FROM `_MyApplications2`;
CREATE VIEW `MyTable3`
AS
SELECT `id`, 'hello world3' AS `hello`
FROM `_MyApplications2`;
CREATE TABLE `_MyApplications1_1` (
  `id` BIGINT NOT NULL,
  `customer_id` BIGINT NOT NULL,
  `loan_type_id` BIGINT NOT NULL,
  `amount` DOUBLE NOT NULL,
  `duration` BIGINT NOT NULL,
  `application_date` TIMESTAMP(3) WITH LOCAL TIME ZONE NOT NULL,
  `updated_at` TIMESTAMP(3) WITH LOCAL TIME ZONE NOT NULL,
  PRIMARY KEY (`id`, `updated_at`) NOT ENFORCED
)
PARTITIONED BY (`id`)
WITH (
  'catalog-name' = 'mydatabase',
  'catalog-table' = '_MyApplications1',
  'catalog-type' = 'hadoop',
  'connector' = 'iceberg',
  'warehouse' = '/tmp/duckdb',
  'write.parquet.page-size-bytes' = '1000'
);
CREATE TABLE `_MyApplications2_2` (
  `id` BIGINT NOT NULL,
  `customer_id` BIGINT NOT NULL,
  `loan_type_id` BIGINT NOT NULL,
  `amount` DOUBLE NOT NULL,
  `duration` BIGINT NOT NULL,
  `application_date` TIMESTAMP(3) WITH LOCAL TIME ZONE NOT NULL,
  `updated_at` TIMESTAMP(3) WITH LOCAL TIME ZONE NOT NULL,
  PRIMARY KEY (`id`, `updated_at`) NOT ENFORCED
) WITH (
  'catalog-name' = 'mydatabase',
  'catalog-table' = '_MyApplications2',
  'catalog-type' = 'hadoop',
  'connector' = 'iceberg',
  'warehouse' = '/tmp/duckdb',
  'write.parquet.page-size-bytes' = '1000'
);
CREATE TABLE `_MyApplications2_3` (
  `id` BIGINT NOT NULL,
  `customer_id` BIGINT NOT NULL,
  `loan_type_id` BIGINT NOT NULL,
  `amount` DOUBLE NOT NULL,
  `duration` BIGINT NOT NULL,
  `application_date` TIMESTAMP(3) WITH LOCAL TIME ZONE NOT NULL,
  `updated_at` TIMESTAMP(3) WITH LOCAL TIME ZONE NOT NULL,
  PRIMARY KEY (`id`, `updated_at`) NOT ENFORCED
) WITH (
  'connector' = 'jdbc-sqrl',
  'driver' = 'org.postgresql.Driver',
  'password' = '${POSTGRES_PASSWORD}',
  'table-name' = '_MyApplications2',
  'url' = 'jdbc:postgresql://${POSTGRES_AUTHORITY}',
  'username' = '${POSTGRES_USERNAME}'
);
EXECUTE STATEMENT SET BEGIN
INSERT INTO `default_catalog`.`default_database`.`_MyApplications1_1`
(SELECT *
 FROM `default_catalog`.`default_database`.`_MyApplications1`)
;
INSERT INTO `default_catalog`.`default_database`.`_MyApplications2_2`
 (SELECT *
  FROM `default_catalog`.`default_database`.`_MyApplications2`)
 ;
 INSERT INTO `default_catalog`.`default_database`.`_MyApplications2_3`
  (SELECT *
   FROM `default_catalog`.`default_database`.`_MyApplications2`)
  ;
  END
>>>iceberg-duckdb-schema.sql
CREATE TABLE IF NOT EXISTS "_MyApplications1" ("id" BIGINT NOT NULL, "customer_id" BIGINT NOT NULL, "loan_type_id" BIGINT NOT NULL, "amount" DOUBLE PRECISION NOT NULL, "duration" BIGINT NOT NULL, "application_date" TIMESTAMP WITH TIME ZONE NOT NULL, "updated_at" TIMESTAMP WITH TIME ZONE NOT NULL, PRIMARY KEY ("id","updated_at"));
CREATE TABLE IF NOT EXISTS "_MyApplications2" ("id" BIGINT NOT NULL, "customer_id" BIGINT NOT NULL, "loan_type_id" BIGINT NOT NULL, "amount" DOUBLE PRECISION NOT NULL, "duration" BIGINT NOT NULL, "application_date" TIMESTAMP WITH TIME ZONE NOT NULL, "updated_at" TIMESTAMP WITH TIME ZONE NOT NULL, PRIMARY KEY ("id","updated_at"))
>>>iceberg-duckdb-views.sql

>>>iceberg-schema.sql
CREATE TABLE IF NOT EXISTS "_MyApplications1" ("id" BIGINT NOT NULL, "customer_id" BIGINT NOT NULL, "loan_type_id" BIGINT NOT NULL, "amount" DOUBLE PRECISION NOT NULL, "duration" BIGINT NOT NULL, "application_date" TIMESTAMP WITH TIME ZONE NOT NULL, "updated_at" TIMESTAMP WITH TIME ZONE NOT NULL, PRIMARY KEY ("id","updated_at")) PARTITIONED BY ("id");
CREATE TABLE IF NOT EXISTS "_MyApplications2" ("id" BIGINT NOT NULL, "customer_id" BIGINT NOT NULL, "loan_type_id" BIGINT NOT NULL, "amount" DOUBLE PRECISION NOT NULL, "duration" BIGINT NOT NULL, "application_date" TIMESTAMP WITH TIME ZONE NOT NULL, "updated_at" TIMESTAMP WITH TIME ZONE NOT NULL, PRIMARY KEY ("id","updated_at"))
>>>iceberg-views.sql

>>>kafka.json
{
  "topics" : [ ],
  "testRunnerTopics" : [ ]
}
>>>postgres-schema.sql
CREATE TABLE IF NOT EXISTS "_MyApplications2" ("id" BIGINT NOT NULL, "customer_id" BIGINT NOT NULL, "loan_type_id" BIGINT NOT NULL, "amount" DOUBLE PRECISION NOT NULL, "duration" BIGINT NOT NULL, "application_date" TIMESTAMP WITH TIME ZONE NOT NULL, "updated_at" TIMESTAMP WITH TIME ZONE NOT NULL, PRIMARY KEY ("id","updated_at"));

CREATE INDEX IF NOT EXISTS "_MyApplications2_hash_c0" ON "_MyApplications2" USING hash ("id")
>>>postgres-views.sql
CREATE OR REPLACE VIEW "MyTable3"("id", "hello") AS SELECT "id", 'hello world3' AS "hello"
FROM "_MyApplications2"
>>>vertx.json
{
  "models" : {
    "v1" : {
      "queries" : [
        {
          "type" : "args",
          "parentType" : "Query",
          "fieldName" : "MyTable1",
          "exec" : {
            "arguments" : [
              {
                "type" : "variable",
                "path" : "offset"
              },
              {
                "type" : "variable",
                "path" : "limit"
              }
            ],
            "query" : {
              "type" : "SqlQuery",
              "sql" : "SELECT \"id\", 'hello world1' AS \"hello\"\nFROM \"iceberg_scan\"('/tmp/duckdb/default_database/_MyApplications1', ALLOW_MOVED_PATHS = TRUE)",
              "parameters" : [ ],
              "pagination" : "LIMIT_AND_OFFSET",
              "cacheDurationMs" : 0,
              "database" : "DUCKDB"
            }
          }
        },
        {
          "type" : "args",
          "parentType" : "Query",
          "fieldName" : "MyTable2",
          "exec" : {
            "arguments" : [
              {
                "type" : "variable",
                "path" : "offset"
              },
              {
                "type" : "variable",
                "path" : "limit"
              }
            ],
            "query" : {
              "type" : "SqlQuery",
              "sql" : "SELECT \"id\", 'hello world2' AS \"hello\"\nFROM \"iceberg_scan\"('/tmp/duckdb/default_database/_MyApplications2', ALLOW_MOVED_PATHS = TRUE)",
              "parameters" : [ ],
              "pagination" : "LIMIT_AND_OFFSET",
              "cacheDurationMs" : 0,
              "database" : "DUCKDB"
            }
          }
        },
        {
          "type" : "args",
          "parentType" : "Query",
          "fieldName" : "MyTable3",
          "exec" : {
            "arguments" : [
              {
                "type" : "variable",
                "path" : "offset"
              },
              {
                "type" : "variable",
                "path" : "limit"
              }
            ],
            "query" : {
              "type" : "SqlQuery",
              "sql" : "SELECT \"id\", 'hello world3' AS \"hello\"\nFROM \"_MyApplications2\"",
              "parameters" : [ ],
              "pagination" : "LIMIT_AND_OFFSET",
              "cacheDurationMs" : 0,
              "database" : "POSTGRES"
            }
          }
        }
      ],
      "mutations" : [ ],
      "subscriptions" : [ ],
      "operations" : [
        {
          "function" : {
            "name" : "GetMyTable1",
            "parameters" : {
              "type" : "object",
              "properties" : {
                "offset" : {
                  "type" : "integer"
                },
                "limit" : {
                  "type" : "integer"
                }
              },
              "required" : [ ]
            }
          },
          "format" : "JSON",
          "apiQuery" : {
            "query" : "query MyTable1($limit: Int = 10, $offset: Int = 0) {\nMyTable1(limit: $limit, offset: $offset) {\nid\nhello\n}\n\n}",
            "queryName" : "MyTable1",
            "operationType" : "QUERY"
          },
          "mcpMethod" : "TOOL",
          "restMethod" : "GET",
          "uriTemplate" : "queries/MyTable1{?offset,limit}"
        },
        {
          "function" : {
            "name" : "GetMyTable2",
            "parameters" : {
              "type" : "object",
              "properties" : {
                "offset" : {
                  "type" : "integer"
                },
                "limit" : {
                  "type" : "integer"
                }
              },
              "required" : [ ]
            }
          },
          "format" : "JSON",
          "apiQuery" : {
            "query" : "query MyTable2($limit: Int = 10, $offset: Int = 0) {\nMyTable2(limit: $limit, offset: $offset) {\nid\nhello\n}\n\n}",
            "queryName" : "MyTable2",
            "operationType" : "QUERY"
          },
          "mcpMethod" : "TOOL",
          "restMethod" : "GET",
          "uriTemplate" : "queries/MyTable2{?offset,limit}"
        },
        {
          "function" : {
            "name" : "GetMyTable3",
            "parameters" : {
              "type" : "object",
              "properties" : {
                "offset" : {
                  "type" : "integer"
                },
                "limit" : {
                  "type" : "integer"
                }
              },
              "required" : [ ]
            }
          },
          "format" : "JSON",
          "apiQuery" : {
            "query" : "query MyTable3($limit: Int = 10, $offset: Int = 0) {\nMyTable3(limit: $limit, offset: $offset) {\nid\nhello\n}\n\n}",
            "queryName" : "MyTable3",
            "operationType" : "QUERY"
          },
          "mcpMethod" : "TOOL",
          "restMethod" : "GET",
          "uriTemplate" : "queries/MyTable3{?offset,limit}"
        }
      ],
      "schema" : {
        "type" : "string",
        "schema" : "\"An RFC-3339 compliant Full Date Scalar\"\nscalar Date\n\n\"A DateTime scalar that handles both full RFC3339 and shorter timestamp formats\"\nscalar DateTime\n\n\"A JSON scalar\"\nscalar JSON\n\n\"24-hour clock time value string in the format `hh:mm:ss` or `hh:mm:ss.sss`.\"\nscalar LocalTime\n\n\"A 64-bit signed integer\"\nscalar Long\n\ntype MyTable1 {\n  id: Long!\n  hello: String!\n}\n\ntype MyTable2 {\n  id: Long!\n  hello: String!\n}\n\ntype MyTable3 {\n  id: Long!\n  hello: String!\n}\n\ntype Query {\n  MyTable1(limit: Int = 10, offset: Int = 0): [MyTable1!]\n  MyTable2(limit: Int = 10, offset: Int = 0): [MyTable2!]\n  MyTable3(limit: Int = 10, offset: Int = 0): [MyTable3!]\n}\n\nenum _McpMethodType {\n  NONE\n  TOOL\n  RESOURCE\n}\n\nenum _RestMethodType {\n  NONE\n  GET\n  POST\n}\n\ndirective @api(mcp: _McpMethodType, rest: _RestMethodType, uri: String) on QUERY | MUTATION | FIELD_DEFINITION\n"
      }
    }
  }
}
