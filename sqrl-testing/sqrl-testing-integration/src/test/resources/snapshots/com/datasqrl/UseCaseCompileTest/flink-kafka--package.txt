>>>pipeline_explain.txt
=== ApplicationStatus
ID:     default_catalog.default_database.ApplicationStatus
Type:   stream
Stage:  flink
Inputs: default_catalog.default_database._ApplicationUpdates, default_catalog.default_database._Applications, default_catalog.default_database._LoanTypes
Annotations:
 - stream-root: _ApplicationUpdates
Primary Key: -
Timestamp  : event_time
Schema:
 - status: VARCHAR(2147483647) CHARACTER SET "UTF-16LE" NOT NULL
 - message: VARCHAR(2147483647) CHARACTER SET "UTF-16LE" NOT NULL
 - event_time: TIMESTAMP_LTZ(3) *ROWTIME* NOT NULL
 - id: BIGINT NOT NULL
 - customer_id: BIGINT NOT NULL
 - loan_type_id: BIGINT NOT NULL
 - amount: DOUBLE NOT NULL
 - duration: BIGINT NOT NULL
 - max_amount: DOUBLE NOT NULL
 - min_amount: DOUBLE NOT NULL
Plan:
LogicalProject(status=[$1], message=[$2], event_time=[$3], id=[$4], customer_id=[$5], loan_type_id=[$6], amount=[$7], duration=[$8], max_amount=[$15], min_amount=[$16])
  LogicalCorrelate(correlation=[$cor3], joinType=[inner], requiredColumns=[{3, 6}])
    LogicalCorrelate(correlation=[$cor2], joinType=[inner], requiredColumns=[{0, 3}])
      LogicalTableScan(table=[[default_catalog, default_database, _ApplicationUpdates]])
      LogicalFilter(condition=[=($0, $cor2.loan_application_id)])
        LogicalSnapshot(period=[$cor2.event_time])
          LogicalTableScan(table=[[default_catalog, default_database, _Applications]])
    LogicalFilter(condition=[=($0, $cor3.loan_type_id)])
      LogicalSnapshot(period=[$cor3.event_time])
        LogicalTableScan(table=[[default_catalog, default_database, _LoanTypes]])
SQL: CREATE VIEW `ApplicationStatus` AS  SELECT u.status, u.message, u.event_time, a.id, a.customer_id, a.loan_type_id,
                            a.amount, a.duration, t.max_amount, t.min_amount
                     FROM _ApplicationUpdates u JOIN _Applications FOR SYSTEM_TIME AS OF u.`event_time` a ON a.id = u.loan_application_id
                                               JOIN _LoanTypes FOR SYSTEM_TIME AS OF u.`event_time` t ON t.id = a.loan_type_id;

=== _ApplicationUpdates
ID:     default_catalog.default_database._ApplicationUpdates
Type:   stream
Stage:  flink
Inputs: default_catalog.default_database._ApplicationUpdates__base
Annotations:
 - stream-root: _ApplicationUpdates
Primary Key: loan_application_id, event_time
Timestamp  : event_time
Schema:
 - loan_application_id: BIGINT NOT NULL
 - status: VARCHAR(2147483647) CHARACTER SET "UTF-16LE" NOT NULL
 - message: VARCHAR(2147483647) CHARACTER SET "UTF-16LE" NOT NULL
 - event_time: TIMESTAMP_LTZ(3) *ROWTIME* NOT NULL
Plan:
LogicalWatermarkAssigner(rowtime=[event_time], watermark=[$3])
  LogicalTableScan(table=[[default_catalog, default_database, _ApplicationUpdates]])
SQL: CREATE VIEW `_ApplicationUpdates__view`
AS
SELECT `_ApplicationUpdates`.`loan_application_id`, `_ApplicationUpdates`.`status`, `_ApplicationUpdates`.`message`, `_ApplicationUpdates`.`event_time`
FROM `default_catalog`.`default_database`.`_ApplicationUpdates` AS `_ApplicationUpdates`
=== _Applications
ID:     default_catalog.default_database._Applications
Type:   state
Stage:  flink
Inputs: default_catalog.default_database._ApplicationsStream
Annotations:
 - mostRecentDistinct: true
 - stream-root: _ApplicationsStream
Primary Key: id
Timestamp  : updated_at
Schema:
 - id: BIGINT NOT NULL
 - customer_id: BIGINT NOT NULL
 - loan_type_id: BIGINT NOT NULL
 - amount: DOUBLE NOT NULL
 - duration: BIGINT NOT NULL
 - application_date: TIMESTAMP_WITH_LOCAL_TIME_ZONE(3) NOT NULL
 - updated_at: TIMESTAMP_LTZ(3) *ROWTIME* NOT NULL
Plan:
LogicalProject(id=[$0], customer_id=[$1], loan_type_id=[$2], amount=[$3], duration=[$4], application_date=[$5], updated_at=[$6])
  LogicalFilter(condition=[=($7, 1)])
    LogicalProject(id=[$0], customer_id=[$1], loan_type_id=[$2], amount=[$3], duration=[$4], application_date=[$5], updated_at=[$6], __sqrlinternal_rownum=[ROW_NUMBER() OVER (PARTITION BY $0 ORDER BY $6 DESC NULLS LAST)])
      LogicalTableScan(table=[[default_catalog, default_database, _ApplicationsStream]])
SQL: CREATE VIEW `_Applications`
AS
SELECT `id`, `customer_id`, `loan_type_id`, `amount`, `duration`, `application_date`, `updated_at`
FROM (SELECT `id`, `customer_id`, `loan_type_id`, `amount`, `duration`, `application_date`, `updated_at`, ROW_NUMBER() OVER (PARTITION BY `id` ORDER BY `updated_at` DESC NULLS LAST) AS `__sqrlinternal_rownum`
  FROM `default_catalog`.`default_database`.`_ApplicationsStream`) AS `t`
WHERE `__sqrlinternal_rownum` = 1
=== _ApplicationsStream
ID:     default_catalog.default_database._ApplicationsStream
Type:   stream
Stage:  flink
Inputs: default_catalog.default_database._ApplicationsStream__base
Annotations:
 - stream-root: _ApplicationsStream
Primary Key: id, updated_at
Timestamp  : updated_at
Schema:
 - id: BIGINT NOT NULL
 - customer_id: BIGINT NOT NULL
 - loan_type_id: BIGINT NOT NULL
 - amount: DOUBLE NOT NULL
 - duration: BIGINT NOT NULL
 - application_date: TIMESTAMP_WITH_LOCAL_TIME_ZONE(3) NOT NULL
 - updated_at: TIMESTAMP_LTZ(3) *ROWTIME* NOT NULL
Plan:
LogicalWatermarkAssigner(rowtime=[updated_at], watermark=[-($6, 1:INTERVAL SECOND)])
  LogicalTableScan(table=[[default_catalog, default_database, _ApplicationsStream]])
SQL: CREATE VIEW `_ApplicationsStream__view`
AS
SELECT `_ApplicationsStream`.`id`, `_ApplicationsStream`.`customer_id`, `_ApplicationsStream`.`loan_type_id`, `_ApplicationsStream`.`amount`, `_ApplicationsStream`.`duration`, `_ApplicationsStream`.`application_date`, `_ApplicationsStream`.`updated_at`
FROM `default_catalog`.`default_database`.`_ApplicationsStream` AS `_ApplicationsStream`
=== _FakeStreamJoin
ID:     default_catalog.default_database._FakeStreamJoin
Type:   stream
Stage:  flink
Inputs: default_catalog.default_database._ApplicationUpdates, default_catalog.default_database._ApplicationsStream
Primary Key: -
Timestamp  : event_time
Schema:
 - id: BIGINT NOT NULL
 - status: VARCHAR(2147483647) CHARACTER SET "UTF-16LE" NOT NULL
 - message: VARCHAR(2147483647) CHARACTER SET "UTF-16LE" NOT NULL
 - event_time: TIMESTAMP_LTZ(3) *ROWTIME* NOT NULL
Plan:
LogicalProject(id=[$4], status=[$1], message=[$2], event_time=[$3])
  LogicalJoin(condition=[=($6, $0)], joinType=[inner])
    LogicalTableScan(table=[[default_catalog, default_database, _ApplicationUpdates]])
    LogicalTableScan(table=[[default_catalog, default_database, _ApplicationsStream]])
SQL: CREATE VIEW `_FakeStreamJoin` AS  SELECT a.id, u.status, u.message, u.event_time
                     FROM _ApplicationUpdates u JOIN _ApplicationsStream a ON a.loan_type_id = u.loan_application_id;

=== _LoanTypes
ID:     default_catalog.default_database._LoanTypes
Type:   state
Stage:  flink
Inputs: default_catalog.default_database._LoanTypesStream
Annotations:
 - mostRecentDistinct: true
 - stream-root: _LoanTypesStream
Primary Key: id
Timestamp  : updated_at
Schema:
 - id: BIGINT NOT NULL
 - name: VARCHAR(2147483647) CHARACTER SET "UTF-16LE" NOT NULL
 - description: VARCHAR(2147483647) CHARACTER SET "UTF-16LE" NOT NULL
 - interest_rate: DOUBLE NOT NULL
 - max_amount: DOUBLE NOT NULL
 - min_amount: DOUBLE NOT NULL
 - max_duration: BIGINT NOT NULL
 - min_duration: BIGINT NOT NULL
 - updated_at: TIMESTAMP_LTZ(3) *ROWTIME* NOT NULL
Plan:
LogicalProject(id=[$0], name=[$1], description=[$2], interest_rate=[$3], max_amount=[$4], min_amount=[$5], max_duration=[$6], min_duration=[$7], updated_at=[$8])
  LogicalFilter(condition=[=($9, 1)])
    LogicalProject(id=[$0], name=[$1], description=[$2], interest_rate=[$3], max_amount=[$4], min_amount=[$5], max_duration=[$6], min_duration=[$7], updated_at=[$8], __sqrlinternal_rownum=[ROW_NUMBER() OVER (PARTITION BY $0 ORDER BY $8 DESC NULLS LAST)])
      LogicalTableScan(table=[[default_catalog, default_database, _LoanTypesStream]])
SQL: CREATE VIEW `_LoanTypes`
AS
SELECT `id`, `name`, `description`, `interest_rate`, `max_amount`, `min_amount`, `max_duration`, `min_duration`, `updated_at`
FROM (SELECT `id`, `name`, `description`, `interest_rate`, `max_amount`, `min_amount`, `max_duration`, `min_duration`, `updated_at`, ROW_NUMBER() OVER (PARTITION BY `id` ORDER BY `updated_at` DESC NULLS LAST) AS `__sqrlinternal_rownum`
  FROM `default_catalog`.`default_database`.`_LoanTypesStream`) AS `t`
WHERE `__sqrlinternal_rownum` = 1
=== _LoanTypesStream
ID:     default_catalog.default_database._LoanTypesStream
Type:   stream
Stage:  flink
Inputs: default_catalog.default_database._LoanTypesStream__base
Annotations:
 - stream-root: _LoanTypesStream
Primary Key: id, updated_at
Timestamp  : updated_at
Schema:
 - id: BIGINT NOT NULL
 - name: VARCHAR(2147483647) CHARACTER SET "UTF-16LE" NOT NULL
 - description: VARCHAR(2147483647) CHARACTER SET "UTF-16LE" NOT NULL
 - interest_rate: DOUBLE NOT NULL
 - max_amount: DOUBLE NOT NULL
 - min_amount: DOUBLE NOT NULL
 - max_duration: BIGINT NOT NULL
 - min_duration: BIGINT NOT NULL
 - updated_at: TIMESTAMP_LTZ(3) *ROWTIME* NOT NULL
Plan:
LogicalWatermarkAssigner(rowtime=[updated_at], watermark=[-($8, 1:INTERVAL SECOND)])
  LogicalTableScan(table=[[default_catalog, default_database, _LoanTypesStream]])
SQL: CREATE VIEW `_LoanTypesStream__view`
AS
SELECT `_LoanTypesStream`.`id`, `_LoanTypesStream`.`name`, `_LoanTypesStream`.`description`, `_LoanTypesStream`.`interest_rate`, `_LoanTypesStream`.`max_amount`, `_LoanTypesStream`.`min_amount`, `_LoanTypesStream`.`max_duration`, `_LoanTypesStream`.`min_duration`, `_LoanTypesStream`.`updated_at`
FROM `default_catalog`.`default_database`.`_LoanTypesStream` AS `_LoanTypesStream`
=== StreamJoin
ID:     logger.StreamJoin
Type:   export
Stage:  flink
Inputs: default_catalog.default_database._FakeStreamJoin

>>>flink-sql-no-functions.sql
CREATE TEMPORARY TABLE `_ApplicationsStream__schema` (
  `id` BIGINT NOT NULL,
  `customer_id` BIGINT NOT NULL,
  `loan_type_id` BIGINT NOT NULL,
  `amount` DOUBLE NOT NULL,
  `duration` BIGINT NOT NULL,
  `application_date` TIMESTAMP(3) WITH LOCAL TIME ZONE NOT NULL,
  `updated_at` TIMESTAMP(3) WITH LOCAL TIME ZONE NOT NULL
) WITH (
  'connector' = 'datagen'
);
CREATE TABLE `_ApplicationsStream` (
  PRIMARY KEY (`id`, `updated_at`) NOT ENFORCED,
  WATERMARK FOR `updated_at` AS `updated_at` - INTERVAL '0.001' SECOND
) WITH (
  'format' = 'flexible-json',
  'path' = '${DATA_PATH}/applications.jsonl',
  'source.monitor-interval' = '10 sec',
  'connector' = 'filesystem'
)
LIKE `_ApplicationsStream__schema`;
CREATE TEMPORARY TABLE `_LoanTypesStream__schema` (
  `id` BIGINT NOT NULL,
  `name` VARCHAR(2147483647) CHARACTER SET `UTF-16LE` NOT NULL,
  `description` VARCHAR(2147483647) CHARACTER SET `UTF-16LE` NOT NULL,
  `interest_rate` DOUBLE NOT NULL,
  `max_amount` DOUBLE NOT NULL,
  `min_amount` DOUBLE NOT NULL,
  `max_duration` BIGINT NOT NULL,
  `min_duration` BIGINT NOT NULL,
  `updated_at` TIMESTAMP(3) WITH LOCAL TIME ZONE NOT NULL
) WITH (
  'connector' = 'datagen'
);
CREATE TABLE `_LoanTypesStream` (
  PRIMARY KEY (`id`, `updated_at`) NOT ENFORCED,
  WATERMARK FOR `updated_at` AS `updated_at` - INTERVAL '0.001' SECOND
) WITH (
  'format' = 'flexible-json',
  'path' = '${DATA_PATH}/loan_types.jsonl',
  'source.monitor-interval' = '10 sec',
  'connector' = 'filesystem'
)
LIKE `_LoanTypesStream__schema`;
CREATE TEMPORARY TABLE `_ApplicationUpdates__schema` (
  `loan_application_id` BIGINT NOT NULL,
  `status` VARCHAR(2147483647) CHARACTER SET `UTF-16LE` NOT NULL,
  `message` VARCHAR(2147483647) CHARACTER SET `UTF-16LE` NOT NULL,
  `event_time` TIMESTAMP(3) WITH LOCAL TIME ZONE NOT NULL
) WITH (
  'connector' = 'datagen'
);
CREATE TABLE `_ApplicationUpdates` (
  PRIMARY KEY (`loan_application_id`, `event_time`) NOT ENFORCED,
  WATERMARK FOR `event_time` AS `event_time`
) WITH (
  'format' = 'flexible-json',
  'path' = '${DATA_PATH}/application_updates.jsonl',
  'connector' = 'filesystem'
)
LIKE `_ApplicationUpdates__schema`;
CREATE VIEW `_Applications`
AS
SELECT `id`, `customer_id`, `loan_type_id`, `amount`, `duration`, `application_date`, `updated_at`
FROM (SELECT `id`, `customer_id`, `loan_type_id`, `amount`, `duration`, `application_date`, `updated_at`, ROW_NUMBER() OVER (PARTITION BY `id` ORDER BY `updated_at` DESC NULLS LAST) AS `__sqrlinternal_rownum`
  FROM `default_catalog`.`default_database`.`_ApplicationsStream`) AS `t`
WHERE `__sqrlinternal_rownum` = 1;
CREATE VIEW `_LoanTypes`
AS
SELECT `id`, `name`, `description`, `interest_rate`, `max_amount`, `min_amount`, `max_duration`, `min_duration`, `updated_at`
FROM (SELECT `id`, `name`, `description`, `interest_rate`, `max_amount`, `min_amount`, `max_duration`, `min_duration`, `updated_at`, ROW_NUMBER() OVER (PARTITION BY `id` ORDER BY `updated_at` DESC NULLS LAST) AS `__sqrlinternal_rownum`
  FROM `default_catalog`.`default_database`.`_LoanTypesStream`) AS `t`
WHERE `__sqrlinternal_rownum` = 1;
CREATE VIEW `ApplicationStatus`
AS
SELECT `u`.`status`, `u`.`message`, `u`.`event_time`, `a`.`id`, `a`.`customer_id`, `a`.`loan_type_id`, `a`.`amount`, `a`.`duration`, `t`.`max_amount`, `t`.`min_amount`
FROM `_ApplicationUpdates` AS `u`
 INNER JOIN `_Applications` FOR SYSTEM_TIME AS OF `u`.`event_time` AS `a` ON `a`.`id` = `u`.`loan_application_id`
 INNER JOIN `_LoanTypes` FOR SYSTEM_TIME AS OF `u`.`event_time` AS `t` ON `t`.`id` = `a`.`loan_type_id`;
CREATE VIEW `ApplicationStatusTest`
AS
SELECT COUNT(*) AS `total_count`, CAST(SUM(`amount`) AS INTEGER) AS `total_amount`
FROM `ApplicationStatus`;
CREATE VIEW `_FakeStreamJoin`
AS
SELECT `a`.`id`, `u`.`status`, `u`.`message`, `u`.`event_time`
FROM `_ApplicationUpdates` AS `u`
 INNER JOIN `_ApplicationsStream` AS `a` ON `a`.`loan_type_id` = `u`.`loan_application_id`;
CREATE TABLE `ApplicationStatus_1` (
  `status` VARCHAR(2147483647) CHARACTER SET `UTF-16LE` NOT NULL,
  `message` VARCHAR(2147483647) CHARACTER SET `UTF-16LE` NOT NULL,
  `event_time` TIMESTAMP(3) WITH LOCAL TIME ZONE NOT NULL,
  `id` BIGINT NOT NULL,
  `customer_id` BIGINT NOT NULL,
  `loan_type_id` BIGINT NOT NULL,
  `amount` DOUBLE NOT NULL,
  `duration` BIGINT NOT NULL,
  `max_amount` DOUBLE NOT NULL,
  `min_amount` DOUBLE NOT NULL
) WITH (
  'connector' = 'kafka',
  'flexible-json.timestamp-format.standard' = 'ISO-8601',
  'format' = 'flexible-json',
  'properties.bootstrap.servers' = '${KAFKA_BOOTSTRAP_SERVERS}',
  'properties.group.id' = '${KAFKA_GROUP_ID}',
  'topic' = 'ApplicationStatus'
);
CREATE TABLE `StreamJoin_2` (
  `id` BIGINT NOT NULL,
  `status` VARCHAR(2147483647) CHARACTER SET `UTF-16LE` NOT NULL,
  `message` VARCHAR(2147483647) CHARACTER SET `UTF-16LE` NOT NULL,
  `event_time` TIMESTAMP(3) WITH LOCAL TIME ZONE NOT NULL
) WITH (
  'connector' = 'print',
  'print-identifier' = 'StreamJoin'
);
EXECUTE STATEMENT SET BEGIN
INSERT INTO `default_catalog`.`default_database`.`ApplicationStatus_1`
SELECT *
 FROM `default_catalog`.`default_database`.`ApplicationStatus`
;
INSERT INTO `default_catalog`.`default_database`.`StreamJoin_2`
 SELECT *
  FROM `default_catalog`.`default_database`.`_FakeStreamJoin`
 ;
 END
>>>kafka.json
{
  "topics" : [
    {
      "topicName" : "ApplicationStatus",
      "tableName" : "ApplicationStatus_1",
      "format" : "flexible-json",
      "numPartitions" : 1,
      "replicationFactor" : 3,
      "type" : "SUBSCRIPTION",
      "config" : { }
    }
  ],
  "testRunnerTopics" : [
    {
      "topicName" : "input_topic1",
      "tableName" : "input_topic1",
      "numPartitions" : 1,
      "replicationFactor" : 3,
      "type" : "SUBSCRIPTION",
      "config" : { }
    },
    {
      "topicName" : "input_topic2",
      "tableName" : "input_topic2",
      "numPartitions" : 1,
      "replicationFactor" : 3,
      "type" : "SUBSCRIPTION",
      "config" : { }
    }
  ]
}
