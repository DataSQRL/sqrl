"use strict";(self.webpackChunkdocumentation=self.webpackChunkdocumentation||[]).push([[8130],{7735:e=>{e.exports=JSON.parse('{"archive":{"blogPosts":[{"id":"data-platform-automation","metadata":{"permalink":"/blog/data-platform-automation","source":"@site/blog/2025-12-10-data-platform-automation.md","title":"Data Platform Automation","description":"DataSQRL is an open-source data automation framework that provides guardrails and feedback for AI coding agents to develop and operate data pipelines, data products, and data APIs autonomously.","date":"2025-12-10T00:00:00.000Z","tags":[{"inline":false,"label":"Release","permalink":"/blog/tags/release","description":"DataSQRL release announcements"}],"readingTime":16.835,"hasTruncateMarker":false,"authors":[{"name":"Matthias Broecheler","title":"CEO of DataSQRL","url":"https://github.com/mbroecheler","page":{"permalink":"/blog/authors/matthias"},"socials":{"linkedin":"https://www.linkedin.com/in/matthiasbroecheler/","github":"https://github.com/mbroecheler","newsletter":"https://www.matthiasb.com"},"imageURL":"/img/headshots/matthias1.png","key":"matthias"}],"frontMatter":{"slug":"data-platform-automation","title":"Data Platform Automation","authors":["matthias"],"tags":["release"]},"unlisted":false,"nextItem":{"title":"DataSQRL 0.7 Release: The Data Delivery Interface","permalink":"/blog/datasqrl-0.7-release"}},"content":"<head>\\n  <meta property=\\"og:image\\" content=\\"/img/blog/release_0.7.0.png\\" />\\n  <meta name=\\"twitter:image\\" content=\\"/img/blog/release_0.7.0.png\\" />\\n</head>\\n\\n# Data Platform Automation\\n\\nDataSQRL is an open-source data automation framework that provides guardrails and feedback for AI coding agents to develop and operate data pipelines, data products, and data APIs autonomously.\\nYou can customize DataSQRL as the foundation of your self-driving data platform.\\n\\n## Why DataSQRL?\\n\\nTo understand *why you need DataSQRL*, let\'s start with the obvious question: **Aren\u2019t LLM-based coding agents good enough?**\\n\\nLLMs are powerful pattern-matching systems but lack grounded models of how data systems behave. Research such as Apple\u2019s 2024 paper [\u201cThe Illusion of Thinking\u201d](https://ml-site.cdn-apple.com/papers/the-illusion-of-thinking.pdf) shows that LLMs struggle with causal, temporal, and systems-level reasoning, which are the capabilities required to design reliable, multi-engine data pipelines. As a result, agents often generate brittle transformations, invalid mappings, and incorrect assumptions about how data moves and evolves over time.\\n\\nThat is why DataSQRL exists. Coding agents are impressive solution generators, but they require a **conceptual model**, **validator**, and **simulator** to ensure correctness, safety, and robustness. In most successful agentic systems deployed in the wild, the neural network\u2013driven reasoning is balanced by an external framework that supplies structure, constraints, and feedback loops.\\n\\nConsider self-driving cars as an example. Neural networks power environmental perception: recognizing lanes, traffic lights, pedestrians. Autonomous driving becomes possible only when this probabilistic perception is grounded in detailed maps, constraint systems, planning modules, and a world model based on physics and real\u2011world dynamics.\\n\\nThe neural network alone cannot infer the rules of the road or the relationships that make a driving environment coherent.\\n\\n**DataSQRL plays that grounding role for coding agents in the context of data platforms and data pipelines.**\\n\\nLet\'s look at the individual components of DataSQRL to understand how it provides that grounding for AI.\\n\\n## World Model\\n\\nFor the purposes of automating data platforms, a world model is a comprehensive conceptual framework that captures data schemas, data processing, and data serving to consumers. Specifically, we are building a world model for *non-transactional* data processing and serving.\\n\\nThe world model provides the frame of reference for implementing safe, reliable data processing systems.\\nIt captures the knowledge from [Database Systems: The Complete Book](http://infolab.stanford.edu/~ullman/dscb.html) combined with 25 years of data engineering experience.\\n\\nDataSQRL breaks the world model down into the *logical* and *physical* models.\\n\\n### Logical World Model\\n\\nThe logical world model expresses what data transformations are needed to produce the desired results.\\n\\nAn obvious choice for the logical model is [Codd\'s relational model](https://en.wikipedia.org/wiki/Relational_model) and its most popular implementation [SQL](https://en.wikipedia.org/wiki/SQL).\\n\\nThe relational model is widely adopted, proven, and provides a solid mathematical foundation. Most LLMs are trained on lots of SQL code and related documentation. And it is easy for humans to read. Modern versions of SQL (e.g., the SQL:2023 standard) support semi-structured data (JSON), polymorphic table functions, and complex pattern matching to address the messy reality of data platforms.\\n\\nWhile the relational model and SQL are a good starting point, we need two additions to achieve the expressibility that modern data platforms require.\\n\\n#### 1. Dataflow\\n\\nThe relational model uses set semantics. That is inconvenient for representing data flows which are important for data pipelines.\\n\\nJennifer Widom\'s [Continuous Query Language](http://infolab.stanford.edu/~arvind/papers/cql-vldbj.pdf) extends the relational model with data streams and relational operators for moving between streams and sets.\\n\\n[Flink SQL](https://nightlies.apache.org/flink/flink-docs-release-2.2/docs/dev/table/overview/), based on [Apache Calcite](https://calcite.apache.org/), is the most widely adopted implementation of this extended relational model. That\'s why we use Flink SQL as the basis of the logical model in DataSQRL.\\n\\nUsing a declarative language for the logical world model has a number of advantages from concise representation to deep introspection, but a practical shortcoming is the fact that some data transformations are easier to express imperatively. Flink SQL overcomes this by supporting [user defined functions](https://nightlies.apache.org/flink/flink-docs-release-2.2/docs/dev/table/functions/udfs/) and [custom table operators](https://nightlies.apache.org/flink/flink-docs-release-2.2/docs/dev/table/functions/ptfs/) in programming languages like Java. This gives us a logical world model grounded in relational algebra with flexible extensibility to express complex data transformations imperatively.\\n\\nDataSQRL builds on Flink SQL and adds 1) concise syntax for common transformations, 2) dbt-style templating, and 3) modular file management and importing. These features help with context management for LLMs by reducing the size of the active context that needs to be maintained during implementation and refinement.\\n\\n```sql\\n-- Ingest data from connected systems\\nIMPORT banking_data.AccountHoldersCDC; -- CDC stream from masterdata\\nIMPORT banking_data.AccountsCDC;       -- CDC stream from database\\nIMPORT banking_data.Transactions;      -- Kafka topic for transactions\\n\\n-- Convert the CDC stream of updates to the most recent version\\nAccounts       := DISTINCT AccountsCDC ON account_id ORDER BY update_time DESC;\\nAccountHolders := DISTINCT AccountHoldersCDC ON holder_id ORDER BY update_time DESC;\\n\\n-- Enrich debit transactions with creditor information using time-consistent join\\nSpendingTransactions :=\\nSELECT\\n    t.*,\\n    h.name AS creditor_name,\\n    h.type AS creditor_type\\nFROM Transactions t\\n         JOIN Accounts FOR SYSTEM_TIME AS OF t.tx_time a\\n           ON t.credit_account_id = a.account_id\\n         JOIN AccountHolders FOR SYSTEM_TIME AS OF t.tx_time h\\n           ON a.holder_id = h.holder_id;\\n```\\n\\nWe call this SQL dialect **SQRL**. You can [read the documentation](/docs/sqrl-language) for a complete reference of the SQRL language.\\n\\n#### 2. Serving\\n\\nIn addition to data processing, a critical function of data platforms is serving data to consumers as data streams, datasets, or data APIs. Data APIs, in particular, are becoming more important with the rise of operational analytics and MCP (Model Context Protocol) for making data accessible to AI agents.\\n\\nTo support data serving, DataSQRL adds support for endpoint definitions via table functions and explicit relationships.\\n\\nTable functions are part of the SQL:2016 standard and return entire tables as result sets computed dynamically based on provided parameters. In DataSQRL, table functions can be defined as API entry points.\\n\\n```sql\\n/** Retrieve spending transactions within the given time-range.\\n  from_time (inclusive) and to_time (exclusive) must be RFC-3339 compliant date time.\\n*/\\nSpendingTransactionsByTime(\\n  account_id STRING NOT NULL METADATA FROM \'auth.accountId\',\\n  from_time TIMESTAMP NOT NULL,\\n  to_time TIMESTAMP NOT NULL\\n) :=\\nSELECT * FROM SpendingTransactions\\nWHERE debit_account_id = :account_id\\n  AND :from_time <= tx_time\\n  AND :to_time > tx_time\\nORDER BY tx_time DESC;\\n```\\n\\nSecondly, DataSQRL allows for explicit relationship definitions between tables which are important for API-based data access where results need to include related entities like *most recent orders* or *recommendations for movie category*. The relational model does not support traversing through an entity-relationship model, which is usually handled by an object-relational mapping layer when exposing an API. To avoid that extra complexity and impedance mismatch in our logical world model, DataSQRL provides first-class support for relationships.\\n\\n```sql\\n-- Create a relationship between holder and accounts filtered by status\\nAccountHolders.accounts(status STRING) :=\\nSELECT * FROM Accounts a\\nWHERE a.holder_id = this.holder_id\\n  AND a.status = :status\\nORDER BY a.account_type ASC;\\n```\\n\\nWith the addition of access functions and relationships, the logical model maps directly to the entity-relationship model of GraphQL which DataSQRL uses as the logical model for API-based data retrieval. This gives DataSQRL a highly expressive interface with a simple extension of the logical model which retains conceptual simplicity of the world model.\\n\\nThe [interface documentation](/docs/interface) provides more details on the serving layer of DataSQRL.\\n\\n### Physical World Model\\n\\nThe physical world model represents *how* the data gets processed and served. It\'s a translation of the logical model into executable code that runs on actual data systems.\\n\\n#### Pipeline Architecture\\n\\nWith [hundreds of database systems](https://db-engines.com/) and many more data infrastructure choices, it is a daunting challenge to construct a simple and coherent physical model that is flexible enough to cover the diverse needs of data platforms.\\n\\nAfter analyzing a wide range of data platforms, we identified that the vast majority of implementations combine multiple data systems from these categories:\\n\\n* **Database**: for storing and querying data, e.g., PostgreSQL, MySQL, SQLServer, Apache Cassandra, Clickhouse, etc.\\n  * **Table Formats and Query Engines**: For analytic data, separating compute from storage can save money and support multiple consumers. DataSQRL conceptualizes this as a \\"disintegrated database\\" with table formats for storage (e.g., Apache Iceberg, DeltaLake, Apache Hudi) and query engines for access (e.g., Apache Spark, Apache Flink, Snowflake).\\n* **Data Processor**: for batch or realtime transformation of data, e.g., Apache Spark, Apache Flink, etc.\\n* **Log/Queue**: for reliably capturing data and moving it between data systems, e.g., Apache Kafka, RedPanda, Kinesis, etc.\\n* **Server**: for capturing and exposing data through an API\\n  * **Cache**: sits between server and database to speed up frequent queries over less-frequently changing data.\\n\\nWe call each data system an *engine* and the above categories *engine types*. When looking at data platform implementations at the level of engine types, we see about 15 patterns emerge (the 10 most popular are [documented here](/img/diagrams/architecture_pattern_overview.png)) that arrange those engines in a directed-acyclic graph (DAG) of data processing.\\n\\nHence, we use a computational DAG that models the flow of data from source to interface as the basis of our physical model. Each node in the DAG represents a logical computation mapped to be executed by an engine. Thus, the physical model provides an integrated view of the entire data flow.\\n\\n<img src=\\"/img/diagrams/automation_overview.png\\" alt=\\"An example physical model DAG\\" width=\\"100%\\"/>\\n\\n#### Transpiler\\n\\nWhile the physical model gives the AI control over what engine executes which computation, the actual mapping of logical to physical plan is done by a deterministic transpiler built in Apache Calcite. This avoids subtle bugs in data mapping and execution. The results of the transpilation are deployment assets which are executed by each engine. For example, the transpiler generates the database schema and queries for Postgres.\\n\\nIn the transpiler component, we make the following simplifying assumptions:\\n\\n* The database engines support a version of SQL (e.g., PostgreSQL, T-SQL) or a subset thereof (e.g., Cassandra Query Language)\\n* The data processor supports a SQL-based dialect (e.g., Spark SQL, Flink SQL)\\n* The log engine is Apache Kafka compatible (e.g., RedPanda, Azure EventHub)\\n* The server has a GraphQL execution engine.\\n\\nThis modular architecture allows new engines to be added by conforming to the engine type interface and implementing the transpiler rules in Calcite where needed. At the same time, it abstracts much of the physical plan mapping complexity from the AI, which produces higher quality results and preserves context for higher-level reasoning.\\n\\n#### Configuration\\n\\nDataSQRL uses a `package.json` file to configure the engines used to execute a data pipeline. The configuration file defines the overall pipeline topology, the individual engine configurations, and the compiler configuration. One file controls how the physical model is derived and executed, making it simple for the AI to experiment with and fine-tune the physical model.\\n\\n#### Interface\\n\\nFor the data serving interface, we use GraphQL schema as the physical model which bidirectionally maps to the access functions, table schema, and relationships defined in the logical plan by naming convention. GraphQL fields are mapped to SQL or Kafka queries based on their respective definitions in SQRL. This allows the AI to fine-tune the API within the GraphQL schema.\\n\\nFurthermore, REST and MCP APIs can be explicitly or implicitly defined through GraphQL operations. Implicit definition traverses the GraphQL schema from root query and mutation fields. Explicitly defined operations are provided as separate GraphQL files.\\n\\nUsing GraphQL as the physical model for the API combines simplicity with flexibility while benefiting from the prevalence of GraphQL in LLM training data.\\n\\n## Analysis\\n\\nThe world model gives AI coding agents a frame of reference to reason about data pipeline and data product implementations. DataSQRL provides analyses to support that reasoning and give users tools to validate the correctness and quality of the generated pipelines and APIs.\\n\\n### Verification & Introspection\\n\\nVerification and introspection complement the world model by reinforcing the concepts, rules, and dependencies. DataSQRL provides analysis at 3 levels: the logical model, physical model, and deployment assets (the code that gets executed by the engines).\\n\\n#### Logical\\n\\nAt the logical level, the DataSQRL compiler verifies syntax, schemas, and data flow semantics. This ensures that the data pipeline is logically coherent and that data integration points (e.g., between the SQL definitions and GraphQL schema) are consistent.\\n\\nOne of the benefits of using relational algebra as the basis for our world model is the ability to run rules and deep traversals over the operators in the relational algebra tree. The DataSQRL compiler uses Apache Calcite\'s rule and RelNode traversal framework to validate timestamp propagation, infer primary keys and data types, validate table types, and more. This validation component can be extended with custom rules to validate domain-specific semantics and constraints.\\n\\nThe validation component was designed to provide comprehensive context and suggested fixes for validation errors. In our testing, this produces significantly better results compared to the AI coding agent having to look up and reason about encountered errors.\\n\\n#### Physical\\n\\nOn compilation, DataSQRL produces the computational data flow DAG that represents the physical model. DataSQRL generates a visual representation as shown above for human validation as well as a concise textual representation that is consumed by coding agents as feedback on their proposed solutions and to reinforce the conceptual data flow of the world model.\\n\\n```text\\n=== CustomerTransaction\\nID:     default_catalog.default_database.CustomerTransaction\\nType:   stream\\nStage:  flink\\nInputs: default_catalog.default_database._CardAssignment, default_catalog.default_database._Merchant, default_catalog.sources.Transaction\\nAnnotations:\\n - stream-root: Transaction\\nPrimary Key: transactionId, time\\nTimestamp  : time\\nSchema:\\n - transactionId: BIGINT NOT NULL\\n - cardNo: VARCHAR(2147483647) CHARACTER SET \\"UTF-16LE\\" NOT NULL\\n - time: TIMESTAMP_LTZ(3) *ROWTIME* NOT NULL\\n - amount: DOUBLE NOT NULL\\n - merchantName: VARCHAR(2147483647) CHARACTER SET \\"UTF-16LE\\" NOT NULL\\n - category: VARCHAR(2147483647) CHARACTER SET \\"UTF-16LE\\" NOT NULL\\n - customerId: BIGINT NOT NULL\\n```\\n\\nThis representation of the physical model combines the inferences from the logical model with the mapping to execution engines to provide a source-to-interface definition of the data flow.\\n\\nValidation at the physical level ensures that data type mappings are consistent and that the engine assignments are valid, i.e., that an assigned engine has the capabilities to execute a particular operator. DataSQRL uses a capabilities component that extracts all requirements from an operator (e.g., temporal join, or a particular function execution) and validates that the engine supports the corresponding capabilities.\\n\\n#### Deployment Assets\\n\\nThe executable deployment assets are transpiled from the physical model. Since the transpilation is deterministic, this yields better results than letting the coding agent generate them, and it keeps the world model concise. However, we generate all deployment assets in a text representation that the coding agent can easily consume as another source of feedback. This is particularly useful during troubleshooting where the deployment assets are the ultimate source of truth of what is being executed and allow the agent to reason \\"backwards\\" to the logical model and how to fix it.\\n\\nSpecifically, we generate:\\n\\n* **Database**: The database schema, index structures, and (parameterized) SQL queries for all views and API entrypoints.\\n* **Data Processor**: The optimized physical plan and compiled execution plan.\\n* **Log**: The topic definitions and filter predicates.\\n* **Server**: The mapping from GraphQL fields to database or Kafka queries as well as operation definitions. Also, the GraphQL schema if it is not provided.\\n\\n### Optimization\\n\\nWhile LLMs\' reasoning ability under uncertainty is outstanding, we have found LLMs to perform worse and less consistently on deterministic optimization and constraint satisfaction problems. This finding is supported by a rich body of research in [neuro-symbolic AI](https://en.wikipedia.org/wiki/Neuro-symbolic_AI) which researches the integration of neural networks (like LLMs) with symbolic computation (e.g., solvers, planners) and has documented how neural networks alone fall short for such tasks.\\n\\nDataSQRL follows the neuro-symbolic approach and provides 3 types of planners for deterministic sub-tasks in the implementation and maintenance of data pipelines:\\n\\n#### Query Optimization\\n\\nQuery rewriting and optimization is a well-established technique for producing high-performing physical plans from relational algebra. DataSQRL relies on Apache Calcite\'s Volcano optimizer and HEP rule engine for this purpose.\\n\\n<img src=\\"/img/screenshots/banking_dag.png\\" alt=\\"Architecture view of DataSQRL pipelines\\" width=\\"100%\\"/>\\n\\n#### Physical Planning\\n\\nThe physical plan DAG is subject to a number of constraints forced by the real-world constraints of physical data movement. For example, to serve data on request in the API, the data needs to be available in the database - we cannot serve the data straight from a data processor, for example. These topological constraints combined with the capabilities of individual engines render many AI-proposed solutions invalid.\\n\\nHence, we implement a physical planner that uses a cost model with a greedy heuristic to assign logical operators to engines in a way that is consistent. The AI can provide hints to force the assignment of certain operators to specific engines which are added as constraints to the optimizer. This gives the AI control over allocations but shifts the burden of constraint satisfaction to a dedicated solver.\\n\\n#### Index Selection\\n\\nEfficiently querying data in the database or table format requires index structures (or partition + sort keys) that support the access paths to the data. Otherwise, we execute inefficient table scans.\\n\\nIndex structure selection is another optimization problem that is better handled by a dedicated optimizer. We use an adaptation of [Ullman et al\'s lattice framework for data cube selection](https://web.eecs.umich.edu/~jag/eecs584/papers/implementing_data_cube.pdf) since data cube selection and index selection are related problems with different optimization functions.\\n\\n## Real World Feedback\\n\\nA world model with complementary verification and introspection provides the foundation for reasoning about data pipelines and getting feedback. However, that feedback is limited to the plan and does not account for the complexities of actual execution. Real-world feedback is critical for iterative refinement of production-grade implementations and troubleshooting issues that arise in operation.\\n\\nDataSQRL provides two sources of real-world feedback: a simulator that\'s used at implementation time and telemetry collection from production deployments that captures the operational status of the pipeline.\\n\\n### Simulator\\n\\nThe DataSQRL simulator executes the configured engines with the generated deployment assets within a Docker environment. The simulator can replay events and records at their original timestamp, allowing for deterministic reproducibility of real-world scenarios. This is important for creating realistic test cases as well as reproducing production issues for troubleshooting and regression testing.\\n\\nBy capturing and faithfully replaying records at their original timestamp, the simulator ensures time-consistent semantics of data flows and makes it simple to construct complex test cases for scenarios like race conditions.\\n\\nSimulation is important in agentic coding workflows because it allows the agent to execute and refine the implementation in a feedback loop that is executed locally and can simulate scenarios that only occur rarely in production.\\n\\nRead more about invoking the [simulator](/docs/compiler#test-command) and writing [reproducible test cases](/docs/howto/testing).\\n\\n### Operations and Telemetry\\n\\nThe most important source of real-world feedback is observing the deployed data pipeline in a production environment (or a closely approximated pre-prod environment). Observability is critical for assessing the health of the pipeline and troubleshooting any issues that may occur.\\n\\nLogs and telemetry collection is a well-established practice for DevOps. What DataSQRL adds is the ability to link observed data back to the physical computation DAG so the agent can accurately reason about cause and effect. For data pipelines that execute across multiple engines, many complex errors arise at the boundary between systems - e.g., an issue in the data processing causes too many writes to the database which degrades performance - and require reasoning across individual systems. To automate such troubleshooting, we need to correlate observations back to the physical data flow and logical model.\\n\\nDataSQRL currently assumes production operation in Kubernetes or Docker and provides hooks for extracting logs and telemetry data.\\nCorrelating that data back to the physical model is not fully abstracted yet and requires individual setup for each deployment. This is work in progress.\\n\\n# Summary\\n\\nDataSQRL is a data automation framework that provides the foundational building blocks for autonomous data platforms. DataSQRL provides a logical and physical world model with validation and introspection for agentic implementation and refinement. It uses a neuro-symbolic approach for integrating solvers, planners, and transpilers to handle deterministic optimizations, following the principle of keeping the agentic context and task scope narrow to improve accuracy.\\nDataSQRL supplies real-world feedback to coding agents through its embedded simulator and telemetry orchestration.\\n\\nDataSQRL is a flexible framework that can be adapted to multiple data engines and extended for custom verification rules.\\n\\n[DataSQRL is open-source](https://github.com/DataSQRL/sqrl) so you can customize it to build a self-driving data platform tailored to your requirements.\\n\\n# Getting Started\\n\\nTo try out DataSQRL:\\n\\n1. [Build a project from scratch with DataSQRL](/docs/intro/getting-started) to see how the components of DataSQRL work\\n2. [Look at example projects](https://github.com/DataSQRL/datasqrl-examples) and run/modify them locally.\\n3. [Read the documentation](docs/intro/index)\\n4. [Check out the open-source project on GitHub](https://github.com/DataSQRL/sqrl)"},{"id":"datasqrl-0.7-release","metadata":{"permalink":"/blog/datasqrl-0.7-release","source":"@site/blog/2025-07-27-datasqrl-0.7.md","title":"DataSQRL 0.7 Release: The Data Delivery Interface","description":"|\\" width=\\"40%\\"/>","date":"2025-07-27T00:00:00.000Z","tags":[{"inline":false,"label":"Release","permalink":"/blog/tags/release","description":"DataSQRL release announcements"}],"readingTime":2.945,"hasTruncateMarker":true,"authors":[{"name":"Matthias Broecheler","title":"CEO of DataSQRL","url":"https://github.com/mbroecheler","page":{"permalink":"/blog/authors/matthias"},"socials":{"linkedin":"https://www.linkedin.com/in/matthiasbroecheler/","github":"https://github.com/mbroecheler","newsletter":"https://www.matthiasb.com"},"imageURL":"/img/headshots/matthias1.png","key":"matthias"}],"frontMatter":{"slug":"datasqrl-0.7-release","title":"DataSQRL 0.7 Release: The Data Delivery Interface","authors":["matthias"],"tags":["release"]},"unlisted":false,"prevItem":{"title":"Data Platform Automation","permalink":"/blog/data-platform-automation"},"nextItem":{"title":"Flink SQL Runner: Run Flink SQL Without JARs or Glue Code","permalink":"/blog/flinkrunner-announcement"}},"content":"<head>\\n  <meta property=\\"og:image\\" content=\\"/img/blog/release_0.7.0.png\\" />\\n  <meta name=\\"twitter:image\\" content=\\"/img/blog/release_0.7.0.png\\" />\\n</head>\\n\\n# DataSQRL 0.7 Release: The Data Delivery Interface\\n\\n<img src=\\"/img/blog/release_0.7.0.png\\" alt=\\"DataSQRL 0.7.0 Release >|\\" width=\\"40%\\"/>\\n\\nDataSQRL 0.7 marks a major milestone in our journey to automate data pipelines, thanks to significant improvements to the serving layer:\\n\\n* Support for the Model Context Protocol (MCP) for tooling and resource access\\n* REST API support\\n* JWT-based authentication and authorization\\n\\nThese features enable developers to build a wide range of production-ready data interfaces.\\nThis release also includes performance and configuration improvements to the serving layer of DataSQRL-generated pipelines.\\n\\nYou can find the full release notes and source code on our [GitHub release page](https://github.com/DataSQRL/sqrl/releases/tag/0.7.0). \\nTo update your local installation of DataSQRL, simply pull the latest Docker image:\\n```bash\\ndocker pull datasqrl/cmd:0.7.0\\n```\\n\\n## The Last Mile: Data Delivery\\n\\nData delivery is the final and most visible stage of any data pipeline. It\'s how users, applications, and AI agents actually access and consume data. Most enterprise data interactions happen through APIs, making the delivery interface a critical component. At DataSQRL, we\'ve invested heavily in automating the upstream parts of the pipeline: from Flink-powered data processing to Postgres-backed storage. With version 0.7, we turn our focus to the serving layer: introducing support for the Model Context Protocol (MCP) and REST APIs, as well as JWT-based authentication and authorization. These additions ensure seamless integration with most authentication providers and enable secure, token-based data access, with fine-grained authorization logic enforced directly in the SQRL script. This completes our vision of end-to-end pipeline automation, where consumption patterns inform data storage and processing\u2014closing the loop between data production and usage.\\n\\nCheck out the [interface documentation](/docs/interface) for more information.\\n\\n\x3c!--truncate--\x3e\\n\\n## Major Contributions\\n\\nIn addition to the flagship features\u2014MCP, REST, and JWT support\u2014which we\u2019ll discuss in more detail in future blog posts, the 0.7.0 release contains a number of additional features and improvements.\\n\\n### Configuration & CLI Improvements\\n- Refactored CLI and Flink configuration logic.\\n- Improved error messages during package config validation.\\n- Enforced predictable ordering and sorting of config keys.\\n- Migrated deprecated config key naming (`-dir` \u2192 `-folder`), now with compile-time warnings.\\n- Standardized configuration schema and structured logging to `/build/logs`.\\n\\n### Testing Infrastructure Enhancements\\n- Added new `sqrl-container-testing` module.\\n- Converted tests to use AssertJ.\\n- Increased test coverage for `SqrlConfig` and dependency mapping.\\n- Fixed test runner error reporting and exit code handling.\\n- Reworked dependent service startup to trigger post-compilation.\\n\\n### Flink-SQL Runner Integration\\n- Integrated `flink-sql-runner` into `DatasqrlRun`.\\n- Temporarily merged `sqrl-test` module into `sqrl-run`.\\n- Simplified Docker image setup (public `ghcr.io` images).\\n- Updated submodule paths and version to `0.7.0`.\\n\\n### Authentication & API Enhancements\\n- Added initial JWT-based authentication support.\\n- Published documentation for JWT and Swagger-based OpenAPI specs for REST endpoints.\\n- Added batch GraphQL mutation support with transactional semantics.\\n- Replaced `GraphQLBigInteger` with native `Long` handling.\\n\\n### Kafka & Runtime Improvements\\n- Kafka topic names now support templating.\\n- Added an async OpenAI test use case and resolved snapshot issues.\\n- Fixed intermittent WebSocket failures in `SubscriptionClient`.\\n\\n### Project Structure and CI Pipeline\\n- Simplified project structure and removed outdated dependency declarations.\\n- Refactored CI pipeline and added automated GitHub package cleanup workflow.\\n\\n### Updated Dependencies\\n\\nUpgraded versions for the following dependencies and patched critical vulnerabilities:\\n\\n- Apache Flink and Flink connectors (e.g., Postgres CDC)\\n- Vert.x and related plugins\\n- Apache Iceberg, DuckDB, PostgreSQL JDBC\\n- AWS SDK BOM\\n- JSON Schema Validator, Netty, OpenCSV\\n- Micrometer, Log4j, Reactor, Immutables, Testcontainers\\n- Maven plugins: Enforcer, GPG, Build-helper"},{"id":"flinkrunner-announcement","metadata":{"permalink":"/blog/flinkrunner-announcement","source":"@site/blog/2025-06-09-flink-sql-runner-annoucement.md","title":"Flink SQL Runner: Run Flink SQL Without JARs or Glue Code","description":"Apache Flink has long been a powerhouse for streaming and batch data processing. And with the rise of Flink SQL, developers can now build sophisticated pipelines using a declarative language they already know. But getting Flink SQL applications into production still comes with friction: packaging JARs, managing connectors, injecting secrets, and wiring up deployment infrastructure.","date":"2025-06-09T00:00:00.000Z","tags":[{"inline":false,"label":"Flink","permalink":"/blog/tags/flink","description":"Apache Flink related posts"},{"inline":false,"label":"DataSQRL","permalink":"/blog/tags/datasqrl","description":"Posts about DataSQRL"}],"readingTime":1.915,"hasTruncateMarker":true,"authors":[{"name":"Matthias Broecheler","title":"CEO of DataSQRL","url":"https://github.com/mbroecheler","page":{"permalink":"/blog/authors/matthias"},"socials":{"linkedin":"https://www.linkedin.com/in/matthiasbroecheler/","github":"https://github.com/mbroecheler","newsletter":"https://www.matthiasb.com"},"imageURL":"/img/headshots/matthias1.png","key":"matthias"}],"frontMatter":{"slug":"flinkrunner-announcement","title":"Flink SQL Runner: Run Flink SQL Without JARs or Glue Code","authors":["matthias"],"tags":["Flink","DataSQRL"]},"unlisted":false,"prevItem":{"title":"DataSQRL 0.7 Release: The Data Delivery Interface","permalink":"/blog/datasqrl-0.7-release"},"nextItem":{"title":"Defining Data Interfaces with FlinkSQL","permalink":"/blog/flinksql-extensions"}},"content":"<head>\\n  <meta property=\\"og:image\\" content=\\"/img/blog/flinksqlrunner_logo.png\\" />\\n  <meta name=\\"twitter:image\\" content=\\"/img/blog/flinksqlrunner_logo.png\\" />\\n</head>\\n\\n# Flink SQL Runner: Run Flink SQL Without JARs or Glue Code\\n\\nApache Flink has long been a powerhouse for streaming and batch data processing. And with the rise of Flink SQL, developers can now build sophisticated pipelines using a declarative language they already know. But getting Flink SQL applications into production still comes with friction: packaging JARs, managing connectors, injecting secrets, and wiring up deployment infrastructure.\\n\\n<img src=\\"/img/blog/flinksqlrunner_logo.png\\" alt=\\"FlinkSQL Runner >\\" width=\\"40%\\"/>\\n\\n[**Flink SQL Runner**](https://github.com/DataSQRL/flink-sql-runner/) is here to change that. It\'s an open-source toolkit that simplifies development, deployment, and operation of Flink SQL applications\u2014locally or in Kubernetes\u2014without manual JAR assembly or scripting custom infrastructure pipelines.\\n\\n\x3c!--truncate--\x3e\\n\\n## From SQL to Production, Minus the Plumbing\\n\\nImagine you\'re writing a Flink SQL job that reads from Kafka, enriches the data, and sinks to Iceberg. In theory, it\'s just SQL. But in practice, production deployment requires:\\n\\n* Assembling dependencies into a JAR\\n* Writing YAML to configure connectors\\n* Injecting secrets for different environments\\n\\nFlink SQL Runner eliminates those headaches. You get:\\n\\n* **Declarative execution** with SQL scripts or compiled plans\\n* **Simple deployments** on Kubernetes via Flink Operator\\n* **Environment isolation** with variable substitution and UDF packaging\\n\\nAll without leaving the SQL layer.\\n\\n### Key Features\\n\\n* **SQL and Plan Execution**: Run raw SQL scripts or pre-compiled execution plans.\\n* **Kubernetes-Native**: Built for the Flink Kubernetes Operator\u2014deploy SQL jobs without writing infrastructure code.\\n* **Composable Toolkit**: Use the pieces you need\u2014Docker image, libraries, extensions\u2014to suit your environment.\\n* **Environment Variable Substitution**: Inject secrets and environment-specific config into SQL or plan files using `${ENV_VAR}` syntax.\\n* **UDF Infrastructure**: Load custom JARs and register system functions easily.\\n* **Function Libraries**: Drop-in UDFs for advanced math and OpenAI integration.\\n\\n\\n### Flexible and Extensible\\n\\nFlink SQL Runner is not a monolith. You can:\\n\\n* Run it standalone with Docker.\\n* Deploy it with the Flink Kubernetes Operator.\\n* Extend it via Maven or Gradle in your own Flink stack:\\n\\n```xml\\n<dependency>\\n  <groupId>com.datasqrl.flinkrunner</groupId>\\n  <artifactId>flink-sql-runner</artifactId>\\n  <version>0.6.0</version>\\n</dependency>\\n```\\n\\n\\n## Get Started\\n\\nThe Flink SQL Runner project is open source and [available on Github](https://github.com/datasqrl/flink-sql-runner).\\nCheck out the README for more information on how to use and deploy the Flink SQL Runner.\\n\\nTry it out, report issues, or contribute your own UDFs."},{"id":"flinksql-extensions","metadata":{"permalink":"/blog/flinksql-extensions","source":"@site/blog/2025-05-09-flink-sql-extensions.md","title":"Defining Data Interfaces with FlinkSQL","description":"FlinkSQL is an amazing innovation in data processing: it packages the power of realtime stream processing within the simplicity of SQL.","date":"2025-05-09T00:00:00.000Z","tags":[{"inline":false,"label":"Join","permalink":"/blog/tags/join","description":"Posts about SQL joins and temporal joins"},{"inline":false,"label":"Flink","permalink":"/blog/tags/flink","description":"Apache Flink related posts"},{"inline":false,"label":"DataSQRL","permalink":"/blog/tags/datasqrl","description":"Posts about DataSQRL"}],"readingTime":3.44,"hasTruncateMarker":true,"authors":[{"name":"Matthias Broecheler","title":"CEO of DataSQRL","url":"https://github.com/mbroecheler","page":{"permalink":"/blog/authors/matthias"},"socials":{"linkedin":"https://www.linkedin.com/in/matthiasbroecheler/","github":"https://github.com/mbroecheler","newsletter":"https://www.matthiasb.com"},"imageURL":"/img/headshots/matthias1.png","key":"matthias"}],"frontMatter":{"slug":"flinksql-extensions","title":"Defining Data Interfaces with FlinkSQL","authors":["matthias"],"tags":["Join","Flink","DataSQRL"]},"unlisted":false,"prevItem":{"title":"Flink SQL Runner: Run Flink SQL Without JARs or Glue Code","permalink":"/blog/flinkrunner-announcement"},"nextItem":{"title":"DataSQRL 0.6 Release: The Streaming Data Framework","permalink":"/blog/datasqrl-0.6-release"}},"content":"<head>\\n  <meta property=\\"og:image\\" content=\\"/img/blog/flinksql_extension_api.png\\" />\\n  <meta name=\\"twitter:image\\" content=\\"/img/blog/flinksql_extension_api.png\\" />\\n</head>\\n\\n# Defining Data Interfaces with FlinkSQL\\n\\n[FlinkSQL](https://nightlies.apache.org/flink/flink-docs-release-1.19/docs/dev/table/sql/overview/) is an amazing innovation in data processing: it packages the power of realtime stream processing within the simplicity of SQL.\\nThat means you can start with the SQL you know and introduce stream processing constructs as you need them.\\n\\n<img src=\\"/img/blog/flinksql_extension_api.png\\" alt=\\"FlinkSQL API Extension >\\" width=\\"40%\\"/>\\n\\nFlinkSQL adds the ability to process data incrementally to the classic set-based semantics of SQL. In addition, FlinkSQL supports source and sink connectors making it easy to ingest data from and move data to other systems. That\'s a powerful combination which covers a lot of data processing use cases.\\n\\nIn fact, it only takes a few extensions to FlinkSQL to build entire data applications. Let\'s see how that works.\\n\\n## Building Data APIs with FlinkSQL\\n\\n```sql\\nCREATE TABLE UserTokens (\\nuserid BIGINT NOT NULL,\\ntokens BIGINT NOT NULL,\\nrequest_time TIMESTAMP_LTZ(3) NOT NULL METADATA FROM \'timestamp\'\\n);\\n\\n/*+query_by_all(userid) */\\nTotalUserTokens := SELECT userid, sum(tokens) as total_tokens,\\ncount(tokens) as total_requests\\nFROM UserTokens GROUP BY userid;\\n\\nUserTokensByTime(userid BIGINT NOT NULL, fromTime TIMESTAMP NOT NULL, toTime TIMESTAMP NOT NULL):=\\n                SELECT * FROM UserTokens WHERE userid = :userid,\\n                request_time >= :fromTime AND request_time < :toTime ORDER BY request_time DESC;\\n\\nUsageAlert := SUBSCRIBE SELECT * FROM UserTokens WHERE tokens > 100000;\\n```\\n\\nThis script defines a sequence of tables. We introduce `:=` as syntactic sugar for the verbose `CREATE TEMPORARY VIEW` syntax.\\n\\nThe `UserTokens` table does not have a configured connector, which mean we treat it as an API mutation endpoint connected to Flink via a Kafka topic that captures the events. This makes it easy to build APIs that capture user activity, transactions, or other types of events.\\n\\n\x3c!--truncate--\x3e\\n\\nNext, we sum up the data collected through the API for each user. This is a standard FlinkSQL aggregation query and we expose the result in our API through the `query_by_all` hint which defines the arguments for the query endpoint of that table.\\n\\nWe can also explicitly define query endpoints with arguments through SQL table functions. FlinkSQL supports table functions natively. All we had to do is provide the syntax for defining the function signature.\\n\\nAnd last, the `SUBSCRIBE` keyword in front of the query defines a subscription endpoint for requests exceeding a certain token count which get pushed to clients in real-time.\\n\\nVoila, we just build ourselves a complete GraphQL API with mutation, query, and subscription endpoints.\\nRun the above script with DataSQRL to see the result:\\n\\n```bash\\ndocker run -it --rm -p 8888:8888 -v $PWD:/build datasqrl/cmd run usertokens.sqrl\\n```\\n\\n## Relationships for Complex Data Structures\\n\\nAnd for extra credit, we can define relationships in FlinkSQL to represent the structure of our data explicitly and expose it in the API:\\n\\n```sql\\nUser.totalTokens := SELECT * FROM TotalUserTokens t WHERE this.userid = t.userid LIMIT 1;\\n```\\n\\nThe `User` table in this example is read from an upsert Kafka topic using a standard FlinkSQL `CREATE TABLE` statement.\\n\\n## Code Modularity and Connector Management\\n\\nMany FlinkSQL projects break the codebase into multiple files for better code readability, modularity, or to swap out sources and sinks. That requires extra infrastructure to manage FlinkSQL files and stitch them together.\\n\\nHow about we do that directly in FlinkSQL?\\n\\n```sql\\nIMPORT source-data.User;\\n```\\n\\nHere, we import the `User` table from a separate file within the `source-data` directory, allowing us to separate the data processing logic from the source configurations. It also enables us to use dependency management to swap out sources for local testing vs production.\\n\\nAnd we can do the same for sinks:\\n\\n```sql\\nEXPORT UsageAlert TO mysinks.UsageAlert;\\n```\\n\\nIn addition to breaking out the sink configuration from the main script, the `EXPORT` statement functions as an `INSERT INTO` statement and creates a `STATEMENT SET` implicitly. That makes the code easier to read.\\n\\n## Learn More\\n\\n[FlinkSQL](https://nightlies.apache.org/flink/flink-docs-release-1.19/docs/dev/table/sql/overview/) is phenomenal extension of the SQL ecosystem to stream processing. With DataSQRL, we are trying to make it easier to build end-to-end data pipelines and complete data applications with FlinkSQL.\\n\\nCheck out the [complete example](/docs/intro/getting-started) which also covers testing, customization, and deployment. Or read the [documentation](/docs/sqrl-language) to learn more."},{"id":"datasqrl-0.6-release","metadata":{"permalink":"/blog/datasqrl-0.6-release","source":"@site/blog/2025-05-07-datasqrl-0.6.md","title":"DataSQRL 0.6 Release: The Streaming Data Framework","description":"The DataSQRL community is proud to announce the release of DataSQRL 0.6. This release marks a major milestone in the evolution of our open-source project, bringing enhanced alignment with Flink SQL and powerful new capabilities to the real-time serving layer.","date":"2025-05-07T00:00:00.000Z","tags":[{"inline":false,"label":"Release","permalink":"/blog/tags/release","description":"DataSQRL release announcements"}],"readingTime":2.56,"hasTruncateMarker":true,"authors":[{"name":"Matthias Broecheler","title":"CEO of DataSQRL","url":"https://github.com/mbroecheler","page":{"permalink":"/blog/authors/matthias"},"socials":{"linkedin":"https://www.linkedin.com/in/matthiasbroecheler/","github":"https://github.com/mbroecheler","newsletter":"https://www.matthiasb.com"},"imageURL":"/img/headshots/matthias1.png","key":"matthias"}],"frontMatter":{"slug":"datasqrl-0.6-release","title":"DataSQRL 0.6 Release: The Streaming Data Framework","authors":["matthias"],"tags":["release"]},"unlisted":false,"prevItem":{"title":"Defining Data Interfaces with FlinkSQL","permalink":"/blog/flinksql-extensions"},"nextItem":{"title":"Why Temporal Join is Stream Processing\u2019s Superpower","permalink":"/blog/temporal-join"}},"content":"<head>\\n  <meta property=\\"og:image\\" content=\\"/img/blog/release_0.6.0.png\\" />\\n  <meta name=\\"twitter:image\\" content=\\"/img/blog/release_0.6.0.png\\" />\\n</head>\\n\\n# DataSQRL 0.6 Release: The Streaming Data Framework\\n\\nThe DataSQRL community is proud to announce the release of DataSQRL 0.6. This release marks a major milestone in the evolution of our open-source project, bringing enhanced alignment with Flink SQL and powerful new capabilities to the real-time serving layer.\\n\\n\\n<img src=\\"/img/blog/release_0.6.0.png\\" alt=\\"DataSQRL 0.6.0 Release >\\" width=\\"40%\\"/>\\n\\nYou can find the full release notes and source code on our [GitHub release page](https://github.com/DataSQRL/sqrl/releases/tag/0.6.0). \\nTo get started with the latest compiler, simply pull the latest Docker image:\\n```bash\\ndocker pull datasqrl/cmd:0.6.0\\n```\\n\\n## A New Chapter: Flink SQL Integration\\n\\nWith DataSQRL 0.6, we are embracing the Flink ecosystem more deeply than ever before. This release introduces a complete re-architecture of the DataSQRL compiler to build directly on top of Flink SQL\'s parser and planner. By aligning our internal model with Flink SQL semantics, we unlock a host of new capabilities and bring DataSQRL users closer to the vibrant Flink ecosystem.\\n\\nThis architectural shift allows DataSQRL to:\\n\\n* **Use Flink SQL syntax as the foundation**, enabling more intuitive query definitions and easier onboarding for users familiar with Flink.\\n* **Extend Flink SQL with domain-specific features**, such as declarative relationship definitions and functions to define the data interface.\\n* **Transpile FlinkSQL to database dialects** for query execution.\\n\\n\x3c!--truncate--\x3e\\n\\n## Serving-Layer Power: Functions & Relationships\\n\\nDataSQRL 0.6 introduces first-class support for defining **functions** and **relationships** in your SQRL scripts. These constructs make it easier to model complex application logic in a modular, declarative fashion.\\n\\nThese features are purpose-built for powering LLM-ready APIs, event-driven architectures, and real-time user-facing applications.\\n\\nCheck out the [language documentation](/docs/sqrl-language) for details.\\n\\n## Developer Tooling\\n\\nDataSQRL 0.6 provides a docker image for compiling, running, and testing SQRL projects. You can now quickly iterate and check the results. Or run automated tests in CI/CD.\\n\\n## Deployment Artifacts\\n\\nDataSQRL 0.6 removes deployment profiles and instead generates all deployment artifacts in the `build/deploy/plan` folder. This makes it easier to integrate with Kubernetes deployment processes (e.g. via Helm) or cloud managed service deployments (e.g. via Terraform).\\n\\n## Breaking Changes & Migration Path\\n\\nAs this is a major release, **DataSQRL 0.6 is not backwards compatible** with version 0.5. The syntax and internal representation have been updated to align with Flink SQL and to support the new compiler architecture.\\n\\nTo help you transition, we\u2019ve provided updated examples and migration guidance in the [DataSQRL examples repository](https://github.com/DataSQRL/datasqrl-examples). We recommend starting with one of the updated use cases to get a feel for the new workflow.\\n\\n## Thanks to the Community\\n\\nThis release wouldn\u2019t have been possible without the contributions, bug reports, and thoughtful feedback from our growing community. Whether you opened a pull request, filed an issue, or joined a discussion, thank you. Your support drives this project forward.\\n\\nWe\u2019re excited to see what you build with DataSQRL 0.6. If you haven\u2019t joined the [community](/community) yet, now\u2019s a great time to get involved: star us on [GitHub](https://github.com/DataSQRL/sqrl), try out the latest release, and share your thoughts.\\n\\nStay tuned for more updates, and happy building."},{"id":"temporal-join","metadata":{"permalink":"/blog/temporal-join","source":"@site/blog/2023-07-10-temporal-join.mdx","title":"Why Temporal Join is Stream Processing\u2019s Superpower","description":"Stream processing technologies like Apache Flink introduce a new type of data transformation that\u2019s very powerful: the temporal join. Temporal joins add context to data streams while being efficient and fast to execute.","date":"2023-07-10T00:00:00.000Z","tags":[{"inline":false,"label":"Join","permalink":"/blog/tags/join","description":"Posts about SQL joins and temporal joins"},{"inline":false,"label":"Flink","permalink":"/blog/tags/flink","description":"Apache Flink related posts"},{"inline":false,"label":"DataSQRL","permalink":"/blog/tags/datasqrl","description":"Posts about DataSQRL"}],"readingTime":7.42,"hasTruncateMarker":true,"authors":[{"name":"Matthias Broecheler","title":"CEO of DataSQRL","url":"https://github.com/mbroecheler","page":{"permalink":"/blog/authors/matthias"},"socials":{"linkedin":"https://www.linkedin.com/in/matthiasbroecheler/","github":"https://github.com/mbroecheler","newsletter":"https://www.matthiasb.com"},"imageURL":"/img/headshots/matthias1.png","key":"matthias"}],"frontMatter":{"slug":"temporal-join","title":"Why Temporal Join is Stream Processing\u2019s Superpower","authors":["matthias"],"tags":["Join","Flink","DataSQRL"]},"unlisted":false,"prevItem":{"title":"DataSQRL 0.6 Release: The Streaming Data Framework","permalink":"/blog/datasqrl-0.6-release"},"nextItem":{"title":"Let\'s Uplevel Our Database Game: Meet DataSQRL","permalink":"/blog/lets-uplevel-database-datasqrl"}},"content":"import useBaseUrl from \'@docusaurus/useBaseUrl\';\\n\\n\\n\\n<head>\\n  <meta property=\\"og:image\\" content={useBaseUrl(\'img/blog/temporal_join.png\')} />\\n  <meta name=\\"twitter:image\\" content={useBaseUrl(\'img/blog/temporal_join.png\')} />\\n</head>\\n\\nStream processing technologies like Apache Flink introduce a new type of data transformation that\u2019s very powerful: the temporal join. Temporal joins add context to data streams while being efficient and fast to execute.\\n\\n<img src={useBaseUrl(\'/img/blog/temporal_join.svg\')} alt=\\"Temporal Join >\\" width=\\"30%\\"/>\\n\\n\\nThis article introduces the temporal join, compares it to the traditional inner join, explains when to use it, and why it is a secret superpower.\\n\\nTable of Contents:\\n* [The Join: A Quick Review](#review)\\n* [The Temporal Join: Linking Stream and State](#tempjoin)\\n* [Temporal Join vs Inner Join](#tempinner)\\n* [Why Temporal Joins are Fast and Efficient](#efficient)\\n* [Temporal Joins Made Easy to Use](#easy)\\n* [Summary](#summary)\\n\\n\x3c!--truncate--\x3e\\n\\n## The Join: A Quick Review {#review}\\n\\nLet\'s take a quick detour down memory lane and revisit the good ol\' join operation. That trusty sidekick in your SQL utility belt helps you link data from two or more tables based on a related column between them.\\n\\nSuppose we are operating a factory with a number of machines that roast and package coffee. We place sensors on each machine to monitor the temperature and detect overheating.\\n\\nWe keep track of the sensors and machines in two database tables.\\n\\nThe `Sensor` table contains the serial number and machine id that the sensor is placed on.\\n\\n| id | serialNo | machineid |\\n|----|----------|-----------|\\n| 1  | X57-774  | 501       |\\n| 2  | X33-453  | 203       |\\n| 3  | X54-554  | 501       |\\n\\nThe `Machine` table contains the name of each machine.\\n\\n| id    | name           |\\n|-------|----------------|\\n| 203   | Iron Roaster   |\\n| 501   | Gritty Grinder |\\n\\nTo identify all the sensors on the machine \u201cIron Roaster\u201d we use the following SQL query which joins the `Sensor` and `Machine` tables:\\n```sql\\nSELECT s.id, s.serialNo FROM Sensor s \\n    JOIN Machine m ON s.machineid = m.id \\n    WHERE m.name = \u201cIron Roaster\u201d\\n```\\n\\nWhy are joins important? Without it, your data tables are like islands, isolated and lonely. Joins bring them together, creating meaningful relationships between data, and enriching data records with context to see the bigger picture.\\n\\nBy default, databases execute joins as **inner** joins which means only matching records are included in the join.\\n\\nSo, now that we\'ve refreshed our memory about the classic join, let\'s dive into the exciting world of temporal joins in stream processing systems like Apache Flink.\\n\\n## The Temporal Join: Linking Stream and State {#tempjoin}\\n\\n<img src={useBaseUrl(\'/img/blog/delorean.jpeg\')} alt=\\"Temporal Join DeLorean >\\" width=\\"40%\\"/>\\n\\nPicture this: you\'re a time traveler. You have the power to access any point in time, past or future, at your will. Now, imagine that your data could do the same. Enter the Temporal Join, the DeLorean of data operations, capable of taking your data on a time-traveling adventure.\\n\\nA Temporal Join is like a regular join but with a twist. It allows you to join a stream of data (the time traveler) with a versioned table (the timeline) based on the time attribute of the data stream. This means that for each record in the stream, the join will find the most recent record in the versioned table that is less than or equal to the stream record\'s time.\\n\\nThe versioned table is a normal state table where we keep track of data changes over time. That is, we keep older versions of each record around to allow the stream to match the correct version in time. Like time travel, temporal joins can make your head spin a bit. Let\u2019s look at an example to break it down.\\n\\n## Temporal Join vs Inner Join {#tempinner}\\n\\nBack to our coffee roasting factory, we collect the temperature readings from each sensor in a data stream.\\n\\n| timestamp           | sensorid | temperature |\\n|---------------------|----------|-------------|\\n| 2023-07-10T07:11:08 | 1        | 105.2       |\\n| 2023-07-10T07:11:08 | 2        | 83.1        |\\n| ...                 |          |             |\\n| 2023-07-10T13:25:16 | 1        | 77.8        |\\n| 2023-07-10T13:25:16 | 2        | 83.5        |\\n\\nAnd we want to know the maximum temperature recorded for each machine.\\n\\nEasy enough, let\u2019s join the temperature data stream with the Sensors table and aggregate by machine id:\\n\\n```sql\\nSELECT s.machineid, MAX(r.temperature) AS maxTemp \\nFROM SensorReading r INNER JOIN Sensor s \\n    ON r.sensorid = s.id GROUP BY s.machineid\\n```\\n\\nBut here is a problem: What if we moved a sensor from one machine to another during the day? With an inner join, all of the sensor\u2019s readings would be linked to the machine it was last placed on. So, if sensor 1 records a high temperature of 105 degrees in the morning and we move the sensor to the \u201cIron Roaster\u201d machine in the afternoon, then we might see the 105 degrees falsely show up as the maximum temperature for the Iron Roaster. See how time played a trick on our join?\\n\\nAnd this happens whenever we join a data stream with a state table that changes over time, like our sensors that get moved around the factory. What to do? Let\u2019s call the temporal join to our rescue:\\n\\n```sql\\nSELECT s.machineid, MAX(r.temperature) AS maxTemp \\nFROM SensorReading r JOIN Sensor FOR SYSTEM_TIME AS OF r.`timestamp` s\\n    ON r.sensorid = s.id GROUP BY s.machineid\\n```\\n\\nPretty much the same query, just a different join type. Just a heads-up: the syntax for temporal joins in Flink SQL is more complex - we\'ll get to that [later](#easy).\\n\\nAs a temporal join, we are joining each sensor reading with the version of the sensor record at the time of the data stream. In other words, the join not only matches the sensor reading with the sensor record based on the id but also based on the timestamp of the reading to ensure it matches the right version of the sensor record. Pretty neat, right?\\n\\nWhenever you join a data stream with a state that changes over time, you want to use the temporal join to make sure your data is lined up correctly in time. Temporal joins are a powerful feature of stream processing engines that would be difficult to implement in a database.\\n\\n\\n## Why Temporal Joins are Fast and Efficient {#efficient}\\n\\n<img src={useBaseUrl(\'/img/blog/flink_logo.svg\')} alt=\\"Apache Flink >\\" width=\\"30%\\"/>\\n\\nNot only do temporal joins solve the time-alignment problem when joining data streams with changing state, modern stream processors like Apache Flink are also incredibly efficient at executing temporal joins. A powerful feature with great performance? Sounds too good to be true. Let\u2019s peek behind the stream processing curtain to find out why.\\n\\nIn stream processing, joins are maintained as the underlying data changes over time. That requires the stream engine to hold all the data it needs to update join records when either side of the join changes. This makes inner joins pretty expensive on data streams.\\n\\nConsider our max-temperature query with the inner join: When we join a temperature reading with the corresponding sensor record, and that record changes, the engine has to update the result join record. To do so, it has to store all the sensor readings to determine which join results are affected by a change in a sensor record. This can lead to a lot of updates and hence a lot of downstream computation. It can also cause system failure when there are a lot of temperature readings in our data stream because the stream engine has to store all of them.\\n\\nTemporal joins, on the other hand, can be executed much more efficiently. The stream engine only needs to store the versions of the sensor table that are within the time bounds of the sensor reading data stream. And it only has to briefly store (if at all) the sensor reading records to ensure they are joined with the most up-to-date sensor records. Moreover, temporal joins don\u2019t require sending out a massive amount of updated join records when sensors change placement since the join is fixed in time.\\n\\n## Time to Wrap Up This Temporal Journey {#summary}\\n\\nWe\'ve reached the end of our time-traveling adventure through the universe of temporal joins. We\'ve seen how they\'re like the DeLorean of data operations, zipping us back and forth through time to make sure our data matches up just right. We\'ve also compared them to the good ol\' inner join.\\n\\nTemporal joins help us avoid the pitfalls of time-alignment problems when joining data streams with changing state. They\'re also super efficient, making them a great choice for high-volume, real-time data processing.\\n\\nAnd that\u2019s why the temporal join is stream processing\'s secret superpower.\\n\\nDataSQRL makes using temporal joins a breeze. With its simplified syntax and smart defaults, it\'s like having a personal tour guide leading you through the sometimes bewildering landscape of stream processing. Take a look at our [Getting Started](/docs/intro/getting-started) to see a complete example of temporal joins in action or take a look at our [other tutorials](/docs/intro/tutorials) for a step-by-step guide to stream processing including temporal joins.\\n\\nHappy data time-traveling, folks!"},{"id":"lets-uplevel-database-datasqrl","metadata":{"permalink":"/blog/lets-uplevel-database-datasqrl","source":"@site/blog/2023-05-15-lets-uplevel-database-datasqrl.mdx","title":"Let\'s Uplevel Our Database Game: Meet DataSQRL","description":"We need to make it easier to build data-driven applications. Databases are great if all your application needs is storing and retrieving data. But if you want to build anything more interesting with data - like serving users recommendations based on the pages they are visiting, detecting fraudulent transactions on your site, or computing real-time features for your machine learning model - you end up building a ton of custom code and infrastructure around the database.","date":"2023-05-15T00:00:00.000Z","tags":[{"inline":false,"label":"DataSQRL","permalink":"/blog/tags/datasqrl","description":"Posts about DataSQRL"},{"inline":false,"label":"Community","permalink":"/blog/tags/community","description":"Community updates and announcements"}],"readingTime":4.7,"hasTruncateMarker":true,"authors":[{"name":"Matthias Broecheler","title":"CEO of DataSQRL","url":"https://github.com/mbroecheler","page":{"permalink":"/blog/authors/matthias"},"socials":{"linkedin":"https://www.linkedin.com/in/matthiasbroecheler/","github":"https://github.com/mbroecheler","newsletter":"https://www.matthiasb.com"},"imageURL":"/img/headshots/matthias1.png","key":"matthias"}],"frontMatter":{"slug":"lets-uplevel-database-datasqrl","title":"Let\'s Uplevel Our Database Game: Meet DataSQRL","authors":["matthias"],"tags":["DataSQRL","community"]},"unlisted":false,"prevItem":{"title":"Why Temporal Join is Stream Processing\u2019s Superpower","permalink":"/blog/temporal-join"}},"content":"import useBaseUrl from \'@docusaurus/useBaseUrl\';\\n\\n\\n**We need to make it easier to build data-driven applications.** Databases are great if all your application needs is storing and retrieving data. But if you want to build anything more interesting with data - like serving users recommendations based on the pages they are visiting, detecting fraudulent transactions on your site, or computing real-time features for your machine learning model - you end up building a ton of custom code and infrastructure around the database.\\n\\nYou need a queue like Kafka to hold your events, a stream processor like Flink to process data, a database like Postgres to store and query the result data, and an API layer to tie it all together.\\n\\n<img src={useBaseUrl(\'/img/reference/full_logo.svg\')} alt=\\"DataSQRL Logo >\\" width=\\"30%\\" />\\n\\nAnd that\u2019s just the price of admission. To get a functioning data layer, you need to make sure that all these components talk to each other and that data flows smoothly between them. Schema synchronization, data model tuning, index selection, query batching \u2026 all that fun stuff.\\n\\nThe point is, you need to do a ton of data plumbing if you want to build a data-driven application. All that data plumbing code is time-consuming to develop, hard to maintain, and expensive to operate.\\n\\nWe need to make building with data easier. That\u2019s why we are sending out this call to action to uplevel our database game. **Join us in figuring out how to simplify the data layer.**\\n\\nWe have an idea to get us started: Meet DataSQRL.\\n\\n\x3c!--truncate--\x3e\\n\\n## Introducing DataSQRL\\n\\nDataSQRL is a build tool that compiles your application\u2019s data layer from a high-level data development language, dubbed SQRL.\\n\\nOur goal is to create a new abstraction layer above the low-level languages often used in data layers, allowing a compiler to handle the tedious tasks of data plumbing, infrastructure assembly, and configuration management.\\n\\nMuch like how you use high-level languages such as Javascript, Python, or Java instead of Assembly for software development, we believe a similar approach should be used for data. \\n\\nSQRL is designed to be a developer-friendly version of SQL, maintaining familiar syntax while adding features necessary for building data-driven applications, like support for nested data and data streams.\\n\\nCheck out this simple SQRL script to build a recommendation engine from clickstream data.\\n\\n```sql\\nIMPORT clickstream.Clickstream; --Import clickstream data from Kafka\\nIMPORT content.Content;         --Import content from CDC stream\\n\\n/* Find next page visits within 10 minutes */\\n_CoVisits := SELECT b.url AS beforeURL, a.url AS afterURL,\\n                    a.event_time AS `timestamp`\\n             FROM Clickstream b INNER JOIN Clickstream a ON b.userid=a.userid\\n                 AND b.event_time < a.event_time AND\\n                     b.event_time >= a.event_time - INTERVAL 10 MINUTE;\\n/* Recommend pages that are visited shortly after */\\n/*+query_by_all(url) */\\nRecommendation := SELECT beforeURL AS url, afterURL AS recommendation,\\n                         count(1) AS frequency FROM _CoVisits\\n                  GROUP BY beforeURL, afterURL\\n                  ORDER BY url ASC, frequency DESC;\\n```\\n\\nThis little SQRL script imports clickstream data, identifies pairs of URLs visited within a 10-minute interval, and compiles these pairs into a set of recommendations, ordered by the frequency of co-visits.\\n\\n<img src={useBaseUrl(\'/img/diagrams/getting_started_diagram2.png\')} alt=\\"Data pipeline >\\" />\\n\\nDataSQRL then takes this script and compiles it into an integrated data pipeline, complete with all necessary data plumbing pre-installed. It configures access to the clickstream. It generates an executable for the stream processor that ingests, validates, joins, and aggregates the clickstream data. It creates the data model and writes the aggregated data to the database. It synchronizes timestamps and schemas between all the components. And it compiles a server executable that queries the database and exposes the computed recommendations through a GraphQL API.\\n\\n**The bottom line: These 9 lines of SQRL code can replace hundreds of lines of complex data plumbing code and save hours of infrastructure setup.**\\n\\nWe believe that all this low-level data plumbing work should be done by a compiler since it is tedious, time-consuming, and error-prone. Let\u2019s uplevel our data game, so we can focus on **what** we are trying to build with data and less on the **how**.\\n\\n## Join Us on this Journey\\n\\n<img src={useBaseUrl(\'/img/undraw/code.svg\')} alt=\\"Join DataSQRL Community >\\" width=\\"50%\\"/>\\n\\n\\nWe have the ambitious goal of designing a higher level of abstraction for data to enable millions of developers to build data-driven applications.\\n\\nWe [just released](https://github.com/DataSQRL/sqrl/releases/tag/v0.1.0) the first version of DataSQRL, and we recognize that we are at the beginning of a long, long road. We need your help. If you are a data nerd, like building with data, or wish it was easier, please [join us on this journey](https://github.com/DataSQRL/sqrl). DataSQRL is an open-source project, and all development activity is transparent.\\n\\nHere are some ideas for how you can contribute:\\n\\n* Share your thoughts: Do you have ideas on how we can improve the SQRL language or the DataSQRL compiler? Jump into [our community](/community) and let us know!\\n* Test the waters: Do you like playing with new technologies? Try out [DataSQRL](/docs/intro/getting-started) and let us know if you find any bugs or missing features.\\n* Spread the word: Think DataSQRL has potential? Share this blog post and [star](https://github.com/DataSQRL/sqrl) DataSQRL on [Github](https://github.com/DataSQRL/sqrl). Your support can help us reach more like-minded individuals.\\n* Code with us: Do you enjoy contributing to open-source projects? Dive into [the code](https://github.com/DataSQRL/sqrl) with us and pick up a [ticket](https://github.com/DataSQRL/sqrl/issues).\\n\\nLet\u2019s uplevel our database game. With your help, we can make building with data fun and productive.\\n\\n## More Information\\n\\nYou probably have a ton of questions now. How do I import my own data? How do I customize the API? How do I deploy SQRL scripts to production? How do I import functions from my favorite programming language?\\n\\nThose are all great questions. Check out [the documentation](/docs/intro) for answers."}]}}')}}]);