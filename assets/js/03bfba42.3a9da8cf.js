"use strict";(self.webpackChunkdocumentation=self.webpackChunkdocumentation||[]).push([[2853],{8453:(e,n,s)=>{s.d(n,{R:()=>r,x:()=>l});var i=s(6540);const t={},a=i.createContext(t);function r(e){const n=i.useContext(a);return i.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:r(e.components),i.createElement(a.Provider,{value:n},e.children)}},8922:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>o,contentTitle:()=>l,default:()=>d,frontMatter:()=>r,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"deepdive","title":"Deep Dive: How DataSQRL Works","description":"The DataSQRL Compiler executes the following steps:","source":"@site/docs/deepdive.md","sourceDirName":".","slug":"/deepdive","permalink":"/docs/deepdive","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"\ud83e\udde9 How To Guides","permalink":"/docs/howto"},"next":{"title":"\ud83d\udd04 Compatibility","permalink":"/docs/compatibility"}}');var t=s(4848),a=s(8453);const r={},l="Deep Dive: How DataSQRL Works",o={},c=[{value:"Architecture",id:"architecture",level:2},{value:"Planner Components",id:"planner-components",level:2},{value:"Packager",id:"packager",level:3},{value:"Parser",id:"parser",level:3},{value:"Logical Plan Analyzer",id:"logical-plan-analyzer",level:3},{value:"DAG Planner",id:"dag-planner",level:3},{value:"Physical Planner",id:"physical-planner",level:3}];function h(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"deep-dive-how-datasqrl-works",children:"Deep Dive: How DataSQRL Works"})}),"\n",(0,t.jsx)(n.p,{children:"The DataSQRL Compiler executes the following steps:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Read Configuration"}),": Read and combine all package.json configuration files to initialize the configuration for the compiler"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Build Project"}),": The ",(0,t.jsx)(n.a,{href:"#packager",children:"packager"})," builds the project structure in ",(0,t.jsx)(n.code,{children:"build/"})," directory."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Parse Scripts"}),": The ",(0,t.jsx)(n.a,{href:"#parser",children:"parser"})," reads the main SQRL script in the ",(0,t.jsx)(n.code,{children:"build/"})," directory and resolves all ",(0,t.jsx)(n.code,{children:"IMPORT"})," and ",(0,t.jsx)(n.code,{children:"EXPORT"})," statements locally against the folder structure in the ",(0,t.jsx)(n.code,{children:"build"})," directory."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Create Logical Plans"}),": The ",(0,t.jsx)(n.a,{href:"#parser",children:"parser"})," converts all statements to logical plans. Most of the work is delegated to the FlinkSQL parser and Apache Calcite. The parser detects the language elements SQRL adds and converts them."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Analyze Logical Plans"}),": The ",(0,t.jsx)(n.a,{href:"#logical-plan-analyzer",children:"logical plan analyzer"})," validates each statement and extract information needed for planning."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Build DAG"}),": The ",(0,t.jsx)(n.a,{href:"#dag-planner",children:"DAG Planner"})," combines all logical statement plans into a processing DAG that defines the flow of data from sources to sinks."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Optimize DAG"}),": The ",(0,t.jsx)(n.a,{href:"#dag-planner",children:"DAG Planner"})," optimizes the DAG and assigns each statement to an ",(0,t.jsx)(n.a,{href:"#architecture",children:"engine"})," for execution."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Generate Physical Plans"}),": The ",(0,t.jsx)(n.a,{href:"#physical-planner",children:"Physical Planner"})," generates deployment assets for each engine and connector configuration to move data between engines."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Write Deployment Artifacts"}),": The deployment artifacts are written to the ",(0,t.jsx)(n.code,{children:"build/deploy"})," folder with the engine plans in ",(0,t.jsx)(n.code,{children:"build/deploy/plan"}),"."]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"The DataSQRL run command executes all compilation steps above and:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Launch"}),": Launches all engines in docker"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Deploy"}),": Deploys the deployment assets to the engines, e.g. installs the database schema, passes the GraphQL execution plan to Vert.x, creates topics in RedPanda, and executes the compiled plan in Flink."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Runs"}),": Runs and monitors the engines as they execute the pipeline."]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"The running data pipeline and the individual engines running each component are accessible locally via the mapped ports."}),"\n",(0,t.jsx)(n.p,{children:"The DataSQRL test command executes all compilation and run steps above and:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Subscriptions"}),": Installs subscription queries to listen for test results (if any)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Mutations"}),": Runs the mutation queries against the API in order (if any) and snapshots the results.","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Waits for the configured interval, number of checkpoints, or Flink job completion based on configuration."}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Queries"}),": Runs the queries against the API to snapshot the results."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Snapshots"}),": Snapshots all subscription results in string order."]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"architecture",children:"Architecture"}),"\n",(0,t.jsx)(n.p,{children:"DataSQRL supports a pluggable engine architecture. A data pipeline or microservice\nconsists of multiple stages and each stage is executed by an engine.\nFor example, a data pipeline may consist of a stream processing, storage, and\nserving stage which are executed by Apache Flink, PostgreSQL, and Vert.x, respectively."}),"\n",(0,t.jsx)(n.p,{children:"DataSQRL supports the following types of stages:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Stream Processing: For processing data as it is ingested","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://flink.apache.org/",children:"Apache Flink"})}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["Log: For moving data between stages reliably","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://kafka.apache.org/",children:"Apache Kafka"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://redpanda.com/",children:"RedPanda"})}),"\n",(0,t.jsxs)(n.li,{children:["Apache Kafka-compatible (e.g. ",(0,t.jsx)(n.a,{href:"https://azure.microsoft.com/en-us/services/event-hubs/",children:"Azure EventHub"}),")"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["Database: For storage and querying data","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://www.postgresql.org/",children:"PostgreSQL"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://iceberg.apache.org/",children:"Apache Iceberg"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://www.yugabyte.com/",children:"Yugabyte"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://duckdb.org/",children:"DuckDB"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://www.snowflake.com/",children:"Snowflake"})}),"\n",(0,t.jsxs)(n.li,{children:["PostgreSQL wire compatible (e.g. ",(0,t.jsx)(n.a,{href:"https://www.cockroachlabs.com/",children:"CockroachDB"}),")"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["Server: For returning data through an API upon request","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://vertx.io/",children:"Vert.x"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://www.graphql-java.com/",children:"GraphQL Java"})}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.li,{children:"Cache: For caching data on the server (coming soon)"}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"Currently, DataSQRL is closely tied to Flink as the stream processing engine. The other engines are modular, making it simple to add additional engines."}),"\n",(0,t.jsx)(n.p,{children:'A data pipeline topology is a sequence of stages. A pipeline topology may contain\nmultiple stages of the same type (e.g. two different database stages).\nAn engine is what executes the deployment assets for a given stage.\nFor example, the FlinkSQL generated by the compiler as part of the deployment assets for the "stream" stage is\nexecuted by the Flink engine.'}),"\n",(0,t.jsxs)(n.p,{children:["The pipeline topology as well as other compiler configuration options are\nspecified in a json configuration file typically called ",(0,t.jsx)(n.code,{children:"package.json"}),".\nThe ",(0,t.jsx)(n.a,{href:"/docs/configuration",children:"configuration documentation"})," lists all the configuration options."]}),"\n",(0,t.jsx)(n.h2,{id:"planner-components",children:"Planner Components"}),"\n",(0,t.jsx)(n.p,{children:"The planner parses a SQRL script, i.e. a sequence of SQL statements, analyzes\nthe statements, constructs a data processing DAG, optimizes the DAG, and finally\nproduces deployment assets for the engines executing the data processing steps."}),"\n",(0,t.jsx)(n.p,{children:"The planner consists of the following components."}),"\n",(0,t.jsx)(n.h3,{id:"packager",children:"Packager"}),"\n",(0,t.jsxs)(n.p,{children:["The Packager populates the ",(0,t.jsx)(n.code,{children:"build/"})," directory with scripts and folders based on the local folder structure and the dependency mapping defined in the configuration file."]}),"\n",(0,t.jsx)(n.p,{children:"As part of this process, the packager executes all registered Preprocessors which prepare local data or copy elements into the build directory."}),"\n",(0,t.jsx)(n.p,{children:"In addition, the packager executes the following special purpose preprocessors:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["The Discovery preprocessor analyzes any ",(0,t.jsx)(n.code,{children:".csv"})," or ",(0,t.jsx)(n.code,{children:".jsonl"})," file in the local directory tree, determines the schema, and creates a table configuration."]}),"\n",(0,t.jsx)(n.li,{children:"The UDF preprocessor extracts UDF definitions from provided jar files."}),"\n",(0,t.jsxs)(n.li,{children:["The static data preprocessors copies ",(0,t.jsx)(n.code,{children:".csv"})," and ",(0,t.jsx)(n.code,{children:".jsonl"})," files into a consolidated data directory for Flink to read at runtime. This requires that filenames for static data files are unique."]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"Preprocessors are internal to DataSQRL and can be extended within the framework."}),"\n",(0,t.jsx)(n.h3,{id:"parser",children:"Parser"}),"\n",(0,t.jsx)(n.p,{children:"The parser is the first stage of the compiler. The parser parses the\nSQRL script into a logical plan by pre-processing any SQRL specific syntax and then\npassing the result to the FlinkSQL parser to produces a logical plan."}),"\n",(0,t.jsx)(n.p,{children:"The parser resolves imports against the build directory using module loaders\nthat retrieve dependencies. It maintains a schema of all defined tables\nin a SQRL script."}),"\n",(0,t.jsx)(n.p,{children:"The transpiler is build on top of Apache Calcite by way of FlinkSQL for all SQL handling.\nIt prepares the statements that are analyzed and planned by the planner."}),"\n",(0,t.jsx)(n.h3,{id:"logical-plan-analyzer",children:"Logical Plan Analyzer"}),"\n",(0,t.jsx)(n.p,{children:"The logical plan analyzer is the second stage of the compiler. It takes the\nlogical plan produced by the transpiler for each table or function\ndefined in the SQRL script and analyzes the logical plan to extract a TableAnalysis\nthat contains information needed by the planner."}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"It keeps track of important metadata like timestamps, primary keys, sort orders, etc"}),"\n",(0,t.jsx)(n.li,{children:"It analyzes the SQL to identify potential issues, semantic inconsistencies, or optimization potential and produces warnings or notices."}),"\n",(0,t.jsx)(n.li,{children:"It extracts cost information for the optimizer."}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"dag-planner",children:"DAG Planner"}),"\n",(0,t.jsx)(n.p,{children:"The DAG planner takes all the individual table and function definitions and assembles them into\na data processing DAG (directed acyclic graph). It prunes the DAG and rewrites the DAG before optimizing the\nDAG to assign each node (i.e. table or function) to a stage in the pipeline."}),"\n",(0,t.jsx)(n.p,{children:"The optimizer uses a cost model and is constrained to produce only viable\npipelines."}),"\n",(0,t.jsx)(n.p,{children:"At the end of the DAG planning process, each table or function defined in the SQRL script\nis assigned to a stage in the pipeline."}),"\n",(0,t.jsx)(n.h3,{id:"physical-planner",children:"Physical Planner"}),"\n",(0,t.jsx)(n.p,{children:"All the tables in a given stage are then passed to the stage engine's physical\nplanner which produces the physical plan for the engine that has been\nconfigured to execute that stage."}),"\n",(0,t.jsx)(n.p,{children:"The physical plan assets produced depend on the engine:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Apache Flink: A FlinkSQL script and compiled plan which contains the generated connector configuration"}),"\n",(0,t.jsx)(n.li,{children:"Postgres: A SQL schema for the tables and index structure as well as a set of SQL view and query definitions."}),"\n",(0,t.jsx)(n.li,{children:"Kafka: A list of topics with configuration"}),"\n",(0,t.jsx)(n.li,{children:"GraphQL: A GraphQL schema and query execution plan"}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"Physical planning can contain additional optimization such as selecting optimal index\nstructures for database tables."}),"\n",(0,t.jsx)(n.p,{children:'An important step in generating the physical plan is generating the connector configuration between engines. When two adjacent nodes in the DAG are assigned to different engines for execution, we consider this a "cut" in the DAG since it cuts the DAG into multiple sub-graphs -- one for each engine. To move data between engines at the cut points (i.e. the edges that connect the respective nodes), a connection needs to be established.'}),"\n",(0,t.jsxs)(n.p,{children:["Connector configuration is generated for the stream processing engine (Apache Flink) and the server engine (Vert.x) to connect to the database, filesystem, streaming platform, etc. The connector configuration is determined by the physical planner based on the logical plan analysis above and instantiated in connector templates that are configured in the ",(0,t.jsx)(n.a,{href:"/docs/configuration",children:"configuration file"}),"."]}),"\n",(0,t.jsx)(n.p,{children:"The physical planner is also responsible for generating the API schemas (e.g. GraphQL schema)\nfor the exposed API if the pipeline contains a server engine. Optionally, the user may\nprovide the API schema in which case the physical planner validates the schema and maps it\nto the SQRL script."}),"\n",(0,t.jsxs)(n.p,{children:["The physical plans are then written out as deployment artifacts to the ",(0,t.jsx)(n.code,{children:"build/deploy"}),"\ndirectory."]})]})}function d(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(h,{...e})}):h(e)}}}]);