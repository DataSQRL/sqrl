"use strict";(self.webpackChunkdocumentation=self.webpackChunkdocumentation||[]).push([[2578],{7019:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>a,contentTitle:()=>o,default:()=>h,frontMatter:()=>l,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"stdlib-docs/README","title":"README","description":"Deploy jars","source":"@site/docs/stdlib-docs/README.md","sourceDirName":"stdlib-docs","slug":"/stdlib-docs/","permalink":"/docs/stdlib-docs/","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{}}');var r=i(4848),t=i(8453);const l={},o="Flink SQL Runner",a={},c=[{value:"Features",id:"features",level:2},{value:"Flink SQL Runner Usage",id:"flink-sql-runner-usage",level:2},{value:"Running Locally",id:"running-locally",level:3},{value:"Running in Kubernetes with Flink Operator",id:"running-in-kubernetes-with-flink-operator",level:3},{value:"Environment Variable Substitution",id:"environment-variable-substitution",level:3},{value:"Building Your Own Flink SQL Runner",id:"building-your-own-flink-sql-runner",level:3},{value:"Flink Extensions",id:"flink-extensions",level:2},{value:"Dead-Letter-Queue Support for Kafka Sources",id:"dead-letter-queue-support-for-kafka-sources",level:3},{value:"JSONB Type",id:"jsonb-type",level:3},{value:"Vector Type",id:"vector-type",level:3},{value:"Function Libraries",id:"function-libraries",level:3},{value:"Usage",id:"usage",level:2},{value:"Within DataSQRL",id:"within-datasqrl",level:3},{value:"Flink SQL Runner",id:"flink-sql-runner-1",level:3},{value:"Custom Flink Implementation",id:"custom-flink-implementation",level:3},{value:"CSV Format",id:"csv-format",level:3},{value:"Community Contributions",id:"community-contributions",level:2},{value:"Releasing",id:"releasing",level:3},{value:"License",id:"license",level:3},{value:"Contact &amp; Support",id:"contact--support",level:3}];function d(e){const n={a:"a",blockquote:"blockquote",br:"br",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,t.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.a,{href:"https://github.com/DataSQRL/flink-sql-runner/actions/workflows/deploy.yml",children:(0,r.jsx)(n.img,{src:"https://github.com/DataSQRL/flink-sql-runner/actions/workflows/deploy.yml/badge.svg",alt:"Deploy jars"})}),"\n",(0,r.jsx)(n.a,{href:"https://github.com/DataSQRL/flink-sql-runner/releases",children:(0,r.jsx)(n.img,{src:"https://img.shields.io/github/v/release/DataSQRL/flink-sql-runner?sort=semver",alt:"GitHub release"})}),"\n",(0,r.jsx)(n.a,{href:"https://hub.docker.com/r/datasqrl/flink-sql-runner/tags",children:(0,r.jsx)(n.img,{src:"https://img.shields.io/docker/v/datasqrl/flink-sql-runner?sort=semver",alt:"Docker Image Version"})}),"\n",(0,r.jsx)(n.a,{href:"https://repo1.maven.org/maven2/com/datasqrl/flinkrunner/flink-sql-runner/",children:(0,r.jsx)(n.img,{src:"https://img.shields.io/maven-central/v/com.datasqrl.flinkrunner/flink-sql-runner",alt:"Maven Central"})})]}),"\n",(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"flink-sql-runner",children:"Flink SQL Runner"})}),"\n",(0,r.jsx)("img",{src:"stdlib-docs/img/runner_logo.png",alt:"Flink SQL Runner Logo",width:"300",align:"right"}),"\n",(0,r.jsx)(n.p,{children:"Tools and extensions for running Apache Flink SQL applications, including Docker images, data types, connectors, function libraries, and formats."}),"\n",(0,r.jsx)(n.p,{children:"This repository contains core components for running Flink SQL applications in production using the Flink Kubernetes Operator, without manual JAR assembly or custom infrastructure."}),"\n",(0,r.jsx)(n.p,{children:"The individual components are modular and the project is composable to make it easy to create your own custom Flink SQL runner."}),"\n",(0,r.jsx)(n.h2,{id:"features",children:"Features"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\ud83d\udcdd ",(0,r.jsx)(n.strong,{children:"SQL Script Execution"}),": Run SQL scripts directly with Flink."]}),"\n",(0,r.jsxs)(n.li,{children:["\ud83e\uddfe ",(0,r.jsx)(n.strong,{children:"Compiled Plan Execution"}),": Run pre-compiled Flink SQL plans to manage production deployments and versioning."]}),"\n",(0,r.jsxs)(n.li,{children:["\ud83d\udd04 ",(0,r.jsx)(n.strong,{children:"Environment Variable Substitution"}),": Inject environment variables ",(0,r.jsx)(n.code,{children:"${VAR}"})," into SQL scripts and configs at runtime."]}),"\n",(0,r.jsxs)(n.li,{children:["\ud83d\udce6 ",(0,r.jsx)(n.strong,{children:"JAR Dependency Management"}),": Reference local directories with required JARs (e.g. UDFs)."]}),"\n",(0,r.jsxs)(n.li,{children:["\ud83c\udf0d ",(0,r.jsx)(n.strong,{children:"Kubernetes-Friendly"}),": Built to run with the Flink Kubernetes Operator."]}),"\n",(0,r.jsxs)(n.li,{children:["\ud83d\udd27 ",(0,r.jsx)(n.strong,{children:"Function Infrastructure"}),": Utilities for writing and loading UDFs as system functions."]}),"\n",(0,r.jsxs)(n.li,{children:["\ud83e\ude84 ",(0,r.jsx)(n.strong,{children:"Flink Extensions"}),":","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"\ud83d\udc80 Dead-letter queue support in Kafka for poison message handling."}),"\n",(0,r.jsx)(n.li,{children:"\ud83d\ude80 Native JSON and Vector types with JSON format and PostgreSQL connector support."}),"\n",(0,r.jsx)(n.li,{children:"\ud83d\udcda Function libraries for additional functionality in Flink SQL (advanced math, OpenAI, etc)"}),"\n",(0,r.jsx)(n.li,{children:"\u2699\ufe0f Additional configuration options for CSV format."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"flink-sql-runner-usage",children:"Flink SQL Runner Usage"}),"\n",(0,r.jsx)(n.p,{children:"You can use the docker image to run Flink SQL scripts or compiled plans locally or in Kubernetes.\nThe docker image contains the executable flink-sql-runner.jar file which supports the following command line arguments:"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Argument"}),(0,r.jsx)(n.th,{children:"Description"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"-p, --planfile"})}),(0,r.jsx)(n.td,{children:"Compiled plan (i.e. JSON file) to execute"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"-s, --sqlfile"})}),(0,r.jsx)(n.td,{children:"Flink SQL script to execute"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"-c, --config-dir"})}),(0,r.jsxs)(n.td,{children:["Directory containing the ",(0,r.jsx)(n.a,{href:"https://nightlies.apache.org/flink/flink-docs-release-1.19/docs/deployment/config/",children:"Flink configuration YAML file"})]})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"-u, --udfpath"})}),(0,r.jsx)(n.td,{children:"Path to JAR files that implement user defined functions (UDFs) or other runtime extensions for Flink"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"-m, --mode"})}),(0,r.jsxs)(n.td,{children:["Optional argument to specify ",(0,r.jsx)(n.a,{href:"https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/datastream/execution_mode/",children:"Flink execution mode"})," (",(0,r.jsx)(n.code,{children:"STREAMING"})," (default), ",(0,r.jsx)(n.code,{children:"BATCH"}),", or ",(0,r.jsx)(n.code,{children:"AUTOMATIC"}),")"]})]})]})]}),"\n",(0,r.jsxs)(n.blockquote,{children:["\n",(0,r.jsx)(n.p,{children:"[!WARNING]\nThe runner expects either a Flink SQL script or a compiled plan - not both."}),"\n",(0,r.jsxs)(n.p,{children:["The ",(0,r.jsx)(n.code,{children:"--mode"})," argument - even if it is explicitly set - will be ignored if the Flink YAML configuration set via ",(0,r.jsx)(n.code,{children:"--config-dir"})," contains ",(0,r.jsx)(n.code,{children:"execution.runtime-mode"}),"."]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["We strongly recommend to run compiled plans for production Flink SQL applications since they support\nlifecycle management of applications, are stable across Flink versions, and provide more control over\nthe executed JobGraph.\nYou can use the ",(0,r.jsx)(n.a,{href:"https://github.com/DataSQRL/sqrl/",children:"SQRL compiler"})," to compile Flink SQL applications to compiled plans."]}),"\n",(0,r.jsx)(n.h3,{id:"running-locally",children:"Running Locally"}),"\n",(0,r.jsx)(n.p,{children:"To run Flink SQL Runner locally using Docker in a self-contained cluster (JobManager and TaskManager in a single container):"}),"\n",(0,r.jsxs)(n.p,{children:["1. Create your SQL script\nPut your Flink SQL (e.g., ",(0,r.jsx)(n.code,{children:"flink.sql"}),") in a local directory, such as:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"./sql-scripts/flink.sql\n"})}),"\n",(0,r.jsx)(n.p,{children:"2. Run the Docker image\nThis starts a full standalone Flink session cluster in one container:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:'docker run -d --rm -it \\\n  -p 8081:8081 \\\n  -v "$PWD/sql-scripts":/flink/sql \\\n  --name runner \\\n  datasqrl/flink-sql-runner:0.8.1-flink-1.19 \\\n  cluster\n'})}),"\n",(0,r.jsx)(n.p,{children:"3. Submit your SQL job\nIn a separate terminal, run:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"docker exec -it runner flink run flink-sql-runner.jar --sqlfile /flink/sql/flink.sql\n"})}),"\n",(0,r.jsx)(n.p,{children:"The job will be submitted to the embedded JobManager and executed using the local TaskManager."}),"\n",(0,r.jsxs)(n.blockquote,{children:["\n",(0,r.jsxs)(n.p,{children:["[!NOTE]",(0,r.jsx)(n.br,{}),"\n","The ",(0,r.jsx)(n.code,{children:"flink-sql-runner.jar"})," is a symlink placed in the Flink root directory (",(0,r.jsx)(n.code,{children:"/opt/flink"}),") for easier access, but the actual file resides in its own plugin directory: ",(0,r.jsx)(n.code,{children:"/opt/flink/plugins/flink-sql-runner"}),".\nIt is possible to add any Flink arguments or run any accessible JAR, just like with a vanilla ",(0,r.jsx)(n.code,{children:"flink run"})," command."]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"4. Inspect output\nIf your SQL uses the print connector as a sink, you can check logs via:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"docker exec -it runner bash -c \"cat /opt/flink/log/$(ls /opt/flink/log | grep 'flink--taskexecutor' | grep '.out')\"\n"})}),"\n",(0,r.jsxs)(n.p,{children:["Or use the Flink UI at ",(0,r.jsx)(n.a,{href:"http://localhost:8081",children:"http://localhost:8081"})," to monitor jobs."]}),"\n",(0,r.jsx)(n.h3,{id:"running-in-kubernetes-with-flink-operator",children:"Running in Kubernetes with Flink Operator"}),"\n",(0,r.jsx)(n.p,{children:"Here's how to use the Flink Jar Runner with the Flink Operator on Kubernetes:"}),"\n",(0,r.jsxs)(n.p,{children:["1. Prepare Your Files: Ensure that your SQL scripts (",(0,r.jsx)(n.code,{children:"statements.sql"}),") or compiled plans (",(0,r.jsx)(n.code,{children:"compiled_plan.json"}),"), and JAR files are accessible within your container."]}),"\n",(0,r.jsx)(n.p,{children:"Example Helm chart configuration:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-yaml",children:'apiVersion: flink.apache.org/v1beta1\nkind: FlinkDeployment\nmetadata:\n  name: sql-example\nspec:\n  image: datasqrl/flink-sql-runner:latest\n  flinkVersion: v1_19\n  flinkConfiguration:\n    taskmanager.numberOfTaskSlots: "1"\n  serviceAccount: flink\n  jobManager:\n    resource:\n      memory: "2048m"\n      cpu: 1\n  taskManager:\n    resource:\n      memory: "2048m"\n      cpu: 1\n  job:\n    jarURI: http://raw.github.com/datasqrl/releases/0.8.1/flink-sql-runner.jar\n    args: ["--sqlfile", "/opt/flink/usrlib/sql-scripts/statements.sql", "--planfile", "/opt/flink/usrlib/sql-scripts/compiled_plan.json", "--udfpath", "/opt/flink/usrlib/jars"]\n    parallelism: 1\n    upgradeMode: stateless\n'})}),"\n",(0,r.jsxs)(n.blockquote,{children:["\n",(0,r.jsx)(n.p,{children:"[!WARNING]\nConfigure either the SQL script OR the compiled plan - not both."}),"\n"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Deploy with Helm:"}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"helm install sql-example -f <your-helm-values>.yaml <your-helm-chart>\n"})}),"\n",(0,r.jsx)(n.h3,{id:"environment-variable-substitution",children:"Environment Variable Substitution"}),"\n",(0,r.jsxs)(n.p,{children:["Flink SQL Runner automatically substitutes environment variables in your configuration files, SQL scripts, and compiled plans for secrets and environment specific configuration. Environment variables must be of the form ",(0,r.jsx)(n.code,{children:"${ENV_VARIABLE}"})," and inside of strings."]}),"\n",(0,r.jsxs)(n.p,{children:["For example, ",(0,r.jsx)(n.code,{children:"${DATA_PATH}"})," is an environment variable inside the connector configuration of a table that is substituted at runtime:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-sql",children:"CREATE TEMPORARY TABLE `MyTable` (\n  ...\n) WITH (\n  'connector' = 'filesystem',\n  'format' = 'json',\n  'path' = '${DATA_PATH}/applications.jsonl',\n  'source.monitor-interval' = '1'\n);\n"})}),"\n",(0,r.jsx)(n.h3,{id:"building-your-own-flink-sql-runner",children:"Building Your Own Flink SQL Runner"}),"\n",(0,r.jsx)(n.p,{children:"The Flink SQL runner is published to Maven Central and you can add it as a dependency in your project to extend\nthe runner to suit your needs."}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Maven:"}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-xml",children:"<dependency>\n  <groupId>com.datasqrl.flinkrunner</groupId>\n  <artifactId>flink-sql-runner</artifactId>\n  <version>0.8.1</version>\n</dependency>\n"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Gradle:"}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-groovy",children:"implementation 'com.datasqrl.flinkrunner:flink-sql-runner:0.8.1'\n"})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"flink-extensions",children:"Flink Extensions"}),"\n",(0,r.jsx)(n.p,{children:"The Flink SQL Runner contains a few extensions to the Flink runtime."}),"\n",(0,r.jsx)(n.h3,{id:"dead-letter-queue-support-for-kafka-sources",children:"Dead-Letter-Queue Support for Kafka Sources"}),"\n",(0,r.jsx)(n.p,{children:"If a Flink SQL application fails to deserialize a message from a Kafka topic, the entire job can fail."}),"\n",(0,r.jsxs)(n.p,{children:["This project implements the ",(0,r.jsx)(n.code,{children:"kafka-safe"})," and ",(0,r.jsx)(n.code,{children:"upsert-kafka-safe"})," ",(0,r.jsx)(n.a,{href:"connectors/kafka-safe",children:"connectors"})," which extend the respective kafka connectors with dead-letter-queue support, so that messages which fail to deserialize can be logged, or sent to a dead-letter-queue, instead of failing the job."]}),"\n",(0,r.jsxs)(n.p,{children:["In addition to the configuration options exposed by the original kafka connectors, the ",(0,r.jsx)(n.code,{children:"-safe"})," versions support the following optional configuration options:"]}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Options"}),(0,r.jsx)(n.th,{children:"Default"}),(0,r.jsx)(n.th,{children:"Type"}),(0,r.jsx)(n.th,{children:"Description"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"scan.deser-failure.handler"}),(0,r.jsx)(n.td,{children:"none"}),(0,r.jsx)(n.td,{children:"String"}),(0,r.jsxs)(n.td,{children:["Use ",(0,r.jsx)(n.code,{children:"log"})," to output failed messages to the logger, ",(0,r.jsx)(n.code,{children:"kafka"})," to output failed messages to a kafka topic, or ",(0,r.jsx)(n.code,{children:"none"})," to fail the job."]})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"scan.deser-failure.topic"}),(0,r.jsx)(n.td,{children:"-"}),(0,r.jsx)(n.td,{children:"String"}),(0,r.jsxs)(n.td,{children:["The topic for the dead-letter-queue that failed messages are written to. Required when the handler is configured to ",(0,r.jsx)(n.code,{children:"kafka"}),"."]})]})]})]}),"\n",(0,r.jsxs)(n.blockquote,{children:["\n",(0,r.jsxs)(n.p,{children:["[!NOTE]",(0,r.jsx)(n.br,{}),"\n","The dead-letter-queue producer will use the same Kafka configuration, that is provided for the Flink SQL table that reads the data."]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"jsonb-type",children:"JSONB Type"}),"\n",(0,r.jsxs)(n.p,{children:["This project adds a ",(0,r.jsx)(n.a,{href:"types/json-type",children:"binary JSON type"})," and associated functions for more efficient JSON handling that does not serialize from and to string repeatedly."]}),"\n",(0,r.jsxs)(n.p,{children:["Native JSON type support is also extended to the ",(0,r.jsx)(n.a,{href:"formats/flexible-json-format",children:"JSON format"})," called ",(0,r.jsx)(n.code,{children:"flexible-json"})," for writing JSON data as nested documents (instead of strings) as well as the ",(0,r.jsx)(n.a,{href:"connectors/postgresql-connector",children:"JDBC connector for PostgreSQL"})," to write JSON data to JSONB columns."]}),"\n",(0,r.jsxs)(n.p,{children:["The binary JSON type is supported by ",(0,r.jsx)(n.a,{href:"/docs/stdlib-docs/stdlib-docs/system-functions#jsonb-functions",children:"these system functions"}),"."]}),"\n",(0,r.jsx)(n.h3,{id:"vector-type",children:"Vector Type"}),"\n",(0,r.jsxs)(n.p,{children:["This project adds a native ",(0,r.jsx)(n.a,{href:"types/vector-type",children:"Vector type"})," and associated functions for more efficient handling of vectors (e.g. for content embeddings)."]}),"\n",(0,r.jsxs)(n.p,{children:["Native Vector type support is also extended to the ",(0,r.jsx)(n.a,{href:"connectors/postgresql-connector",children:"JDBC connector for PostgreSQL"})," to write vector data to vector columns for the ",(0,r.jsx)(n.code,{children:"pgvector"})," extension."]}),"\n",(0,r.jsxs)(n.p,{children:["The native vector type is supported by ",(0,r.jsx)(n.a,{href:"/docs/stdlib-docs/stdlib-docs/system-functions#vector-functions",children:"these system functions"}),"."]}),"\n",(0,r.jsx)(n.h3,{id:"function-libraries",children:"Function Libraries"}),"\n",(0,r.jsx)("img",{src:"stdlib-docs/img/sqrl_functions_logo.png",alt:"Flink SQL Runner Logo",width:"300",align:"right"}),"\n",(0,r.jsx)(n.p,{children:"Implementation of Flink SQL and SQRL functions that can be added as user-defined functions (UDFs) to support additional functionality."}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"/docs/stdlib-docs/stdlib-docs/library-functions#math",children:"Math"}),": Advanced math functions"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"/docs/stdlib-docs/stdlib-docs/library-functions#openai",children:"OpenAI"}),": Function for calling completions, structured data extraction, and vector embeddings."]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"usage",children:"Usage"}),"\n",(0,r.jsx)(n.h3,{id:"within-datasqrl",children:"Within DataSQRL"}),"\n",(0,r.jsxs)(n.p,{children:["If you are using the ",(0,r.jsx)(n.a,{href:"https://github.com/DataSQRL/sqrl",children:"DataSQRL framework"})," to compile your SQRL project, you can import the function library as follows:"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.code,{children:"IMPORT stdlib.[library-name].*"})}),"\n",(0,r.jsxs)(n.p,{children:["where ",(0,r.jsx)(n.code,{children:"[library-name]"})," is replaced with the name of the library, e.g. ",(0,r.jsx)(n.code,{children:"stdlib.math.*"}),"."]}),"\n",(0,r.jsx)(n.p,{children:"To import a single function:"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.code,{children:"IMPORT stdlib.[library-name].[function-name]"})}),"\n",(0,r.jsxs)(n.p,{children:["e.g. ",(0,r.jsx)(n.code,{children:"stdlib.text.split"}),"."]}),"\n",(0,r.jsx)(n.h3,{id:"flink-sql-runner-1",children:"Flink SQL Runner"}),"\n",(0,r.jsx)(n.p,{children:"To use a function library with the Flink SQL Runner:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Copy the JAR file for the function library to the UDF directory that is passed as an argument."}),"\n",(0,r.jsx)(n.li,{children:"Declare the function in your Flink SQL script:"}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-sql",children:"CREATE FUNCTION TheFunctionToAdd AS 'com.datasqrl.flinkrunner.[library-name].[function-name]';\n"})}),"\n",(0,r.jsxs)(n.p,{children:["where you replace ",(0,r.jsx)(n.code,{children:"[library-name]"})," with the name of the function library and ",(0,r.jsx)(n.code,{children:"[function-name]"})," with the name of the function."]}),"\n",(0,r.jsx)(n.h3,{id:"custom-flink-implementation",children:"Custom Flink Implementation"}),"\n",(0,r.jsx)(n.p,{children:"If you are building your own Flink SQL runner, you can depend on the function modules and load the functions into your project."}),"\n",(0,r.jsx)(n.h3,{id:"csv-format",children:"CSV Format"}),"\n",(0,r.jsxs)(n.p,{children:["The ",(0,r.jsx)(n.code,{children:"flexible-csv"})," format extends the standard csv format with a configuration option ",(0,r.jsx)(n.code,{children:"skip-header"})," to skip the first row in a CSV file (i.e. the header)."]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"community-contributions",children:"Community Contributions"}),"\n",(0,r.jsxs)(n.p,{children:["Contributions are welcome! Feel free to open an issue or submit a ",(0,r.jsx)(n.a,{href:"https://github.com/DataSQRL/flink-sql-runner/pulls",children:"pull request"})," on GitHub."]}),"\n",(0,r.jsx)(n.h3,{id:"releasing",children:"Releasing"}),"\n",(0,r.jsxs)(n.p,{children:["Release process is fully automated and driven by github release. Just ",(0,r.jsx)(n.a,{href:"https://github.com/DataSQRL/flink-sql-runner/releases/new",children:"create a new release"})," and github action will take care of the rest. The new release version will match the ",(0,r.jsx)(n.code,{children:"tag"}),", so must use ",(0,r.jsx)(n.a,{href:"https://semver.org/",children:"semver"})," when selecting tag name."]}),"\n",(0,r.jsx)(n.h3,{id:"license",children:"License"}),"\n",(0,r.jsxs)(n.p,{children:["This project is licensed under the Apache 2 License. See the ",(0,r.jsx)(n.a,{href:"https://github.com/DataSQRL/flink-sql-runner/blob/main/LICENSE",children:"LICENSE"})," file for details."]}),"\n",(0,r.jsx)(n.h3,{id:"contact--support",children:"Contact & Support"}),"\n",(0,r.jsxs)(n.p,{children:["For any questions or support, please open an ",(0,r.jsx)(n.a,{href:"https://github.com/DataSQRL/flink-sql-runner/issues",children:"issue"})," in the GitHub repository."]})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>l,x:()=>o});var s=i(6540);const r={},t=s.createContext(r);function l(e){const n=s.useContext(t);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:l(e.components),s.createElement(t.Provider,{value:n},e.children)}}}]);