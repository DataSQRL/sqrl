"use strict";(self.webpackChunkdocumentation=self.webpackChunkdocumentation||[]).push([[5312],{110:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>r,default:()=>h,frontMatter:()=>o,metadata:()=>s,toc:()=>c});var s=n(1941),i=n(4848),a=n(8453);const o={slug:"flinksql-extensions",title:"Defining Data Interfaces with FlinkSQL",authors:["matthias"],tags:["Join","Flink","DataSQRL"]},r="Defining Data Interfaces with FlinkSQL",l={authorsImageUrls:[void 0]},c=[{value:"Building Data APIs with FlinkSQL",id:"building-data-apis-with-flinksql",level:2},{value:"Relationships for Complex Data Structures",id:"relationships-for-complex-data-structures",level:2},{value:"Code Modularity and Connector Management",id:"code-modularity-and-connector-management",level:2},{value:"Learn More",id:"learn-more",level:2}];function d(e){const t={a:"a",code:"code",h2:"h2",p:"p",pre:"pre",...(0,a.R)(),...e.components},{Head:n}=t;return n||function(e,t){throw new Error("Expected "+(t?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Head",!0),(0,i.jsxs)(i.Fragment,{children:[(0,i.jsxs)(n,{children:[(0,i.jsx)("meta",{property:"og:image",content:"/img/blog/flinksql_extension_api.png"}),(0,i.jsx)("meta",{name:"twitter:image",content:"/img/blog/flinksql_extension_api.png"})]}),"\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.a,{href:"https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/table/sql/overview/",children:"FlinkSQL"})," is an amazing innovation in data processing: it packages the power of realtime stream processing within the simplicity of SQL.\nThat means you can start with the SQL you know and introduce stream processing constructs as you need them."]}),"\n",(0,i.jsx)("img",{src:"/img/blog/flinksql_extension_api.png",alt:"FlinkSQL API Extension >",width:"40%"}),"\n",(0,i.jsx)(t.p,{children:"FlinkSQL adds the ability to process data incrementally to the classic set-based semantics of SQL. In addition, FlinkSQL supports source and sink connectors making it easy to ingest data from and move data to other systems. That's a powerful combination which covers a lot of data processing use cases."}),"\n",(0,i.jsx)(t.p,{children:"In fact, it only takes a few extensions to FlinkSQL to build entire data applications. Let's see how that works."}),"\n",(0,i.jsx)(t.h2,{id:"building-data-apis-with-flinksql",children:"Building Data APIs with FlinkSQL"}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{className:"language-sql",children:"CREATE TABLE UserTokens (\nuserid BIGINT NOT NULL,\ntokens BIGINT NOT NULL,\nrequest_time TIMESTAMP_LTZ(3) NOT NULL METADATA FROM 'timestamp'\n);\n\n/*+query_by_all(userid) */\nTotalUserTokens := SELECT userid, sum(tokens) as total_tokens,\ncount(tokens) as total_requests\nFROM UserTokens GROUP BY userid;\n\nUserTokensByTime(userid BIGINT NOT NULL, fromTime TIMESTAMP NOT NULL, toTime TIMESTAMP NOT NULL):=\n                SELECT * FROM UserTokens WHERE userid = :userid,\n                request_time >= :fromTime AND request_time < :toTime ORDER BY request_time DESC;\n\nUsageAlert := SUBSCRIBE SELECT * FROM UserTokens WHERE tokens > 100000;\n"})}),"\n",(0,i.jsxs)(t.p,{children:["This script defines a sequence of tables. We introduce ",(0,i.jsx)(t.code,{children:":="})," as syntactic sugar for the verbose ",(0,i.jsx)(t.code,{children:"CREATE TEMPORARY VIEW"})," syntax."]}),"\n",(0,i.jsxs)(t.p,{children:["The ",(0,i.jsx)(t.code,{children:"UserTokens"})," table does not have a configured connector, which mean we treat it as an API mutation endpoint connected to Flink via a Kafka topic that captures the events. This makes it easy to build APIs that capture user activity, transactions, or other types of events."]}),"\n",(0,i.jsxs)(t.p,{children:["Next, we sum up the data collected through the API for each user. This is a standard FlinkSQL aggregation query and we expose the result in our API through the ",(0,i.jsx)(t.code,{children:"query_by_all"})," hint which defines the arguments for the query endpoint of that table."]}),"\n",(0,i.jsx)(t.p,{children:"We can also explicitly define query endpoints with arguments through SQL table functions. FlinkSQL supports table functions natively. All we had to do is provide the syntax for defining the function signature."}),"\n",(0,i.jsxs)(t.p,{children:["And last, the ",(0,i.jsx)(t.code,{children:"SUBSCRIBE"})," keyword in front of the query defines a subscription endpoint for requests exceeding a certain token count which get pushed to clients in real-time."]}),"\n",(0,i.jsx)(t.p,{children:"Voila, we just build ourselves a complete GraphQL API with mutation, query, and subscription endpoints.\nRun the above script with DataSQRL to see the result:"}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{className:"language-bash",children:"docker run -it --rm -p 8888:8888 -v $PWD:/build datasqrl/cmd run usertokens.sqrl\n"})}),"\n",(0,i.jsx)(t.h2,{id:"relationships-for-complex-data-structures",children:"Relationships for Complex Data Structures"}),"\n",(0,i.jsx)(t.p,{children:"And for extra credit, we can define relationships in FlinkSQL to represent the structure of our data explicitly and expose it in the API:"}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{className:"language-sql",children:"User.totalTokens := SELECT * FROM TotalUserTokens t WHERE this.userid = t.userid LIMIT 1;\n"})}),"\n",(0,i.jsxs)(t.p,{children:["The ",(0,i.jsx)(t.code,{children:"User"})," table in this example is read from an upsert Kafka topic using a standard FlinkSQL ",(0,i.jsx)(t.code,{children:"CREATE TABLE"})," statement."]}),"\n",(0,i.jsx)(t.h2,{id:"code-modularity-and-connector-management",children:"Code Modularity and Connector Management"}),"\n",(0,i.jsx)(t.p,{children:"Many FlinkSQL projects break the codebase into multiple files for better code readability, modularity, or to swap out sources and sinks. That requires extra infrastructure to manage FlinkSQL files and stitch them together."}),"\n",(0,i.jsx)(t.p,{children:"How about we do that directly in FlinkSQL?"}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{className:"language-sql",children:"IMPORT source-data.User;\n"})}),"\n",(0,i.jsxs)(t.p,{children:["Here, we import the ",(0,i.jsx)(t.code,{children:"User"})," table from a separate file within the ",(0,i.jsx)(t.code,{children:"source-data"})," directory, allowing us to separate the data processing logic from the source configurations. It also enables us to use dependency management to swap out sources for local testing vs production."]}),"\n",(0,i.jsx)(t.p,{children:"And we can do the same for sinks:"}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{className:"language-sql",children:"EXPORT UsageAlert TO mysinks.UsageAlert;\n"})}),"\n",(0,i.jsxs)(t.p,{children:["In addition to breaking out the sink configuration from the main script, the ",(0,i.jsx)(t.code,{children:"EXPORT"})," statement functions as an ",(0,i.jsx)(t.code,{children:"INSERT INTO"})," statement and creates a ",(0,i.jsx)(t.code,{children:"STATEMENT SET"})," implicitly. That makes the code easier to read."]}),"\n",(0,i.jsx)(t.h2,{id:"learn-more",children:"Learn More"}),"\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.a,{href:"https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/table/sql/overview/",children:"FlinkSQL"})," is phenomenal extension of the SQL ecosystem to stream processing. With DataSQRL, we are trying to make it easier to build end-to-end data pipelines and complete data applications with FlinkSQL."]}),"\n",(0,i.jsxs)(t.p,{children:["Check out the ",(0,i.jsx)(t.a,{href:"/docs/getting-started",children:"complete example"})," which also covers testing, customization, and deployment. Or read the ",(0,i.jsx)(t.a,{href:"/docs/sqrl-language",children:"documentation"})," to learn more."]})]})}function h(e={}){const{wrapper:t}={...(0,a.R)(),...e.components};return t?(0,i.jsx)(t,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},1941:e=>{e.exports=JSON.parse('{"permalink":"/blog/flinksql-extensions","source":"@site/blog/2025-05-09-flink-sql-extensions.md","title":"Defining Data Interfaces with FlinkSQL","description":"FlinkSQL is an amazing innovation in data processing: it packages the power of realtime stream processing within the simplicity of SQL.","date":"2025-05-09T00:00:00.000Z","tags":[{"inline":true,"label":"Join","permalink":"/blog/tags/join"},{"inline":true,"label":"Flink","permalink":"/blog/tags/flink"},{"inline":true,"label":"DataSQRL","permalink":"/blog/tags/data-sqrl"}],"readingTime":3.435,"hasTruncateMarker":false,"authors":[{"name":"Matthias Broecheler","title":"CEO of DataSQRL","url":"https://github.com/mbroecheler","page":{"permalink":"/blog/authors/matthias"},"socials":{"linkedin":"https://www.linkedin.com/in/matthiasbroecheler/","github":"https://github.com/mbroecheler","newsletter":"https://www.matthiasb.com"},"imageURL":"/img/headshots/matthias1.png","key":"matthias"}],"frontMatter":{"slug":"flinksql-extensions","title":"Defining Data Interfaces with FlinkSQL","authors":["matthias"],"tags":["Join","Flink","DataSQRL"]},"unlisted":false,"nextItem":{"title":"DataSQRL 0.6 Release: The Streaming Data Framework","permalink":"/blog/datasqrl-0.6-release"}}')},8453:(e,t,n)=>{n.d(t,{R:()=>o,x:()=>r});var s=n(6540);const i={},a=s.createContext(i);function o(e){const t=s.useContext(a);return s.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function r(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:o(e.components),s.createElement(a.Provider,{value:t},e.children)}}}]);