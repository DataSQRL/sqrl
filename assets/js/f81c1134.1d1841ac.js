"use strict";(self.webpackChunkdocumentation=self.webpackChunkdocumentation||[]).push([[8130],{7735:e=>{e.exports=JSON.parse('{"archive":{"blogPosts":[{"id":"datasqrl-0.7-release","metadata":{"permalink":"/blog/datasqrl-0.7-release","source":"@site/blog/2025-07-27-datasqrl-0.7.md","title":"DataSQRL 0.7 Release: The Data Delivery Interface","description":"|\\" width=\\"40%\\"/>","date":"2025-07-27T00:00:00.000Z","tags":[{"inline":false,"label":"Release","permalink":"/blog/tags/release","description":"DataSQRL release announcements"}],"readingTime":2.945,"hasTruncateMarker":true,"authors":[{"name":"Matthias Broecheler","title":"CEO of DataSQRL","url":"https://github.com/mbroecheler","page":{"permalink":"/blog/authors/matthias"},"socials":{"linkedin":"https://www.linkedin.com/in/matthiasbroecheler/","github":"https://github.com/mbroecheler","newsletter":"https://www.matthiasb.com"},"imageURL":"/img/headshots/matthias1.png","key":"matthias"}],"frontMatter":{"slug":"datasqrl-0.7-release","title":"DataSQRL 0.7 Release: The Data Delivery Interface","authors":["matthias"],"tags":["release"]},"unlisted":false,"nextItem":{"title":"Flink SQL Runner: Run Flink SQL Without JARs or Glue Code","permalink":"/blog/flinkrunner-announcement"}},"content":"<head>\\n  <meta property=\\"og:image\\" content=\\"/img/blog/release_0.7.0.png\\" />\\n  <meta name=\\"twitter:image\\" content=\\"/img/blog/release_0.7.0.png\\" />\\n</head>\\n\\n# DataSQRL 0.7 Release: The Data Delivery Interface\\n\\n<img src=\\"/img/blog/release_0.7.0.png\\" alt=\\"DataSQRL 0.7.0 Release >|\\" width=\\"40%\\"/>\\n\\nDataSQRL 0.7 marks a major milestone in our journey to automate data pipelines, thanks to significant improvements to the serving layer:\\n\\n* Support for the Model Context Protocol (MCP) for tooling and resource access\\n* REST API support\\n* JWT-based authentication and authorization\\n\\nThese features enable developers to build a wide range of production-ready data interfaces.\\nThis release also includes performance and configuration improvements to the serving layer of DataSQRL-generated pipelines.\\n\\nYou can find the full release notes and source code on our [GitHub release page](https://github.com/DataSQRL/sqrl/releases/tag/0.7.0). \\nTo update your local installation of DataSQRL, simply pull the latest Docker image:\\n```bash\\ndocker pull datasqrl/cmd:0.7.0\\n```\\n\\n## The Last Mile: Data Delivery\\n\\nData delivery is the final and most visible stage of any data pipeline. It\'s how users, applications, and AI agents actually access and consume data. Most enterprise data interactions happen through APIs, making the delivery interface a critical component. At DataSQRL, we\'ve invested heavily in automating the upstream parts of the pipeline: from Flink-powered data processing to Postgres-backed storage. With version 0.7, we turn our focus to the serving layer: introducing support for the Model Context Protocol (MCP) and REST APIs, as well as JWT-based authentication and authorization. These additions ensure seamless integration with most authentication providers and enable secure, token-based data access, with fine-grained authorization logic enforced directly in the SQRL script. This completes our vision of end-to-end pipeline automation, where consumption patterns inform data storage and processing\u2014closing the loop between data production and usage.\\n\\nCheck out the [interface documentation](../docs/interface) for more information.\\n\\n\x3c!--truncate--\x3e\\n\\n## Major Contributions\\n\\nIn addition to the flagship features\u2014MCP, REST, and JWT support\u2014which we\u2019ll discuss in more detail in future blog posts, the 0.7.0 release contains a number of additional features and improvements.\\n\\n### Configuration & CLI Improvements\\n- Refactored CLI and Flink configuration logic.\\n- Improved error messages during package config validation.\\n- Enforced predictable ordering and sorting of config keys.\\n- Migrated deprecated config key naming (`-dir` \u2192 `-folder`), now with compile-time warnings.\\n- Standardized configuration schema and structured logging to `/build/logs`.\\n\\n### Testing Infrastructure Enhancements\\n- Added new `sqrl-container-testing` module.\\n- Converted tests to use AssertJ.\\n- Increased test coverage for `SqrlConfig` and dependency mapping.\\n- Fixed test runner error reporting and exit code handling.\\n- Reworked dependent service startup to trigger post-compilation.\\n\\n### Flink-SQL Runner Integration\\n- Integrated `flink-sql-runner` into `DatasqrlRun`.\\n- Temporarily merged `sqrl-test` module into `sqrl-run`.\\n- Simplified Docker image setup (public `ghcr.io` images).\\n- Updated submodule paths and version to `0.7.0`.\\n\\n### Authentication & API Enhancements\\n- Added initial JWT-based authentication support.\\n- Published documentation for JWT and Swagger-based OpenAPI specs for REST endpoints.\\n- Added batch GraphQL mutation support with transactional semantics.\\n- Replaced `GraphQLBigInteger` with native `Long` handling.\\n\\n### Kafka & Runtime Improvements\\n- Kafka topic names now support templating.\\n- Added an async OpenAI test use case and resolved snapshot issues.\\n- Fixed intermittent WebSocket failures in `SubscriptionClient`.\\n\\n### Project Structure and CI Pipeline\\n- Simplified project structure and removed outdated dependency declarations.\\n- Refactored CI pipeline and added automated GitHub package cleanup workflow.\\n\\n### Updated Dependencies\\n\\nUpgraded versions for the following dependencies and patched critical vulnerabilities:\\n\\n- Apache Flink and Flink connectors (e.g., Postgres CDC)\\n- Vert.x and related plugins\\n- Apache Iceberg, DuckDB, PostgreSQL JDBC\\n- AWS SDK BOM\\n- JSON Schema Validator, Netty, OpenCSV\\n- Micrometer, Log4j, Reactor, Immutables, Testcontainers\\n- Maven plugins: Enforcer, GPG, Build-helper"},{"id":"flinkrunner-announcement","metadata":{"permalink":"/blog/flinkrunner-announcement","source":"@site/blog/2025-06-09-flink-sql-runner-annoucement.md","title":"Flink SQL Runner: Run Flink SQL Without JARs or Glue Code","description":"Apache Flink has long been a powerhouse for streaming and batch data processing. And with the rise of Flink SQL, developers can now build sophisticated pipelines using a declarative language they already know. But getting Flink SQL applications into production still comes with friction: packaging JARs, managing connectors, injecting secrets, and wiring up deployment infrastructure.","date":"2025-06-09T00:00:00.000Z","tags":[{"inline":true,"label":"Flink","permalink":"/blog/tags/flink"},{"inline":true,"label":"DataSQRL","permalink":"/blog/tags/data-sqrl"}],"readingTime":1.915,"hasTruncateMarker":true,"authors":[{"name":"Matthias Broecheler","title":"CEO of DataSQRL","url":"https://github.com/mbroecheler","page":{"permalink":"/blog/authors/matthias"},"socials":{"linkedin":"https://www.linkedin.com/in/matthiasbroecheler/","github":"https://github.com/mbroecheler","newsletter":"https://www.matthiasb.com"},"imageURL":"/img/headshots/matthias1.png","key":"matthias"}],"frontMatter":{"slug":"flinkrunner-announcement","title":"Flink SQL Runner: Run Flink SQL Without JARs or Glue Code","authors":["matthias"],"tags":["Flink","DataSQRL"]},"unlisted":false,"prevItem":{"title":"DataSQRL 0.7 Release: The Data Delivery Interface","permalink":"/blog/datasqrl-0.7-release"},"nextItem":{"title":"Defining Data Interfaces with FlinkSQL","permalink":"/blog/flinksql-extensions"}},"content":"<head>\\n  <meta property=\\"og:image\\" content=\\"/img/blog/flinksqlrunner_logo.png\\" />\\n  <meta name=\\"twitter:image\\" content=\\"/img/blog/flinksqlrunner_logo.png\\" />\\n</head>\\n\\n# Flink SQL Runner: Run Flink SQL Without JARs or Glue Code\\n\\nApache Flink has long been a powerhouse for streaming and batch data processing. And with the rise of Flink SQL, developers can now build sophisticated pipelines using a declarative language they already know. But getting Flink SQL applications into production still comes with friction: packaging JARs, managing connectors, injecting secrets, and wiring up deployment infrastructure.\\n\\n<img src=\\"/img/blog/flinksqlrunner_logo.png\\" alt=\\"FlinkSQL Runner >\\" width=\\"40%\\"/>\\n\\n[**Flink SQL Runner**](https://github.com/DataSQRL/flink-sql-runner/) is here to change that. It\'s an open-source toolkit that simplifies development, deployment, and operation of Flink SQL applications\u2014locally or in Kubernetes\u2014without manual JAR assembly or scripting custom infrastructure pipelines.\\n\\n\x3c!--truncate--\x3e\\n\\n## From SQL to Production, Minus the Plumbing\\n\\nImagine you\'re writing a Flink SQL job that reads from Kafka, enriches the data, and sinks to Iceberg. In theory, it\'s just SQL. But in practice, production deployment requires:\\n\\n* Assembling dependencies into a JAR\\n* Writing YAML to configure connectors\\n* Injecting secrets for different environments\\n\\nFlink SQL Runner eliminates those headaches. You get:\\n\\n* **Declarative execution** with SQL scripts or compiled plans\\n* **Simple deployments** on Kubernetes via Flink Operator\\n* **Environment isolation** with variable substitution and UDF packaging\\n\\nAll without leaving the SQL layer.\\n\\n### Key Features\\n\\n* **SQL and Plan Execution**: Run raw SQL scripts or pre-compiled execution plans.\\n* **Kubernetes-Native**: Built for the Flink Kubernetes Operator\u2014deploy SQL jobs without writing infrastructure code.\\n* **Composable Toolkit**: Use the pieces you need\u2014Docker image, libraries, extensions\u2014to suit your environment.\\n* **Environment Variable Substitution**: Inject secrets and environment-specific config into SQL or plan files using `${ENV_VAR}` syntax.\\n* **UDF Infrastructure**: Load custom JARs and register system functions easily.\\n* **Function Libraries**: Drop-in UDFs for advanced math and OpenAI integration.\\n\\n\\n### Flexible and Extensible\\n\\nFlink SQL Runner is not a monolith. You can:\\n\\n* Run it standalone with Docker.\\n* Deploy it with the Flink Kubernetes Operator.\\n* Extend it via Maven or Gradle in your own Flink stack:\\n\\n```xml\\n<dependency>\\n  <groupId>com.datasqrl.flinkrunner</groupId>\\n  <artifactId>flink-sql-runner</artifactId>\\n  <version>0.6.0</version>\\n</dependency>\\n```\\n\\n\\n## Get Started\\n\\nThe Flink SQL Runner project is open source and [available on Github](https://github.com/datasqrl/flink-sql-runner).\\nCheck out the README for more information on how to use and deploy the Flink SQL Runner.\\n\\nTry it out, report issues, or contribute your own UDFs."},{"id":"flinksql-extensions","metadata":{"permalink":"/blog/flinksql-extensions","source":"@site/blog/2025-05-09-flink-sql-extensions.md","title":"Defining Data Interfaces with FlinkSQL","description":"FlinkSQL is an amazing innovation in data processing: it packages the power of realtime stream processing within the simplicity of SQL.","date":"2025-05-09T00:00:00.000Z","tags":[{"inline":true,"label":"Join","permalink":"/blog/tags/join"},{"inline":true,"label":"Flink","permalink":"/blog/tags/flink"},{"inline":true,"label":"DataSQRL","permalink":"/blog/tags/data-sqrl"}],"readingTime":3.44,"hasTruncateMarker":true,"authors":[{"name":"Matthias Broecheler","title":"CEO of DataSQRL","url":"https://github.com/mbroecheler","page":{"permalink":"/blog/authors/matthias"},"socials":{"linkedin":"https://www.linkedin.com/in/matthiasbroecheler/","github":"https://github.com/mbroecheler","newsletter":"https://www.matthiasb.com"},"imageURL":"/img/headshots/matthias1.png","key":"matthias"}],"frontMatter":{"slug":"flinksql-extensions","title":"Defining Data Interfaces with FlinkSQL","authors":["matthias"],"tags":["Join","Flink","DataSQRL"]},"unlisted":false,"prevItem":{"title":"Flink SQL Runner: Run Flink SQL Without JARs or Glue Code","permalink":"/blog/flinkrunner-announcement"},"nextItem":{"title":"DataSQRL 0.6 Release: The Streaming Data Framework","permalink":"/blog/datasqrl-0.6-release"}},"content":"<head>\\n  <meta property=\\"og:image\\" content=\\"/img/blog/flinksql_extension_api.png\\" />\\n  <meta name=\\"twitter:image\\" content=\\"/img/blog/flinksql_extension_api.png\\" />\\n</head>\\n\\n# Defining Data Interfaces with FlinkSQL\\n\\n[FlinkSQL](https://nightlies.apache.org/flink/flink-docs-release-1.19/docs/dev/table/sql/overview/) is an amazing innovation in data processing: it packages the power of realtime stream processing within the simplicity of SQL.\\nThat means you can start with the SQL you know and introduce stream processing constructs as you need them.\\n\\n<img src=\\"/img/blog/flinksql_extension_api.png\\" alt=\\"FlinkSQL API Extension >\\" width=\\"40%\\"/>\\n\\nFlinkSQL adds the ability to process data incrementally to the classic set-based semantics of SQL. In addition, FlinkSQL supports source and sink connectors making it easy to ingest data from and move data to other systems. That\'s a powerful combination which covers a lot of data processing use cases.\\n\\nIn fact, it only takes a few extensions to FlinkSQL to build entire data applications. Let\'s see how that works.\\n\\n## Building Data APIs with FlinkSQL\\n\\n```sql\\nCREATE TABLE UserTokens (\\nuserid BIGINT NOT NULL,\\ntokens BIGINT NOT NULL,\\nrequest_time TIMESTAMP_LTZ(3) NOT NULL METADATA FROM \'timestamp\'\\n);\\n\\n/*+query_by_all(userid) */\\nTotalUserTokens := SELECT userid, sum(tokens) as total_tokens,\\ncount(tokens) as total_requests\\nFROM UserTokens GROUP BY userid;\\n\\nUserTokensByTime(userid BIGINT NOT NULL, fromTime TIMESTAMP NOT NULL, toTime TIMESTAMP NOT NULL):=\\n                SELECT * FROM UserTokens WHERE userid = :userid,\\n                request_time >= :fromTime AND request_time < :toTime ORDER BY request_time DESC;\\n\\nUsageAlert := SUBSCRIBE SELECT * FROM UserTokens WHERE tokens > 100000;\\n```\\n\\nThis script defines a sequence of tables. We introduce `:=` as syntactic sugar for the verbose `CREATE TEMPORARY VIEW` syntax.\\n\\nThe `UserTokens` table does not have a configured connector, which mean we treat it as an API mutation endpoint connected to Flink via a Kafka topic that captures the events. This makes it easy to build APIs that capture user activity, transactions, or other types of events.\\n\\n\x3c!--truncate--\x3e\\n\\nNext, we sum up the data collected through the API for each user. This is a standard FlinkSQL aggregation query and we expose the result in our API through the `query_by_all` hint which defines the arguments for the query endpoint of that table.\\n\\nWe can also explicitly define query endpoints with arguments through SQL table functions. FlinkSQL supports table functions natively. All we had to do is provide the syntax for defining the function signature.\\n\\nAnd last, the `SUBSCRIBE` keyword in front of the query defines a subscription endpoint for requests exceeding a certain token count which get pushed to clients in real-time.\\n\\nVoila, we just build ourselves a complete GraphQL API with mutation, query, and subscription endpoints.\\nRun the above script with DataSQRL to see the result:\\n\\n```bash\\ndocker run -it --rm -p 8888:8888 -v $PWD:/build datasqrl/cmd run usertokens.sqrl\\n```\\n\\n## Relationships for Complex Data Structures\\n\\nAnd for extra credit, we can define relationships in FlinkSQL to represent the structure of our data explicitly and expose it in the API:\\n\\n```sql\\nUser.totalTokens := SELECT * FROM TotalUserTokens t WHERE this.userid = t.userid LIMIT 1;\\n```\\n\\nThe `User` table in this example is read from an upsert Kafka topic using a standard FlinkSQL `CREATE TABLE` statement.\\n\\n## Code Modularity and Connector Management\\n\\nMany FlinkSQL projects break the codebase into multiple files for better code readability, modularity, or to swap out sources and sinks. That requires extra infrastructure to manage FlinkSQL files and stitch them together.\\n\\nHow about we do that directly in FlinkSQL?\\n\\n```sql\\nIMPORT source-data.User;\\n```\\n\\nHere, we import the `User` table from a separate file within the `source-data` directory, allowing us to separate the data processing logic from the source configurations. It also enables us to use dependency management to swap out sources for local testing vs production.\\n\\nAnd we can do the same for sinks:\\n\\n```sql\\nEXPORT UsageAlert TO mysinks.UsageAlert;\\n```\\n\\nIn addition to breaking out the sink configuration from the main script, the `EXPORT` statement functions as an `INSERT INTO` statement and creates a `STATEMENT SET` implicitly. That makes the code easier to read.\\n\\n## Learn More\\n\\n[FlinkSQL](https://nightlies.apache.org/flink/flink-docs-release-1.19/docs/dev/table/sql/overview/) is phenomenal extension of the SQL ecosystem to stream processing. With DataSQRL, we are trying to make it easier to build end-to-end data pipelines and complete data applications with FlinkSQL.\\n\\nCheck out the [complete example](/docs/getting-started) which also covers testing, customization, and deployment. Or read the [documentation](/docs/sqrl-language) to learn more."},{"id":"datasqrl-0.6-release","metadata":{"permalink":"/blog/datasqrl-0.6-release","source":"@site/blog/2025-05-07-datasqrl-0.6.md","title":"DataSQRL 0.6 Release: The Streaming Data Framework","description":"The DataSQRL community is proud to announce the release of DataSQRL 0.6. This release marks a major milestone in the evolution of our open-source project, bringing enhanced alignment with Flink SQL and powerful new capabilities to the real-time serving layer.","date":"2025-05-07T00:00:00.000Z","tags":[{"inline":false,"label":"Release","permalink":"/blog/tags/release","description":"DataSQRL release announcements"}],"readingTime":2.56,"hasTruncateMarker":true,"authors":[{"name":"Matthias Broecheler","title":"CEO of DataSQRL","url":"https://github.com/mbroecheler","page":{"permalink":"/blog/authors/matthias"},"socials":{"linkedin":"https://www.linkedin.com/in/matthiasbroecheler/","github":"https://github.com/mbroecheler","newsletter":"https://www.matthiasb.com"},"imageURL":"/img/headshots/matthias1.png","key":"matthias"}],"frontMatter":{"slug":"datasqrl-0.6-release","title":"DataSQRL 0.6 Release: The Streaming Data Framework","authors":["matthias"],"tags":["release"]},"unlisted":false,"prevItem":{"title":"Defining Data Interfaces with FlinkSQL","permalink":"/blog/flinksql-extensions"},"nextItem":{"title":"Why Temporal Join is Stream Processing\u2019s Superpower","permalink":"/blog/temporal-join"}},"content":"<head>\\n  <meta property=\\"og:image\\" content=\\"/img/blog/release_0.6.0.png\\" />\\n  <meta name=\\"twitter:image\\" content=\\"/img/blog/release_0.6.0.png\\" />\\n</head>\\n\\n# DataSQRL 0.6 Release: The Streaming Data Framework\\n\\nThe DataSQRL community is proud to announce the release of DataSQRL 0.6. This release marks a major milestone in the evolution of our open-source project, bringing enhanced alignment with Flink SQL and powerful new capabilities to the real-time serving layer.\\n\\n\\n<img src=\\"/img/blog/release_0.6.0.png\\" alt=\\"DataSQRL 0.6.0 Release >\\" width=\\"40%\\"/>\\n\\nYou can find the full release notes and source code on our [GitHub release page](https://github.com/DataSQRL/sqrl/releases/tag/0.6.0). \\nTo get started with the latest compiler, simply pull the latest Docker image:\\n```bash\\ndocker pull datasqrl/cmd:0.6.0\\n```\\n\\n## A New Chapter: Flink SQL Integration\\n\\nWith DataSQRL 0.6, we are embracing the Flink ecosystem more deeply than ever before. This release introduces a complete re-architecture of the DataSQRL compiler to build directly on top of Flink SQL\'s parser and planner. By aligning our internal model with Flink SQL semantics, we unlock a host of new capabilities and bring DataSQRL users closer to the vibrant Flink ecosystem.\\n\\nThis architectural shift allows DataSQRL to:\\n\\n* **Use Flink SQL syntax as the foundation**, enabling more intuitive query definitions and easier onboarding for users familiar with Flink.\\n* **Extend Flink SQL with domain-specific features**, such as declarative relationship definitions and functions to define the data interface.\\n* **Transpile FlinkSQL to database dialects** for query execution.\\n\\n\x3c!--truncate--\x3e\\n\\n## Serving-Layer Power: Functions & Relationships\\n\\nDataSQRL 0.6 introduces first-class support for defining **functions** and **relationships** in your SQRL scripts. These constructs make it easier to model complex application logic in a modular, declarative fashion.\\n\\nThese features are purpose-built for powering LLM-ready APIs, event-driven architectures, and real-time user-facing applications.\\n\\nCheck out the [language documentation](/docs/sqrl-language) for details.\\n\\n## Developer Tooling\\n\\nDataSQRL 0.6 provides a docker image for compiling, running, and testing SQRL projects. You can now quickly iterate and check the results. Or run automated tests in CI/CD.\\n\\n## Deployment Artifacts\\n\\nDataSQRL 0.6 removes deployment profiles and instead generates all deployment artifacts in the `build/deploy/plan` folder. This makes it easier to integrate with Kubernetes deployment processes (e.g. via Helm) or cloud managed service deployments (e.g. via Terraform).\\n\\n## Breaking Changes & Migration Path\\n\\nAs this is a major release, **DataSQRL 0.6 is not backwards compatible** with version 0.5. The syntax and internal representation have been updated to align with Flink SQL and to support the new compiler architecture.\\n\\nTo help you transition, we\u2019ve provided updated examples and migration guidance in the [DataSQRL examples repository](https://github.com/DataSQRL/datasqrl-examples). We recommend starting with one of the updated use cases to get a feel for the new workflow.\\n\\n## Thanks to the Community\\n\\nThis release wouldn\u2019t have been possible without the contributions, bug reports, and thoughtful feedback from our growing community. Whether you opened a pull request, filed an issue, or joined a discussion, thank you. Your support drives this project forward.\\n\\nWe\u2019re excited to see what you build with DataSQRL 0.6. If you haven\u2019t joined the [community](/community) yet, now\u2019s a great time to get involved: star us on [GitHub](https://github.com/DataSQRL/sqrl), try out the latest release, and share your thoughts.\\n\\nStay tuned for more updates, and happy building."},{"id":"temporal-join","metadata":{"permalink":"/blog/temporal-join","source":"@site/blog/2023-07-10-temporal-join.mdx","title":"Why Temporal Join is Stream Processing\u2019s Superpower","description":"Stream processing technologies like Apache Flink introduce a new type of data transformation that\u2019s very powerful: the temporal join. Temporal joins add context to data streams while being efficient and fast to execute.","date":"2023-07-10T00:00:00.000Z","tags":[{"inline":true,"label":"Join","permalink":"/blog/tags/join"},{"inline":true,"label":"Flink","permalink":"/blog/tags/flink"},{"inline":true,"label":"DataSQRL","permalink":"/blog/tags/data-sqrl"}],"readingTime":7.42,"hasTruncateMarker":true,"authors":[{"name":"Matthias Broecheler","title":"CEO of DataSQRL","url":"https://github.com/mbroecheler","page":{"permalink":"/blog/authors/matthias"},"socials":{"linkedin":"https://www.linkedin.com/in/matthiasbroecheler/","github":"https://github.com/mbroecheler","newsletter":"https://www.matthiasb.com"},"imageURL":"/img/headshots/matthias1.png","key":"matthias"}],"frontMatter":{"slug":"temporal-join","title":"Why Temporal Join is Stream Processing\u2019s Superpower","authors":["matthias"],"tags":["Join","Flink","DataSQRL"]},"unlisted":false,"prevItem":{"title":"DataSQRL 0.6 Release: The Streaming Data Framework","permalink":"/blog/datasqrl-0.6-release"},"nextItem":{"title":"Let\'s Uplevel Our Database Game: Meet DataSQRL","permalink":"/blog/lets-uplevel-database-datasqrl"}},"content":"import useBaseUrl from \'@docusaurus/useBaseUrl\';\\n\\n\\n\\n<head>\\n  <meta property=\\"og:image\\" content={useBaseUrl(\'img/blog/temporal_join.png\')} />\\n  <meta name=\\"twitter:image\\" content={useBaseUrl(\'img/blog/temporal_join.png\')} />\\n</head>\\n\\nStream processing technologies like Apache Flink introduce a new type of data transformation that\u2019s very powerful: the temporal join. Temporal joins add context to data streams while being efficient and fast to execute.\\n\\n<img src={useBaseUrl(\'/img/blog/temporal_join.svg\')} alt=\\"Temporal Join >\\" width=\\"30%\\"/>\\n\\n\\nThis article introduces the temporal join, compares it to the traditional inner join, explains when to use it, and why it is a secret superpower.\\n\\nTable of Contents:\\n* [The Join: A Quick Review](#review)\\n* [The Temporal Join: Linking Stream and State](#tempjoin)\\n* [Temporal Join vs Inner Join](#tempinner)\\n* [Why Temporal Joins are Fast and Efficient](#efficient)\\n* [Temporal Joins Made Easy to Use](#easy)\\n* [Summary](#summary)\\n\\n\x3c!--truncate--\x3e\\n\\n## The Join: A Quick Review {#review}\\n\\nLet\'s take a quick detour down memory lane and revisit the good ol\' join operation. That trusty sidekick in your SQL utility belt helps you link data from two or more tables based on a related column between them.\\n\\nSuppose we are operating a factory with a number of machines that roast and package coffee. We place sensors on each machine to monitor the temperature and detect overheating.\\n\\nWe keep track of the sensors and machines in two database tables.\\n\\nThe `Sensor` table contains the serial number and machine id that the sensor is placed on.\\n\\n| id | serialNo | machineid |\\n|----|----------|-----------|\\n| 1  | X57-774  | 501       |\\n| 2  | X33-453  | 203       |\\n| 3  | X54-554  | 501       |\\n\\nThe `Machine` table contains the name of each machine.\\n\\n| id    | name           |\\n|-------|----------------|\\n| 203   | Iron Roaster   |\\n| 501   | Gritty Grinder |\\n\\nTo identify all the sensors on the machine \u201cIron Roaster\u201d we use the following SQL query which joins the `Sensor` and `Machine` tables:\\n```sql\\nSELECT s.id, s.serialNo FROM Sensor s \\n    JOIN Machine m ON s.machineid = m.id \\n    WHERE m.name = \u201cIron Roaster\u201d\\n```\\n\\nWhy are joins important? Without it, your data tables are like islands, isolated and lonely. Joins bring them together, creating meaningful relationships between data, and enriching data records with context to see the bigger picture.\\n\\nBy default, databases execute joins as **inner** joins which means only matching records are included in the join.\\n\\nSo, now that we\'ve refreshed our memory about the classic join, let\'s dive into the exciting world of temporal joins in stream processing systems like Apache Flink.\\n\\n## The Temporal Join: Linking Stream and State {#tempjoin}\\n\\n<img src={useBaseUrl(\'/img/blog/delorean.jpeg\')} alt=\\"Temporal Join DeLorean >\\" width=\\"40%\\"/>\\n\\nPicture this: you\'re a time traveler. You have the power to access any point in time, past or future, at your will. Now, imagine that your data could do the same. Enter the Temporal Join, the DeLorean of data operations, capable of taking your data on a time-traveling adventure.\\n\\nA Temporal Join is like a regular join but with a twist. It allows you to join a stream of data (the time traveler) with a versioned table (the timeline) based on the time attribute of the data stream. This means that for each record in the stream, the join will find the most recent record in the versioned table that is less than or equal to the stream record\'s time.\\n\\nThe versioned table is a normal state table where we keep track of data changes over time. That is, we keep older versions of each record around to allow the stream to match the correct version in time. Like time travel, temporal joins can make your head spin a bit. Let\u2019s look at an example to break it down.\\n\\n## Temporal Join vs Inner Join {#tempinner}\\n\\nBack to our coffee roasting factory, we collect the temperature readings from each sensor in a data stream.\\n\\n| timestamp           | sensorid | temperature |\\n|---------------------|----------|-------------|\\n| 2023-07-10T07:11:08 | 1        | 105.2       |\\n| 2023-07-10T07:11:08 | 2        | 83.1        |\\n| ...                 |          |             |\\n| 2023-07-10T13:25:16 | 1        | 77.8        |\\n| 2023-07-10T13:25:16 | 2        | 83.5        |\\n\\nAnd we want to know the maximum temperature recorded for each machine.\\n\\nEasy enough, let\u2019s join the temperature data stream with the Sensors table and aggregate by machine id:\\n\\n```sql\\nSELECT s.machineid, MAX(r.temperature) AS maxTemp \\nFROM SensorReading r INNER JOIN Sensor s \\n    ON r.sensorid = s.id GROUP BY s.machineid\\n```\\n\\nBut here is a problem: What if we moved a sensor from one machine to another during the day? With an inner join, all of the sensor\u2019s readings would be linked to the machine it was last placed on. So, if sensor 1 records a high temperature of 105 degrees in the morning and we move the sensor to the \u201cIron Roaster\u201d machine in the afternoon, then we might see the 105 degrees falsely show up as the maximum temperature for the Iron Roaster. See how time played a trick on our join?\\n\\nAnd this happens whenever we join a data stream with a state table that changes over time, like our sensors that get moved around the factory. What to do? Let\u2019s call the temporal join to our rescue:\\n\\n```sql\\nSELECT s.machineid, MAX(r.temperature) AS maxTemp \\nFROM SensorReading r JOIN Sensor FOR SYSTEM_TIME AS OF r.`timestamp` s\\n    ON r.sensorid = s.id GROUP BY s.machineid\\n```\\n\\nPretty much the same query, just a different join type. Just a heads-up: the syntax for temporal joins in Flink SQL is more complex - we\'ll get to that [later](#easy).\\n\\nAs a temporal join, we are joining each sensor reading with the version of the sensor record at the time of the data stream. In other words, the join not only matches the sensor reading with the sensor record based on the id but also based on the timestamp of the reading to ensure it matches the right version of the sensor record. Pretty neat, right?\\n\\nWhenever you join a data stream with a state that changes over time, you want to use the temporal join to make sure your data is lined up correctly in time. Temporal joins are a powerful feature of stream processing engines that would be difficult to implement in a database.\\n\\n\\n## Why Temporal Joins are Fast and Efficient {#efficient}\\n\\n<img src={useBaseUrl(\'/img/blog/flink_logo.svg\')} alt=\\"Apache Flink >\\" width=\\"30%\\"/>\\n\\nNot only do temporal joins solve the time-alignment problem when joining data streams with changing state, modern stream processors like Apache Flink are also incredibly efficient at executing temporal joins. A powerful feature with great performance? Sounds too good to be true. Let\u2019s peek behind the stream processing curtain to find out why.\\n\\nIn stream processing, joins are maintained as the underlying data changes over time. That requires the stream engine to hold all the data it needs to update join records when either side of the join changes. This makes inner joins pretty expensive on data streams.\\n\\nConsider our max-temperature query with the inner join: When we join a temperature reading with the corresponding sensor record, and that record changes, the engine has to update the result join record. To do so, it has to store all the sensor readings to determine which join results are affected by a change in a sensor record. This can lead to a lot of updates and hence a lot of downstream computation. It can also cause system failure when there are a lot of temperature readings in our data stream because the stream engine has to store all of them.\\n\\nTemporal joins, on the other hand, can be executed much more efficiently. The stream engine only needs to store the versions of the sensor table that are within the time bounds of the sensor reading data stream. And it only has to briefly store (if at all) the sensor reading records to ensure they are joined with the most up-to-date sensor records. Moreover, temporal joins don\u2019t require sending out a massive amount of updated join records when sensors change placement since the join is fixed in time.\\n\\n## Time to Wrap Up This Temporal Journey {#summary}\\n\\nWe\'ve reached the end of our time-traveling adventure through the universe of temporal joins. We\'ve seen how they\'re like the DeLorean of data operations, zipping us back and forth through time to make sure our data matches up just right. We\'ve also compared them to the good ol\' inner join.\\n\\nTemporal joins help us avoid the pitfalls of time-alignment problems when joining data streams with changing state. They\'re also super efficient, making them a great choice for high-volume, real-time data processing.\\n\\nAnd that\u2019s why the temporal join is stream processing\'s secret superpower.\\n\\nDataSQRL makes using temporal joins a breeze. With its simplified syntax and smart defaults, it\'s like having a personal tour guide leading you through the sometimes bewildering landscape of stream processing. Take a look at our [Getting Started](/docs/getting-started) to see a complete example of temporal joins in action or take a look at our [other tutorials](/docs/tutorials) for a step-by-step guide to stream processing including temporal joins.\\n\\nHappy data time-traveling, folks!"},{"id":"lets-uplevel-database-datasqrl","metadata":{"permalink":"/blog/lets-uplevel-database-datasqrl","source":"@site/blog/2023-05-15-lets-uplevel-database-datasqrl.mdx","title":"Let\'s Uplevel Our Database Game: Meet DataSQRL","description":"We need to make it easier to build data-driven applications. Databases are great if all your application needs is storing and retrieving data. But if you want to build anything more interesting with data - like serving users recommendations based on the pages they are visiting, detecting fraudulent transactions on your site, or computing real-time features for your machine learning model - you end up building a ton of custom code and infrastructure around the database.","date":"2023-05-15T00:00:00.000Z","tags":[{"inline":true,"label":"DataSQRL","permalink":"/blog/tags/data-sqrl"},{"inline":true,"label":"community","permalink":"/blog/tags/community"}],"readingTime":4.7,"hasTruncateMarker":true,"authors":[{"name":"Matthias Broecheler","title":"CEO of DataSQRL","url":"https://github.com/mbroecheler","page":{"permalink":"/blog/authors/matthias"},"socials":{"linkedin":"https://www.linkedin.com/in/matthiasbroecheler/","github":"https://github.com/mbroecheler","newsletter":"https://www.matthiasb.com"},"imageURL":"/img/headshots/matthias1.png","key":"matthias"}],"frontMatter":{"slug":"lets-uplevel-database-datasqrl","title":"Let\'s Uplevel Our Database Game: Meet DataSQRL","authors":["matthias"],"tags":["DataSQRL","community"]},"unlisted":false,"prevItem":{"title":"Why Temporal Join is Stream Processing\u2019s Superpower","permalink":"/blog/temporal-join"}},"content":"import useBaseUrl from \'@docusaurus/useBaseUrl\';\\n\\n\\n**We need to make it easier to build data-driven applications.** Databases are great if all your application needs is storing and retrieving data. But if you want to build anything more interesting with data - like serving users recommendations based on the pages they are visiting, detecting fraudulent transactions on your site, or computing real-time features for your machine learning model - you end up building a ton of custom code and infrastructure around the database.\\n\\nYou need a queue like Kafka to hold your events, a stream processor like Flink to process data, a database like Postgres to store and query the result data, and an API layer to tie it all together.\\n\\n<img src={useBaseUrl(\'/img/reference/full_logo.svg\')} alt=\\"DataSQRL Logo >\\" width=\\"30%\\" />\\n\\nAnd that\u2019s just the price of admission. To get a functioning data layer, you need to make sure that all these components talk to each other and that data flows smoothly between them. Schema synchronization, data model tuning, index selection, query batching \u2026 all that fun stuff.\\n\\nThe point is, you need to do a ton of data plumbing if you want to build a data-driven application. All that data plumbing code is time-consuming to develop, hard to maintain, and expensive to operate.\\n\\nWe need to make building with data easier. That\u2019s why we are sending out this call to action to uplevel our database game. **Join us in figuring out how to simplify the data layer.**\\n\\nWe have an idea to get us started: Meet DataSQRL.\\n\\n\x3c!--truncate--\x3e\\n\\n## Introducing DataSQRL\\n\\nDataSQRL is a build tool that compiles your application\u2019s data layer from a high-level data development language, dubbed SQRL.\\n\\nOur goal is to create a new abstraction layer above the low-level languages often used in data layers, allowing a compiler to handle the tedious tasks of data plumbing, infrastructure assembly, and configuration management.\\n\\nMuch like how you use high-level languages such as Javascript, Python, or Java instead of Assembly for software development, we believe a similar approach should be used for data. \\n\\nSQRL is designed to be a developer-friendly version of SQL, maintaining familiar syntax while adding features necessary for building data-driven applications, like support for nested data and data streams.\\n\\nCheck out this simple SQRL script to build a recommendation engine from clickstream data.\\n\\n```sql\\nIMPORT clickstream.Clickstream; --Import clickstream data from Kafka\\nIMPORT content.Content;         --Import content from CDC stream\\n\\n/* Find next page visits within 10 minutes */\\n_CoVisits := SELECT b.url AS beforeURL, a.url AS afterURL,\\n                    a.event_time AS `timestamp`\\n             FROM Clickstream b INNER JOIN Clickstream a ON b.userid=a.userid\\n                 AND b.event_time < a.event_time AND\\n                     b.event_time >= a.event_time - INTERVAL 10 MINUTE;\\n/* Recommend pages that are visited shortly after */\\n/*+query_by_all(url) */\\nRecommendation := SELECT beforeURL AS url, afterURL AS recommendation,\\n                         count(1) AS frequency FROM _CoVisits\\n                  GROUP BY beforeURL, afterURL\\n                  ORDER BY url ASC, frequency DESC;\\n```\\n\\nThis little SQRL script imports clickstream data, identifies pairs of URLs visited within a 10-minute interval, and compiles these pairs into a set of recommendations, ordered by the frequency of co-visits.\\n\\n<img src={useBaseUrl(\'/img/diagrams/getting_started_diagram2.png\')} alt=\\"Data pipeline >\\" />\\n\\nDataSQRL then takes this script and compiles it into an integrated data pipeline, complete with all necessary data plumbing pre-installed. It configures access to the clickstream. It generates an executable for the stream processor that ingests, validates, joins, and aggregates the clickstream data. It creates the data model and writes the aggregated data to the database. It synchronizes timestamps and schemas between all the components. And it compiles a server executable that queries the database and exposes the computed recommendations through a GraphQL API.\\n\\n**The bottom line: These 9 lines of SQRL code can replace hundreds of lines of complex data plumbing code and save hours of infrastructure setup.**\\n\\nWe believe that all this low-level data plumbing work should be done by a compiler since it is tedious, time-consuming, and error-prone. Let\u2019s uplevel our data game, so we can focus on **what** we are trying to build with data and less on the **how**.\\n\\n## Join Us on this Journey\\n\\n<img src={useBaseUrl(\'/img/undraw/code.svg\')} alt=\\"Join DataSQRL Community >\\" width=\\"50%\\"/>\\n\\n\\nWe have the ambitious goal of designing a higher level of abstraction for data to enable millions of developers to build data-driven applications.\\n\\nWe [just released](https://github.com/DataSQRL/sqrl/releases/tag/v0.1.0) the first version of DataSQRL, and we recognize that we are at the beginning of a long, long road. We need your help. If you are a data nerd, like building with data, or wish it was easier, please [join us on this journey](https://github.com/DataSQRL/sqrl). DataSQRL is an open-source project, and all development activity is transparent.\\n\\nHere are some ideas for how you can contribute:\\n\\n* Share your thoughts: Do you have ideas on how we can improve the SQRL language or the DataSQRL compiler? Jump into [our community](/community) and let us know!\\n* Test the waters: Do you like playing with new technologies? Try out [DataSQRL](/docs/getting-started) and let us know if you find any bugs or missing features.\\n* Spread the word: Think DataSQRL has potential? Share this blog post and [star](https://github.com/DataSQRL/sqrl) DataSQRL on [Github](https://github.com/DataSQRL/sqrl). Your support can help us reach more like-minded individuals.\\n* Code with us: Do you enjoy contributing to open-source projects? Dive into [the code](https://github.com/DataSQRL/sqrl) with us and pick up a [ticket](https://github.com/DataSQRL/sqrl/issues).\\n\\nLet\u2019s uplevel our database game. With your help, we can make building with data fun and productive.\\n\\n## More Information\\n\\nYou probably have a ton of questions now. How do I import my own data? How do I customize the API? How do I deploy SQRL scripts to production? How do I import functions from my favorite programming language?\\n\\nThose are all great questions. Check out [the documentation](/docs/intro) for answers."}]}}')}}]);