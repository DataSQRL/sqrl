package ai.datasqrl.physical.stream.flink.plan;

import ai.datasqrl.plan.calcite.hints.*;
import ai.datasqrl.plan.calcite.table.ImportedSourceTable;
import ai.datasqrl.plan.calcite.util.SqrlRexUtil;
import ai.datasqrl.plan.calcite.util.TimePredicate;
import ai.datasqrl.plan.calcite.util.TimeTumbleFunctionCall;
import com.google.common.base.Preconditions;
import com.google.common.collect.Iterables;
import lombok.AllArgsConstructor;
import org.apache.calcite.rel.RelNode;
import org.apache.calcite.rel.RelShuttleImpl;
import org.apache.calcite.rel.core.*;
import org.apache.calcite.rel.logical.*;
import org.apache.calcite.rel.type.RelDataType;
import org.apache.calcite.rel.type.RelDataTypeField;
import org.apache.calcite.rex.RexInputRef;
import org.apache.calcite.rex.RexNode;
import org.apache.calcite.rex.RexShuttle;
import org.apache.calcite.sql.SqlOperator;
import org.apache.calcite.tools.RelBuilder;
import org.apache.calcite.util.ImmutableBitSet;
import org.apache.commons.collections.ListUtils;
import org.apache.flink.table.api.bridge.java.internal.StreamTableEnvironmentImpl;
import org.apache.flink.table.planner.calcite.FlinkRelBuilder;
import org.apache.flink.table.planner.calcite.FlinkRexBuilder;
import org.apache.flink.table.planner.delegation.StreamPlanner;
import org.apache.flink.table.planner.functions.sql.FlinkSqlOperatorTable;

import java.util.ArrayList;
import java.util.List;
import java.util.Optional;
import java.util.function.Supplier;
import java.util.stream.Collectors;

/**
 * Rewrites the streaming physical plan generated by the optimizer for Flink.
 * Specifically, this means:
 *  - Injecting a Flink Calcite cluster to replace the SQRL calcite cluster
 *  - Adding watermarks and propagating timestamp columns
 *  - Expanding temporal joins
 *  - Expanding time-based aggregations into Flink window aggregations
 *  - Handling interval joins
 */
@AllArgsConstructor
public class FlinkPhysicalPlanRewriter extends RelShuttleImpl {
  StreamTableEnvironmentImpl tEnv;
  Supplier<FlinkRelBuilder> relBuilderFactory;

  public static RelNode rewrite(StreamTableEnvironmentImpl tEnv, RelNode input) {
    return input.accept(new FlinkPhysicalPlanRewriter(tEnv, () -> ((StreamPlanner) tEnv.getPlanner()).getRelBuilder()));
  }

  private FlinkRelBuilder getBuilder() {
    return relBuilderFactory.get();
  }

  @Override
  public RelNode visit(TableScan scan) {
    ImportedSourceTable t = scan.getTable().unwrap(ImportedSourceTable.class);
    String tableName = t.getNameId();
    FlinkRelBuilder relBuilder = getBuilder();
    relBuilder.scan(tableName);
    SqrlHint.fromRel(scan, WatermarkHint.CONSTRUCTOR).ifPresent(watermark -> addWatermark(relBuilder,watermark.getTimestampIdx()));
    return relBuilder.build();
  }

  @Override
  public RelNode visit(LogicalProject project) {
    FlinkRelBuilder relBuilder = getBuilder();
    relBuilder.push(project.getInput().accept(this));
    relBuilder.project(project.getProjects().stream().map(rex -> rewrite(rex,relBuilder)).collect(Collectors.toList()),
            project.getRowType().getFieldList().stream().map(f -> f.getName()).collect(Collectors.toList()));
    SqrlHint.fromRel(project, WatermarkHint.CONSTRUCTOR).ifPresent(watermark -> addWatermark(relBuilder,watermark.getTimestampIdx()));
    return relBuilder.build();
  }

  private void addWatermark(FlinkRelBuilder relBuilder, int timestampIndex) {
    relBuilder.watermark(timestampIndex,getRexBuilder(relBuilder).makeInputRef(relBuilder.peek(), timestampIndex));
  }

  @Override
  public RelNode visit(LogicalFilter filter) {
    FlinkRelBuilder relBuilder = getBuilder();
    relBuilder.push(filter.getInput().accept(this));
    relBuilder.filter(filter.getVariablesSet(),rewrite(filter.getCondition(),relBuilder));
    return relBuilder.build();
  }

  @Override
  public RelNode visit(LogicalJoin join) {
    FlinkRelBuilder relBuilder = getBuilder();
    Optional<TemporalJoinHint> temporalHintOpt = SqrlHint.fromRel(join,TemporalJoinHint.CONSTRUCTOR);
    Preconditions.checkArgument(temporalHintOpt.isEmpty(),"Not yet implemented");
    RelNode left = join.getLeft().accept(this), right = join.getRight().accept(this);
    relBuilder.push(left);
    relBuilder.push(right);
    JoinRelType joinType = join.getJoinType();
    relBuilder.join(joinType,rewrite(join.getCondition(), relBuilder, left, right));
    return relBuilder.build();
  }

  @Override
  public RelNode visit(LogicalAggregate aggregate) {
    FlinkRelBuilder relBuilder = getBuilder();
    Optional<TumbleAggregationHint> tumbleHintOpt = SqrlHint.fromRel(aggregate, TumbleAggregationHint.CONSTRUCTOR);
    Optional<SlidingAggregationHint> slideHintOpt = SqrlHint.fromRel(aggregate, SlidingAggregationHint.CONSTRUCTOR);
    ImmutableBitSet groupBy = Iterables.getOnlyElement(aggregate.groupSets);
    List<AggregateCall> aggCalls = aggregate.getAggCallList();
    RelNode input = aggregate.getInput().accept(this);
    if (tumbleHintOpt.isPresent() || slideHintOpt.isPresent()) {
      Preconditions.checkArgument(tumbleHintOpt.isPresent() ^ slideHintOpt.isPresent());
      FlinkRexBuilder rexBuilder = getRexBuilder(relBuilder);
      int inputFieldCount = input.getRowType().getFieldCount();
      RelDataType inputType = input.getRowType();
      relBuilder.push(input.getInput(0));

      final int timestampIdx;
      final long[] intervalsMs;
      final SqlOperator windowFunction;

      if (tumbleHintOpt.isPresent()) {
        TumbleAggregationHint tumbleHint = tumbleHintOpt.get();
        timestampIdx = tumbleHint.getTimestampIdx();
        //Extract bucketing function from project
        Preconditions.checkArgument(input instanceof LogicalProject, "Expected projection as input");
        List<RexNode> projects = new ArrayList<>(((LogicalProject) input).getProjects());
        SqrlRexUtil rexUtil = new SqrlRexUtil(relBuilder.getTypeFactory());
        TimeTumbleFunctionCall bucketFct = rexUtil.getTimeBucketingFunction(projects.get(timestampIdx)).get();
        projects.set(timestampIdx, rexBuilder.makeInputRef(input.getInput(0), bucketFct.getTimestampColumnIndex()));
        long intervalMs = bucketFct.getSpecification().getBucketWidthMillis();
        intervalsMs = new long[]{intervalMs};
        windowFunction = FlinkSqlOperatorTable.TUMBLE;
        relBuilder.project(projects, inputType.getFieldNames());
      } else {
        SlidingAggregationHint slideHint = slideHintOpt.get();
        timestampIdx = slideHint.getTimestampIdx();
        intervalsMs = new long[]{slideHint.getSlideWidthMs(), slideHint.getIntervalWidthMs()};
        windowFunction = FlinkSqlOperatorTable.HOP;
      }

      makeWindow(relBuilder,windowFunction,timestampIdx,intervalsMs);
      //Need to add all 3 window columns that are added to groupBy and then project out all but window_time
      int window_start = inputFieldCount, window_end = inputFieldCount+1, window_time = inputFieldCount+2;
      List<Integer> groupByIdx = new ArrayList<>();
      List<Integer> projectIdx = new ArrayList<>();
      List<String> projectNames = new ArrayList<>();
      int index = 0;
      int window_time_idx = (groupBy.cardinality()-1)+3-1; //Points at window_time at the end of groupByIdx
      for (int idx : groupBy.asList()) {
        if (idx==timestampIdx) {
          projectIdx.add(window_time_idx);
          projectNames.add(inputType.getFieldNames().get(timestampIdx));
        } else {
          groupByIdx.add(idx);
          projectIdx.add(index++);
          projectNames.add(inputType.getFieldNames().get(idx));
        }
      }
      groupByIdx.add(window_start); groupByIdx.add(window_end);
      groupByIdx.add(window_time); //Window_time is new timestamp
      index+=3;
      assert window_time_idx==index-1;

      for (int i = 0; i < aggCalls.size(); i++) {
        projectIdx.add(index++);
        projectNames.add(null);
      }
      relBuilder.aggregate(relBuilder.groupKey(ImmutableBitSet.of(groupByIdx)),aggCalls);
      relBuilder.project(projectIdx.stream().map(idx -> rexBuilder.makeInputRef(relBuilder.peek(), idx))
              .collect(Collectors.toList()),projectNames);
    } else {
      //Normal aggregation
      relBuilder.push(input);
      relBuilder.aggregate(relBuilder.groupKey(groupBy),aggCalls);
    }
    return relBuilder.build();
  }

  private FlinkRelBuilder makeWindow(FlinkRelBuilder relBuilder, SqlOperator operator, int timestampIdx, long... intervalsMs) {
    Preconditions.checkArgument(intervalsMs!=null && intervalsMs.length>0);
    FlinkRexBuilder rexBuilder = getRexBuilder(relBuilder);
    RelNode input = relBuilder.peek();

    List<RexNode> operandList = new ArrayList<>();
    operandList.add(rexBuilder.makeRangeReference(input));
    operandList.add(rexBuilder.makeCall(FlinkSqlOperatorTable.DESCRIPTOR,rexBuilder.makeInputRef(input,timestampIdx)));
    for (long intervalArg : intervalsMs) {
      operandList.add(TimePredicate.makeInterval(intervalArg, rexBuilder));
    }

    //this window functions adds 3 columns to end of relation: window_start/_end/_time
    relBuilder.functionScan(FlinkSqlOperatorTable.TUMBLE,1,operandList);
    LogicalTableFunctionScan tfs = (LogicalTableFunctionScan) relBuilder.build();

    //Flink expects an inputref for the last column of the original relation as the first operand
    operandList = ListUtils.union(List.of(rexBuilder.makeInputRef(input,input.getRowType().getFieldCount()-1)),
            operandList.subList(1,operandList.size()));
    relBuilder.push(tfs.copy(tfs.getTraitSet(),tfs.getInputs(),
            rexBuilder.makeCall(tfs.getRowType(), operator, operandList),
            tfs.getElementType(),tfs.getRowType(),tfs.getColumnMappings()));
    return relBuilder;
  }

  @Override
  public RelNode visit(LogicalValues values) {
    return getBuilder().values(values.tuples, values.getRowType()).build();
  }

  @Override
  public RelNode visit(LogicalUnion union) {
    throw new UnsupportedOperationException("Not yet implemented");
    //return super.visit(union);
  }

  @Override
  public RelNode visit(LogicalCorrelate correlate) {
    FlinkRelBuilder relBuilder = getBuilder();
    relBuilder.push(correlate.getLeft().accept(this));
    RelDataType base = relBuilder.peek().getRowType();
    relBuilder.push(correlate.getRight().accept(this));
    relBuilder.correlate(correlate.getJoinType(), correlate.getCorrelationId(),
            correlate.getRequiredColumns().asList().stream().map(i -> relBuilder.getRexBuilder().makeInputRef(base,i)).collect(Collectors.toList()));
    return relBuilder.build();
  }

  @Override
  public RelNode visit(RelNode other) {
    if (other instanceof Uncollect) {
      Uncollect uncollect = (Uncollect) other;
      RelBuilder relBuilder = getBuilder();
      relBuilder.push(uncollect.getInput().accept(this));
      relBuilder.uncollect(List.of(),uncollect.withOrdinality);
      return relBuilder.build();
    }
    throw new UnsupportedOperationException("not yet implemented:" + other.getClass());
  }

  @Override
  public RelNode visit(TableFunctionScan scan) {
    throw new UnsupportedOperationException("Not yet supported");
//    List<RelNode> inputs = scan.getInputs().stream()
//        .map(this::visit)
//        .collect(Collectors.toList());
//    return new LogicalTableFunctionScan(cluster, defaultTrait, inputs, scan.getCall(),
//        scan.getElementType(), scan.getRowType(), scan.getColumnMappings());
  }

  @Override
  public RelNode visit(LogicalIntersect intersect) {
    throw new UnsupportedOperationException("Not yet supported");
  }

  @Override
  public RelNode visit(LogicalMinus minus) {
    throw new UnsupportedOperationException("Not yet supported");
  }

  @Override
  public RelNode visit(LogicalSort sort) {
    throw new UnsupportedOperationException("Sorts are not supported during materialization");
  }


  /*
  ====== Rewriting RexNodes
   */

  private static FlinkRexBuilder getRexBuilder(FlinkRelBuilder relBuilder) {
    return new FlinkRexBuilder(relBuilder.getTypeFactory());
  }

  private RexNode rewrite(RexNode node, FlinkRelBuilder relBuilder) {
    return rewrite(node, relBuilder, relBuilder.peek());
  }

  private RexNode rewrite(RexNode node, FlinkRelBuilder relBuilder, RelNode... inputNodes) {
    Preconditions.checkArgument(inputNodes!=null && inputNodes.length>0);
    List<RelDataTypeField> fields;
    if (inputNodes.length==1) {
      fields = inputNodes[0].getRowType().getFieldList();
    } else {
      fields = new ArrayList<>();
      for (RelNode input : inputNodes) {
        fields.addAll(input.getRowType().getFieldList());
      }
    }
    return node.accept(new RexRewriter(fields,
            FlinkPhysicalPlanRewriter.getRexBuilder(relBuilder)));
  }

  @AllArgsConstructor
  private static class RexRewriter extends RexShuttle {

    private final List<RelDataTypeField> inputFields;
    private final FlinkRexBuilder rexBuilder;

    @Override
    public RexNode visitInputRef(RexInputRef ref) {
      return rexBuilder.makeInputRef(inputFields.get(ref.getIndex()).getType(),ref.getIndex());
    }
  }

}
